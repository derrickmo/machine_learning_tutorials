{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations",
    "## 01-06: Linear Algebra for Machine Learning",
    "",
    "**Objective:** Build the core linear algebra toolkit used throughout ML —",
    "eigendecomposition, singular value decomposition (SVD), and low-rank",
    "approximation — from scratch, then apply them to real data.",
    "",
    "**Prerequisites:** 01-01 (Python, NumPy & Tensor Speed), 01-02 (Advanced NumPy & PyTorch Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 0 — Setup & Prerequisites",
    "",
    "Linear algebra is the language of machine learning. Matrices represent",
    "datasets, weight matrices define neural network layers, and matrix",
    "decompositions reveal the hidden structure in data.",
    "",
    "This notebook covers:",
    "- **Vector and matrix norms** — measuring magnitude and distance",
    "- **Matrix decompositions** — eigendecomposition, SVD, Cholesky",
    "- **Low-rank approximation** — compressing matrices while preserving information",
    "- **PCA from scratch** — the most important application of eigendecomposition",
    "- **Condition numbers** — understanding numerical stability",
    "",
    "We use synthetic data, the California Housing dataset, and image data",
    "to ground every concept in a practical ML application.",
    "",
    "**Prerequisites:** 01-01, 01-02 (Advanced NumPy & PyTorch Operations)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA: {torch.version.cuda}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "import random\n",
    "\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "FIGSIZE = (10, 6)\n",
    "COLORS = {\n",
    "    'blue': '#1E88E5',\n",
    "    'red': '#E53935',\n",
    "    'green': '#43A047',\n",
    "    'orange': '#FF9800',\n",
    "    'purple': '#9C27B0',\n",
    "    'teal': '#00897B',\n",
    "    'gray': '#757575',\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading",
    "",
    "We load the California Housing dataset for PCA and regression examples,",
    "and create synthetic data for geometric visualizations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# California Housing\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data.astype(np.float64)\n",
    "y_housing = housing.target.astype(np.float64)\n",
    "feature_names = housing.feature_names\n",
    "print(f'California Housing: {X_housing.shape} ({X_housing.shape[1]} features)')\n",
    "print(f'Features: {feature_names}')\n",
    "\n",
    "# Standardize for PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_housing)\n",
    "print(f'Scaled data: mean ≈ {X_scaled.mean(axis=0).round(2)}, std ≈ {X_scaled.std(axis=0).round(2)}')\n",
    "\n",
    "# Synthetic 2D data for geometric visualization\n",
    "rng = np.random.RandomState(SEED)\n",
    "cov_matrix = np.array([[3.0, 1.5], [1.5, 1.0]])\n",
    "X_2d = rng.multivariate_normal(mean=[0, 0], cov=cov_matrix, size=300)\n",
    "print(f'\\n2D synthetic data: {X_2d.shape}')\n",
    "print(f'  Covariance matrix:\\n{np.cov(X_2d.T).round(3)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 1 — Linear Algebra Foundations from Scratch",
    "",
    "We build up from norms and inner products to the two most important",
    "matrix decompositions in ML: eigendecomposition and SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vector and Matrix Norms",
    "",
    "Norms measure the *size* of vectors and matrices. They appear everywhere",
    "in ML — loss functions, regularization, gradient clipping, convergence.",
    "",
    "**Vector norms:**",
    "- $L^1$ norm: $\\|\\mathbf{x}\\|_1 = \\sum_i |x_i|$ — sparsity measure (Lasso)",
    "- $L^2$ norm: $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}$ — Euclidean distance (Ridge)",
    "- $L^\\infty$ norm: $\\|\\mathbf{x}\\|_\\infty = \\max_i |x_i|$ — largest component",
    "",
    "**Matrix norms:**",
    "- Frobenius norm: $\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$ — \"size\" of a matrix",
    "- Spectral norm: $\\|\\mathbf{A}\\|_2 = \\sigma_{\\max}(\\mathbf{A})$ — largest singular value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_vector_norms(x: np.ndarray) -> dict[str, float]:\n",
    "    \"\"\"Compute L1, L2, and L-infinity norms of a vector.\n",
    "\n",
    "    Args:\n",
    "        x: Input vector of shape (n,).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping norm names to values.\n",
    "    \"\"\"\n",
    "    l1 = np.sum(np.abs(x))\n",
    "    l2 = np.sqrt(np.sum(x ** 2))\n",
    "    linf = np.max(np.abs(x))\n",
    "    return {'L1': l1, 'L2': l2, 'L_inf': linf}\n",
    "\n",
    "\n",
    "def compute_matrix_norms(A: np.ndarray) -> dict[str, float]:\n",
    "    \"\"\"Compute Frobenius and spectral norms of a matrix.\n",
    "\n",
    "    Args:\n",
    "        A: Input matrix of shape (m, n).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping norm names to values.\n",
    "    \"\"\"\n",
    "    frobenius = np.sqrt(np.sum(A ** 2))\n",
    "    spectral = np.max(np.linalg.svd(A, compute_uv=False))\n",
    "    return {'Frobenius': frobenius, 'Spectral': spectral}\n",
    "\n",
    "\n",
    "# Test with example vectors and matrices\n",
    "x_test = np.array([3.0, -4.0, 1.0, 2.0])\n",
    "A_test = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "vec_norms = compute_vector_norms(x_test)\n",
    "mat_norms = compute_matrix_norms(A_test)\n",
    "\n",
    "print(f'Vector x = {x_test}')\n",
    "for name, val in vec_norms.items():\n",
    "    np_val = np.linalg.norm(x_test, ord={'L1': 1, 'L2': 2, 'L_inf': np.inf}[name])\n",
    "    print(f'  {name}: {val:.4f} (NumPy: {np_val:.4f}) ✓' if np.isclose(val, np_val) else f'  {name}: MISMATCH')\n",
    "\n",
    "print(f'\\nMatrix A shape = {A_test.shape}')\n",
    "for name, val in mat_norms.items():\n",
    "    np_ord = 'fro' if name == 'Frobenius' else 2\n",
    "    np_val = np.linalg.norm(A_test, ord=np_ord)\n",
    "    print(f'  {name}: {val:.4f} (NumPy: {np_val:.4f}) ✓' if np.isclose(val, np_val) else f'  {name}: MISMATCH')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Geometric Meaning of Linear Transformations",
    "",
    "A matrix $\\mathbf{A}$ transforms vectors by stretching, rotating, or reflecting",
    "them. Understanding this geometry is key to understanding eigenvalues",
    "and singular values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_linear_transformation(A: np.ndarray) -> None:\n",
    "    \"\"\"Visualize how a 2x2 matrix transforms the unit circle.\n",
    "\n",
    "    Args:\n",
    "        A: 2x2 transformation matrix.\n",
    "    \"\"\"\n",
    "    assert A.shape == (2, 2), f'Expected (2,2), got {A.shape}'\n",
    "\n",
    "    theta = np.linspace(0, 2 * np.pi, 100)\n",
    "    circle = np.vstack([np.cos(theta), np.sin(theta)])  # (2, 100)\n",
    "    transformed = A @ circle  # (2, 100)\n",
    "\n",
    "    # Basis vectors\n",
    "    e1 = np.array([1, 0])\n",
    "    e2 = np.array([0, 1])\n",
    "    Ae1 = A @ e1\n",
    "    Ae2 = A @ e2\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Before\n",
    "    axes[0].plot(circle[0], circle[1], color=COLORS['blue'], linewidth=2)\n",
    "    axes[0].arrow(0, 0, e1[0]*0.9, e1[1]*0.9, head_width=0.08,\n",
    "                   color=COLORS['red'], linewidth=2)\n",
    "    axes[0].arrow(0, 0, e2[0]*0.9, e2[1]*0.9, head_width=0.08,\n",
    "                   color=COLORS['green'], linewidth=2)\n",
    "    axes[0].set_title('Before: Unit Circle')\n",
    "    axes[0].set_xlim(-3, 3)\n",
    "    axes[0].set_ylim(-3, 3)\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(0, color='gray', linewidth=0.5)\n",
    "    axes[0].axvline(0, color='gray', linewidth=0.5)\n",
    "\n",
    "    # After\n",
    "    axes[1].plot(transformed[0], transformed[1], color=COLORS['blue'], linewidth=2)\n",
    "    axes[1].arrow(0, 0, Ae1[0]*0.9, Ae1[1]*0.9, head_width=0.08,\n",
    "                   color=COLORS['red'], linewidth=2)\n",
    "    axes[1].arrow(0, 0, Ae2[0]*0.9, Ae2[1]*0.9, head_width=0.08,\n",
    "                   color=COLORS['green'], linewidth=2)\n",
    "    axes[1].set_title(f'After: A · circle (det={np.linalg.det(A):.2f})')\n",
    "    axes[1].set_xlim(-3, 3)\n",
    "    axes[1].set_ylim(-3, 3)\n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(0, color='gray', linewidth=0.5)\n",
    "    axes[1].axvline(0, color='gray', linewidth=0.5)\n",
    "\n",
    "    plt.suptitle(f'Linear Transformation: A = {A.tolist()}', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Stretching + rotation\n",
    "A_transform = np.array([[2.0, 0.5], [0.5, 1.0]])\n",
    "visualize_linear_transformation(A_transform)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Eigendecomposition from Scratch",
    "",
    "For a square matrix $\\mathbf{A}$, an eigenvector $\\mathbf{v}$ satisfies:",
    "",
    "$$\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$$",
    "",
    "where $\\lambda$ is the corresponding eigenvalue. The matrix can be decomposed as:",
    "",
    "$$\\mathbf{A} = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^{-1}$$",
    "",
    "For symmetric matrices (covariance matrices!), $\\mathbf{V}$ is orthogonal:",
    "$\\mathbf{A} = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^T$",
    "",
    "We implement the **power iteration** method to find the dominant eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def power_iteration(\n",
    "    A: np.ndarray,\n",
    "    num_iterations: int = 100,\n",
    "    tolerance: float = 1e-10,\n",
    ") -> tuple[float, np.ndarray]:\n",
    "    \"\"\"Find the dominant eigenvalue/eigenvector using power iteration.\n",
    "\n",
    "    The power method repeatedly multiplies a random vector by A and\n",
    "    normalizes. It converges to the eigenvector with the largest\n",
    "    absolute eigenvalue.\n",
    "\n",
    "    Args:\n",
    "        A: Square matrix of shape (n, n).\n",
    "        num_iterations: Maximum iterations.\n",
    "        tolerance: Convergence threshold for eigenvalue change.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (dominant_eigenvalue, dominant_eigenvector).\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    assert A.shape == (n, n), f'Expected square matrix, got {A.shape}'\n",
    "\n",
    "    # Start with random vector\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    v = rng.randn(n)\n",
    "    v = v / np.linalg.norm(v)\n",
    "\n",
    "    eigenvalue = 0.0\n",
    "    for iteration in range(num_iterations):\n",
    "        # Multiply by A\n",
    "        Av = A @ v\n",
    "\n",
    "        # Rayleigh quotient for eigenvalue estimate\n",
    "        new_eigenvalue = v @ Av\n",
    "\n",
    "        # Normalize\n",
    "        v = Av / np.linalg.norm(Av)\n",
    "\n",
    "        # Check convergence\n",
    "        if abs(new_eigenvalue - eigenvalue) < tolerance:\n",
    "            break\n",
    "        eigenvalue = new_eigenvalue\n",
    "\n",
    "    return eigenvalue, v\n",
    "\n",
    "\n",
    "# Test on our covariance matrix\n",
    "cov_2d = np.cov(X_2d.T)\n",
    "our_eigval, our_eigvec = power_iteration(cov_2d)\n",
    "\n",
    "# Compare with NumPy\n",
    "np_eigvals, np_eigvecs = np.linalg.eigh(cov_2d)  # eigh for symmetric\n",
    "# NumPy returns in ascending order; dominant is the last\n",
    "np_dominant_val = np_eigvals[-1]\n",
    "np_dominant_vec = np_eigvecs[:, -1]\n",
    "\n",
    "# Eigenvectors can differ by sign; compare absolute values\n",
    "sign_match = np.sign(our_eigvec[0]) == np.sign(np_dominant_vec[0])\n",
    "if not sign_match:\n",
    "    np_dominant_vec = -np_dominant_vec\n",
    "\n",
    "print(f'Covariance matrix:\\n{cov_2d.round(4)}')\n",
    "print(f'\\nPower iteration:')\n",
    "print(f'  Eigenvalue: {our_eigval:.6f}')\n",
    "print(f'  Eigenvector: {our_eigvec.round(6)}')\n",
    "print(f'\\nNumPy eigh:')\n",
    "print(f'  Eigenvalue: {np_dominant_val:.6f}')\n",
    "print(f'  Eigenvector: {np_dominant_vec.round(6)}')\n",
    "print(f'\\nMatch: eigenvalue close = {np.isclose(our_eigval, np_dominant_val)}')\n",
    "print(f'Match: eigenvector close = {np.allclose(our_eigvec, np_dominant_vec, atol=1e-4)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement **deflation** to find all eigenvalues/eigenvectors iteratively.",
    "The idea: after finding the dominant eigenpair $(\\lambda_1, \\mathbf{v}_1)$, we",
    "subtract its contribution and repeat on the residual matrix."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def eigendecomposition_power(\n",
    "    A: np.ndarray,\n",
    "    num_components: int | None = None,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute eigendecomposition using power iteration + deflation.\n",
    "\n",
    "    Only works well for symmetric positive semi-definite matrices\n",
    "    (like covariance matrices).\n",
    "\n",
    "    Args:\n",
    "        A: Symmetric matrix of shape (n, n).\n",
    "        num_components: Number of eigenvalues to compute. Defaults to n.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (eigenvalues, eigenvectors) sorted descending by eigenvalue.\n",
    "        eigenvalues has shape (k,), eigenvectors has shape (n, k).\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    if num_components is None:\n",
    "        num_components = n\n",
    "\n",
    "    eigenvalues = []\n",
    "    eigenvectors = []\n",
    "    residual = A.copy()\n",
    "\n",
    "    for _ in range(num_components):\n",
    "        eigval, eigvec = power_iteration(residual)\n",
    "        eigenvalues.append(eigval)\n",
    "        eigenvectors.append(eigvec)\n",
    "\n",
    "        # Deflate: remove this component\n",
    "        residual = residual - eigval * np.outer(eigvec, eigvec)\n",
    "\n",
    "    return np.array(eigenvalues), np.column_stack(eigenvectors)\n",
    "\n",
    "\n",
    "# Full eigendecomposition of 2D covariance\n",
    "our_vals, our_vecs = eigendecomposition_power(cov_2d)\n",
    "\n",
    "print('Our eigendecomposition (descending order):')\n",
    "for i in range(len(our_vals)):\n",
    "    print(f'  λ_{i+1} = {our_vals[i]:.6f}, v_{i+1} = {our_vecs[:, i].round(6)}')\n",
    "\n",
    "print(f'\\nNumPy eigenvalues (ascending): {np_eigvals.round(6)}')\n",
    "print(f'Our eigenvalues (descending):  {our_vals.round(6)}')\n",
    "\n",
    "# Verify reconstruction: A ≈ V Λ V^T\n",
    "reconstructed = our_vecs @ np.diag(our_vals) @ our_vecs.T\n",
    "reconstruction_error = np.linalg.norm(cov_2d - reconstructed, 'fro')\n",
    "print(f'\\nReconstruction error (Frobenius): {reconstruction_error:.10f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualizing Eigenvectors",
    "",
    "Eigenvectors of the covariance matrix show the principal axes of variation.",
    "The eigenvalues tell us how much variance lies along each axis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_eigenvectors(X: np.ndarray, eigenvalues: np.ndarray, eigenvectors: np.ndarray) -> None:\n",
    "    \"\"\"Visualize data with eigenvectors of the covariance matrix.\n",
    "\n",
    "    Args:\n",
    "        X: Data matrix of shape (n_samples, 2).\n",
    "        eigenvalues: Eigenvalues of shape (2,).\n",
    "        eigenvectors: Eigenvectors of shape (2, 2), columns are eigenvectors.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Plot data\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=10, alpha=0.4, color=COLORS['blue'])\n",
    "\n",
    "    # Plot eigenvectors (scaled by eigenvalue for visualization)\n",
    "    mean = X.mean(axis=0)\n",
    "    for i, (val, color, label) in enumerate([\n",
    "        (eigenvalues[0], COLORS['red'], 'PC1'),\n",
    "        (eigenvalues[1], COLORS['green'], 'PC2'),\n",
    "    ]):\n",
    "        vec = eigenvectors[:, i] * np.sqrt(val) * 2  # Scale for visibility\n",
    "        ax.annotate('', xy=mean + vec, xytext=mean,\n",
    "                     arrowprops=dict(arrowstyle='->', color=color, lw=3))\n",
    "        ax.annotate(f'{label} (λ={val:.2f})',\n",
    "                     xy=mean + vec * 1.1, fontsize=11, color=color,\n",
    "                     fontweight='bold')\n",
    "\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title('Data with Principal Component Directions')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Variance explained\n",
    "    total_var = eigenvalues.sum()\n",
    "    explained = eigenvalues / total_var * 100\n",
    "    print(f'Variance explained: PC1={explained[0]:.1f}%, PC2={explained[1]:.1f}%')\n",
    "    print(f'Total variance: {total_var:.4f}')\n",
    "\n",
    "\n",
    "visualize_eigenvectors(X_2d, our_vals, our_vecs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Singular Value Decomposition (SVD)",
    "",
    "SVD generalizes eigendecomposition to **any** matrix (not just square).",
    "Every matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ can be decomposed as:",
    "",
    "$$\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^T$$",
    "",
    "where:",
    "- $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ — left singular vectors (orthogonal)",
    "- $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{m \\times n}$ — singular values on diagonal (non-negative)",
    "- $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ — right singular vectors (orthogonal)",
    "",
    "The singular values $\\sigma_i$ are the square roots of eigenvalues of $\\mathbf{A}^T\\mathbf{A}$.",
    "",
    "**Why SVD matters in ML:**",
    "- PCA (most common dimensionality reduction)",
    "- Low-rank approximation (model compression)",
    "- Pseudoinverse (least squares solutions)",
    "- Matrix completion (recommendation systems)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def svd_from_eigen(A: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute SVD by eigendecomposing A^T A and A A^T.\n",
    "\n",
    "    This demonstrates the connection between SVD and eigendecomposition.\n",
    "    Not numerically stable for large matrices — use np.linalg.svd in practice.\n",
    "\n",
    "    Args:\n",
    "        A: Matrix of shape (m, n).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (U, sigma, Vt) where A ≈ U @ diag(sigma) @ Vt.\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Right singular vectors: eigenvectors of A^T A\n",
    "    AtA = A.T @ A\n",
    "    eigvals_v, V = np.linalg.eigh(AtA)\n",
    "\n",
    "    # Sort descending\n",
    "    idx = np.argsort(eigvals_v)[::-1]\n",
    "    eigvals_v = eigvals_v[idx]\n",
    "    V = V[:, idx]\n",
    "\n",
    "    # Singular values = sqrt of eigenvalues (clip for numerical stability)\n",
    "    sigma = np.sqrt(np.maximum(eigvals_v, 0))\n",
    "\n",
    "    # Left singular vectors: U = A V Σ^{-1}\n",
    "    rank = np.sum(sigma > 1e-10)\n",
    "    U = np.zeros((m, m))\n",
    "    for i in range(rank):\n",
    "        U[:, i] = A @ V[:, i] / sigma[i]\n",
    "\n",
    "    # Fill remaining columns with orthogonal complement (Gram-Schmidt)\n",
    "    if rank < m:\n",
    "        remaining = np.eye(m)\n",
    "        for i in range(rank, m):\n",
    "            vec = remaining[:, i]\n",
    "            for j in range(i):\n",
    "                vec = vec - (U[:, j] @ vec) * U[:, j]\n",
    "            norm = np.linalg.norm(vec)\n",
    "            if norm > 1e-10:\n",
    "                U[:, i] = vec / norm\n",
    "\n",
    "    return U, sigma[:min(m, n)], V.T\n",
    "\n",
    "\n",
    "# Test SVD on a small matrix\n",
    "A_svd = np.array([[1.0, 2.0, 3.0],\n",
    "                   [4.0, 5.0, 6.0]], dtype=np.float64)\n",
    "\n",
    "U_ours, sigma_ours, Vt_ours = svd_from_eigen(A_svd)\n",
    "U_np, sigma_np, Vt_np = np.linalg.svd(A_svd, full_matrices=True)\n",
    "\n",
    "print(f'Matrix A ({A_svd.shape}):')\n",
    "print(A_svd)\n",
    "print(f'\\nOur singular values:  {sigma_ours.round(6)}')\n",
    "print(f'NumPy singular values: {sigma_np.round(6)}')\n",
    "print(f'Match: {np.allclose(sigma_ours, sigma_np, atol=1e-6)}')\n",
    "\n",
    "# Verify reconstruction\n",
    "Sigma_mat = np.zeros_like(A_svd)\n",
    "np.fill_diagonal(Sigma_mat, sigma_ours)\n",
    "reconstructed_svd = U_ours @ Sigma_mat @ Vt_ours\n",
    "print(f'\\nReconstruction error: {np.linalg.norm(A_svd - reconstructed_svd):.10f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Low-Rank Approximation (Truncated SVD)",
    "",
    "The **Eckart-Young theorem** says that the best rank-$k$ approximation",
    "of a matrix (in Frobenius norm) is obtained by keeping only the top $k$",
    "singular values:",
    "",
    "$$\\mathbf{A}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$",
    "",
    "This is the mathematical foundation of:",
    "- **PCA** (keep top $k$ components)",
    "- **Model compression** (approximate weight matrices)",
    "- **Image compression** (keep top $k$ singular values of pixel matrix)",
    "- **Denoising** (noise lives in small singular values)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def low_rank_approximation(\n",
    "    A: np.ndarray,\n",
    "    rank: int,\n",
    ") -> tuple[np.ndarray, float, float]:\n",
    "    \"\"\"Compute rank-k approximation of a matrix using truncated SVD.\n",
    "\n",
    "    Args:\n",
    "        A: Input matrix of shape (m, n).\n",
    "        rank: Number of singular values to keep.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (approximation, relative_error, compression_ratio).\n",
    "    \"\"\"\n",
    "    U, sigma, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "    # Truncate to rank k\n",
    "    U_k = U[:, :rank]\n",
    "    sigma_k = sigma[:rank]\n",
    "    Vt_k = Vt[:rank, :]\n",
    "\n",
    "    # Reconstruct\n",
    "    A_k = U_k @ np.diag(sigma_k) @ Vt_k\n",
    "\n",
    "    # Error metrics\n",
    "    relative_error = np.linalg.norm(A - A_k, 'fro') / np.linalg.norm(A, 'fro')\n",
    "\n",
    "    # Compression: store U_k (m×k) + sigma_k (k) + Vt_k (k×n) vs m×n\n",
    "    m, n = A.shape\n",
    "    original_params = m * n\n",
    "    compressed_params = m * rank + rank + rank * n\n",
    "    compression_ratio = original_params / compressed_params\n",
    "\n",
    "    return A_k, relative_error, compression_ratio\n",
    "\n",
    "\n",
    "# Demonstrate on a random matrix\n",
    "rng = np.random.RandomState(SEED)\n",
    "A_demo = rng.randn(50, 30)\n",
    "\n",
    "results = []\n",
    "for k in [1, 2, 5, 10, 20, 30]:\n",
    "    A_k, rel_err, comp_ratio = low_rank_approximation(A_demo, k)\n",
    "    results.append({\n",
    "        'Rank': k,\n",
    "        'Relative Error': f'{rel_err:.4f}',\n",
    "        'Error %': f'{rel_err*100:.1f}%',\n",
    "        'Compression': f'{comp_ratio:.1f}×',\n",
    "        'Params (original)': A_demo.size,\n",
    "        'Params (compressed)': 50 * k + k + k * 30,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print('=== Low-Rank Approximation Quality ===')\n",
    "print(results_df.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Image Compression with SVD",
    "",
    "A classic demonstration of low-rank approximation: compress a grayscale",
    "image by keeping only the top $k$ singular values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_image_compression() -> None:\n",
    "    \"\"\"Show SVD-based image compression at different ranks.\"\"\"\n",
    "    # Create a synthetic grayscale image (gradient + shapes)\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    size = 128\n",
    "    image = np.zeros((size, size))\n",
    "\n",
    "    # Gradient background\n",
    "    x_grid, y_grid = np.meshgrid(np.linspace(0, 1, size), np.linspace(0, 1, size))\n",
    "    image += 0.3 * x_grid + 0.2 * y_grid\n",
    "\n",
    "    # Add circles\n",
    "    for cx, cy, r, intensity in [(40, 40, 15, 0.8), (90, 60, 20, 0.6), (60, 100, 12, 0.7)]:\n",
    "        mask = (x_grid * size - cx)**2 + (y_grid * size - cy)**2 < r**2\n",
    "        image[mask] = intensity\n",
    "\n",
    "    # Add noise\n",
    "    image += rng.randn(size, size) * 0.05\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    # Compute SVD\n",
    "    U, sigma, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "\n",
    "    # Compress at different ranks\n",
    "    ranks = [1, 5, 10, 20, 50, 128]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, k in enumerate(ranks):\n",
    "        A_k = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
    "        rel_err = np.linalg.norm(image - A_k, 'fro') / np.linalg.norm(image, 'fro')\n",
    "        comp_ratio = (size * size) / (size * k + k + k * size)\n",
    "\n",
    "        axes[idx].imshow(A_k, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[idx].set_title(f'Rank {k} (error={rel_err:.2%}, {comp_ratio:.1f}× compression)')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.suptitle('SVD Image Compression', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Singular value spectrum\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].semilogy(sigma, 'o-', color=COLORS['blue'], markersize=3)\n",
    "    axes[0].set_xlabel('Index')\n",
    "    axes[0].set_ylabel('Singular Value (log scale)')\n",
    "    axes[0].set_title('Singular Value Spectrum')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    cumulative_energy = np.cumsum(sigma ** 2) / np.sum(sigma ** 2)\n",
    "    axes[1].plot(cumulative_energy, color=COLORS['green'], linewidth=2)\n",
    "    axes[1].axhline(0.95, color=COLORS['red'], linestyle='--', label='95% energy')\n",
    "    axes[1].axhline(0.99, color=COLORS['orange'], linestyle='--', label='99% energy')\n",
    "    k_95 = np.searchsorted(cumulative_energy, 0.95) + 1\n",
    "    k_99 = np.searchsorted(cumulative_energy, 0.99) + 1\n",
    "    axes[1].axvline(k_95, color=COLORS['red'], linestyle=':', alpha=0.5)\n",
    "    axes[1].axvline(k_99, color=COLORS['orange'], linestyle=':', alpha=0.5)\n",
    "    axes[1].set_xlabel('Number of Singular Values')\n",
    "    axes[1].set_ylabel('Cumulative Energy')\n",
    "    axes[1].set_title(f'Cumulative Energy (95% at k={k_95}, 99% at k={k_99})')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "demonstrate_image_compression()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Special Matrices in ML",
    "",
    "Certain matrix types appear repeatedly in ML and have efficient decompositions:",
    "",
    "- **Symmetric positive definite (SPD):** Covariance matrices, kernel matrices,",
    "  Hessians. All eigenvalues are positive. Has Cholesky decomposition $\\mathbf{A} = \\mathbf{L}\\mathbf{L}^T$.",
    "- **Orthogonal:** $\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}$. Rotation matrices, eigenvectors of",
    "  symmetric matrices. Preserves lengths and angles.",
    "- **Diagonal:** Only diagonal entries are nonzero. Scaling operations.",
    "- **Sparse:** Most entries are zero. Adjacency matrices, bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_special_matrices() -> None:\n",
    "    \"\"\"Demonstrate special matrix types and the Cholesky decomposition.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    # Build a symmetric positive definite matrix\n",
    "    B = rng.randn(4, 4)\n",
    "    spd = B.T @ B + 0.1 * np.eye(4)  # Guaranteed SPD\n",
    "\n",
    "    # Cholesky decomposition: A = L L^T\n",
    "    L = np.linalg.cholesky(spd)\n",
    "    reconstructed = L @ L.T\n",
    "\n",
    "    print('=== Symmetric Positive Definite Matrix ===')\n",
    "    print(f'Eigenvalues: {np.linalg.eigvalsh(spd).round(4)}')\n",
    "    print(f'All positive: {np.all(np.linalg.eigvalsh(spd) > 0)}')\n",
    "    print()\n",
    "\n",
    "    print('=== Cholesky Decomposition (A = L L^T) ===')\n",
    "    print(f'L (lower triangular):')\n",
    "    print(L.round(4))\n",
    "    print(f'\\nL is lower triangular: {np.allclose(L, np.tril(L))}')\n",
    "    print(f'Reconstruction error: {np.linalg.norm(spd - reconstructed):.2e}')\n",
    "    print()\n",
    "\n",
    "    # Cholesky is 2x faster than general eigendecomposition\n",
    "    large_B = rng.randn(200, 200)\n",
    "    large_spd = large_B.T @ large_B + np.eye(200)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        np.linalg.cholesky(large_spd)\n",
    "    chol_time = time.perf_counter() - start\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        np.linalg.eigh(large_spd)\n",
    "    eigh_time = time.perf_counter() - start\n",
    "\n",
    "    print(f'200×200 SPD matrix (100 runs):')\n",
    "    print(f'  Cholesky: {chol_time*1000:.1f}ms')\n",
    "    print(f'  Eigendecomposition: {eigh_time*1000:.1f}ms')\n",
    "    print(f'  Cholesky speedup: {eigh_time/chol_time:.1f}×')\n",
    "    print()\n",
    "\n",
    "    # Orthogonal matrix check\n",
    "    Q, _ = np.linalg.qr(rng.randn(4, 4))\n",
    "    print('=== Orthogonal Matrix ===')\n",
    "    print(f'Q^T Q ≈ I: {np.allclose(Q.T @ Q, np.eye(4))}')\n",
    "    print(f'det(Q) = {np.linalg.det(Q):.4f} (±1 for orthogonal)')\n",
    "    print(f'Preserves norm: |Qx|/|x| = {np.linalg.norm(Q @ np.ones(4)) / np.linalg.norm(np.ones(4)):.6f}')\n",
    "\n",
    "\n",
    "demonstrate_special_matrices()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Orthogonal Projections",
    "",
    "Projection is the core operation behind PCA, least squares, and attention.",
    "The projection of $\\mathbf{b}$ onto the column space of $\\mathbf{A}$ is:",
    "",
    "$$\\text{proj}_{\\mathbf{A}}(\\mathbf{b}) = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T \\mathbf{b}$$",
    "",
    "The **projection matrix** $\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T$",
    "satisfies $\\mathbf{P}^2 = \\mathbf{P}$ (idempotent) and $\\mathbf{P}^T = \\mathbf{P}$ (symmetric)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_projections() -> None:\n",
    "    \"\"\"Demonstrate orthogonal projection and its properties.\"\"\"\n",
    "    # Project a 3D point onto a 2D plane\n",
    "    A = np.array([[1.0, 0.0], [0.0, 1.0], [0.0, 0.0]])  # xy-plane basis\n",
    "    b = np.array([3.0, 4.0, 5.0])\n",
    "\n",
    "    # Projection matrix\n",
    "    P = A @ np.linalg.inv(A.T @ A) @ A.T\n",
    "    proj_b = P @ b\n",
    "    residual = b - proj_b\n",
    "\n",
    "    print('=== Orthogonal Projection ===')\n",
    "    print(f'Point b = {b}')\n",
    "    print(f'Projection onto xy-plane: {proj_b}')\n",
    "    print(f'Residual (perpendicular): {residual}')\n",
    "    print(f'Residual ⊥ projection: dot = {proj_b @ residual:.10f}')\n",
    "    print()\n",
    "\n",
    "    # Properties of projection matrix\n",
    "    print('Projection matrix properties:')\n",
    "    print(f'  Idempotent (P² = P): {np.allclose(P @ P, P)}')\n",
    "    print(f'  Symmetric (P^T = P): {np.allclose(P.T, P)}')\n",
    "    print(f'  Rank = {np.linalg.matrix_rank(P)} (dimension of subspace)')\n",
    "    print()\n",
    "\n",
    "    # Connection to PCA: projecting data onto top-k eigenvectors\n",
    "    # is a special case of orthogonal projection\n",
    "    V_k = our_vecs[:, :1]  # Top eigenvector from earlier\n",
    "    P_pca = V_k @ V_k.T   # Rank-1 projection\n",
    "    X_proj_pca = (P_pca @ X_2d.T).T\n",
    "\n",
    "    print('PCA as projection:')\n",
    "    print(f'  Projection matrix rank: {np.linalg.matrix_rank(P_pca)}')\n",
    "    print(f'  P² = P: {np.allclose(P_pca @ P_pca, P_pca)}')\n",
    "    print(f'  Projected data shape: {X_proj_pca.shape}')\n",
    "    recon_error = np.linalg.norm(X_2d - X_proj_pca, 'fro') / np.linalg.norm(X_2d, 'fro')\n",
    "    print(f'  Reconstruction error: {recon_error:.4f}')\n",
    "\n",
    "\n",
    "demonstrate_projections()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 2 — Putting It All Together: PCA from Scratch",
    "",
    "Principal Component Analysis (PCA) is the most important application of",
    "eigendecomposition / SVD in ML. It finds the directions of maximum variance",
    "and projects data onto them for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class PCAFromScratch:\n",
    "    \"\"\"Principal Component Analysis implemented from scratch using SVD.\n",
    "\n",
    "    PCA finds orthogonal directions that maximize variance and projects\n",
    "    data onto the top-k of those directions.\n",
    "\n",
    "    Attributes:\n",
    "        n_components: Number of principal components to keep.\n",
    "        components: Principal component directions of shape (k, n_features).\n",
    "        explained_variance: Variance along each component.\n",
    "        explained_variance_ratio: Fraction of total variance per component.\n",
    "        mean: Feature means used for centering.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components: int = 2) -> None:\n",
    "        \"\"\"Initialize PCA.\n",
    "\n",
    "        Args:\n",
    "            n_components: Number of components to keep.\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.components: np.ndarray | None = None\n",
    "        self.explained_variance: np.ndarray | None = None\n",
    "        self.explained_variance_ratio: np.ndarray | None = None\n",
    "        self.mean: np.ndarray | None = None\n",
    "\n",
    "    def fit(self, X: np.ndarray) -> 'PCAFromScratch':\n",
    "        \"\"\"Fit PCA by computing SVD of the centered data.\n",
    "\n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Step 1: Center the data\n",
    "        self.mean = X.mean(axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Step 2: SVD of centered data\n",
    "        # X = U Σ V^T → covariance ∝ V Σ² V^T\n",
    "        U, sigma, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "\n",
    "        # Step 3: Top-k components\n",
    "        self.components = Vt[:self.n_components]\n",
    "\n",
    "        # Step 4: Explained variance\n",
    "        self.explained_variance = (sigma[:self.n_components] ** 2) / (n_samples - 1)\n",
    "        total_variance = np.sum(sigma ** 2) / (n_samples - 1)\n",
    "        self.explained_variance_ratio = self.explained_variance / total_variance\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Project data onto principal components.\n",
    "\n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Projected data of shape (n_samples, n_components).\n",
    "        \"\"\"\n",
    "        assert self.components is not None, 'Must call fit() first'\n",
    "        X_centered = X - self.mean\n",
    "        return X_centered @ self.components.T\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Fit and transform in one step.\n",
    "\n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Projected data of shape (n_samples, n_components).\n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    def inverse_transform(self, X_projected: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Reconstruct data from principal component space.\n",
    "\n",
    "        Args:\n",
    "            X_projected: Projected data of shape (n_samples, n_components).\n",
    "\n",
    "        Returns:\n",
    "            Reconstructed data of shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        assert self.components is not None, 'Must call fit() first'\n",
    "        return X_projected @ self.components + self.mean\n",
    "\n",
    "\n",
    "# Sanity check on 2D data\n",
    "pca_2d = PCAFromScratch(n_components=1)\n",
    "X_2d_proj = pca_2d.fit_transform(X_2d)\n",
    "X_2d_recon = pca_2d.inverse_transform(X_2d_proj)\n",
    "\n",
    "assert X_2d_proj.shape == (300, 1), f'Expected (300, 1), got {X_2d_proj.shape}'\n",
    "print(f'2D data: {X_2d.shape} → projected: {X_2d_proj.shape} → reconstructed: {X_2d_recon.shape}')\n",
    "print(f'Variance explained: {pca_2d.explained_variance_ratio[0]:.4f} ({pca_2d.explained_variance_ratio[0]*100:.1f}%)')\n",
    "print(f'Reconstruction error: {np.linalg.norm(X_2d - X_2d_recon):.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the PCA projection and reconstruction on our 2D data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_pca_2d(X: np.ndarray, X_recon: np.ndarray, pca: PCAFromScratch) -> None:\n",
    "    \"\"\"Visualize PCA projection and reconstruction on 2D data.\n",
    "\n",
    "    Args:\n",
    "        X: Original data of shape (n, 2).\n",
    "        X_recon: Reconstructed data of shape (n, 2).\n",
    "        pca: Fitted PCAFromScratch instance.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Left: original + PC direction + projections\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], s=10, alpha=0.4, color=COLORS['blue'], label='Original')\n",
    "    axes[0].scatter(X_recon[:, 0], X_recon[:, 1], s=10, alpha=0.4,\n",
    "                     color=COLORS['red'], label='Projected onto PC1')\n",
    "\n",
    "    # Draw lines from original to projection\n",
    "    for i in range(0, len(X), 10):\n",
    "        axes[0].plot([X[i, 0], X_recon[i, 0]], [X[i, 1], X_recon[i, 1]],\n",
    "                      color=COLORS['gray'], alpha=0.2, linewidth=0.5)\n",
    "\n",
    "    # Draw PC direction\n",
    "    mean = pca.mean\n",
    "    pc = pca.components[0]\n",
    "    scale = 4\n",
    "    axes[0].annotate('', xy=mean + pc * scale, xytext=mean - pc * scale,\n",
    "                      arrowprops=dict(arrowstyle='->', color=COLORS['red'], lw=2))\n",
    "\n",
    "    axes[0].set_xlabel('$x_1$')\n",
    "    axes[0].set_ylabel('$x_2$')\n",
    "    axes[0].set_title('PCA: Projection onto 1st Principal Component')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Right: reconstruction error distribution\n",
    "    errors = np.linalg.norm(X - X_recon, axis=1)\n",
    "    axes[1].hist(errors, bins=30, color=COLORS['purple'], alpha=0.7, edgecolor='white')\n",
    "    axes[1].axvline(errors.mean(), color=COLORS['red'], linestyle='--',\n",
    "                     label=f'Mean error: {errors.mean():.3f}')\n",
    "    axes[1].set_xlabel('Reconstruction Error')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Reconstruction Error Distribution')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_pca_2d(X_2d, X_2d_recon, pca_2d)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 3 — Application: PCA on California Housing",
    "",
    "Let's apply our PCA implementation to a real dataset with 8 features",
    "and compare against sklearn's implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def apply_pca_housing() -> None:\n",
    "    \"\"\"Apply PCA to California Housing dataset and analyze results.\"\"\"\n",
    "    # Our PCA\n",
    "    pca_ours = PCAFromScratch(n_components=8)\n",
    "    X_proj_ours = pca_ours.fit_transform(X_scaled)\n",
    "\n",
    "    # sklearn PCA\n",
    "    pca_sklearn = SklearnPCA(n_components=8)\n",
    "    X_proj_sklearn = pca_sklearn.fit_transform(X_scaled)\n",
    "\n",
    "    # Compare explained variance ratios\n",
    "    comparison = pd.DataFrame({\n",
    "        'Component': [f'PC{i+1}' for i in range(8)],\n",
    "        'Ours': pca_ours.explained_variance_ratio.round(6),\n",
    "        'sklearn': pca_sklearn.explained_variance_ratio_.round(6),\n",
    "        'Match': np.isclose(pca_ours.explained_variance_ratio,\n",
    "                            pca_sklearn.explained_variance_ratio_, atol=1e-4),\n",
    "    })\n",
    "    print('=== Explained Variance Ratio Comparison ===')\n",
    "    print(comparison.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Cumulative variance plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Scree plot\n",
    "    axes[0].bar(range(1, 9), pca_ours.explained_variance_ratio * 100,\n",
    "                color=COLORS['blue'], alpha=0.7)\n",
    "    axes[0].set_xlabel('Principal Component')\n",
    "    axes[0].set_ylabel('Explained Variance (%)')\n",
    "    axes[0].set_title('Scree Plot')\n",
    "    axes[0].set_xticks(range(1, 9))\n",
    "    for i, v in enumerate(pca_ours.explained_variance_ratio):\n",
    "        axes[0].text(i + 1, v * 100 + 0.5, f'{v*100:.1f}%', ha='center', fontsize=8)\n",
    "\n",
    "    # Cumulative\n",
    "    cumvar = np.cumsum(pca_ours.explained_variance_ratio)\n",
    "    axes[1].plot(range(1, 9), cumvar * 100, 'o-', color=COLORS['green'], linewidth=2)\n",
    "    axes[1].axhline(95, color=COLORS['red'], linestyle='--', label='95% threshold')\n",
    "    k_95 = np.searchsorted(cumvar, 0.95) + 1\n",
    "    axes[1].axvline(k_95, color=COLORS['red'], linestyle=':', alpha=0.5)\n",
    "    axes[1].set_xlabel('Number of Components')\n",
    "    axes[1].set_ylabel('Cumulative Explained Variance (%)')\n",
    "    axes[1].set_title(f'Cumulative Variance (95% at k={k_95})')\n",
    "    axes[1].set_xticks(range(1, 9))\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Components needed for 95% variance: {k_95} (out of 8)')\n",
    "    print(f'Dimensionality reduction: {8} → {k_95} ({k_95/8*100:.0f}% of original features)')\n",
    "\n",
    "\n",
    "apply_pca_housing()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PCA for Visualization: 2D Projection",
    "",
    "One of PCA's most common uses is projecting high-dimensional data to 2D",
    "for visualization. Let's visualize the housing data colored by price."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_pca_projection() -> None:\n",
    "    \"\"\"Project California Housing to 2D with PCA and visualize.\"\"\"\n",
    "    pca_2comp = PCAFromScratch(n_components=2)\n",
    "    X_2d_housing = pca_2comp.fit_transform(X_scaled)\n",
    "    assert X_2d_housing.shape == (len(X_scaled), 2), f'Expected (n, 2), got {X_2d_housing.shape}'\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Colored by target (house price)\n",
    "    sc = axes[0].scatter(\n",
    "        X_2d_housing[:, 0], X_2d_housing[:, 1],\n",
    "        c=y_housing, cmap='viridis', s=3, alpha=0.4,\n",
    "    )\n",
    "    axes[0].set_xlabel(f'PC1 ({pca_2comp.explained_variance_ratio[0]*100:.1f}%)')\n",
    "    axes[0].set_ylabel(f'PC2 ({pca_2comp.explained_variance_ratio[1]*100:.1f}%)')\n",
    "    axes[0].set_title('PCA Projection (color = house price)')\n",
    "    plt.colorbar(sc, ax=axes[0], shrink=0.8, label='Median House Value')\n",
    "\n",
    "    # Component loadings\n",
    "    loadings = pca_2comp.components.T  # (n_features, n_components)\n",
    "    for i, name in enumerate(feature_names):\n",
    "        axes[1].arrow(0, 0, loadings[i, 0], loadings[i, 1],\n",
    "                       head_width=0.03, color=COLORS['blue'], alpha=0.7)\n",
    "        axes[1].text(loadings[i, 0] * 1.15, loadings[i, 1] * 1.15,\n",
    "                      name, fontsize=9, ha='center')\n",
    "\n",
    "    # Draw unit circle for reference\n",
    "    theta = np.linspace(0, 2 * np.pi, 100)\n",
    "    axes[1].plot(np.cos(theta), np.sin(theta), '--', color=COLORS['gray'], alpha=0.3)\n",
    "    axes[1].set_xlabel('PC1 Loading')\n",
    "    axes[1].set_ylabel('PC2 Loading')\n",
    "    axes[1].set_title('Feature Loadings (Biplot)')\n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_xlim(-1.2, 1.2)\n",
    "    axes[1].set_ylim(-1.2, 1.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_pca_projection()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 PCA for Noise Reduction",
    "",
    "Low-rank approximation can denoise data. The signal lives in the top",
    "principal components, while noise spreads across all components."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_pca_denoising() -> None:\n",
    "    \"\"\"Show PCA denoising on synthetic data.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    # Create clean low-rank data\n",
    "    n_samples = 200\n",
    "    true_dim = 3\n",
    "    n_features = 10\n",
    "\n",
    "    # Generate from a 3D subspace\n",
    "    latent = rng.randn(n_samples, true_dim)\n",
    "    mixing = rng.randn(true_dim, n_features)\n",
    "    X_clean = latent @ mixing\n",
    "\n",
    "    # Add noise\n",
    "    noise_level = 1.0\n",
    "    noise = rng.randn(n_samples, n_features) * noise_level\n",
    "    X_noisy = X_clean + noise\n",
    "\n",
    "    # Denoise with PCA at different ranks\n",
    "    pca_full = PCAFromScratch(n_components=n_features)\n",
    "    pca_full.fit(X_noisy)\n",
    "\n",
    "    errors = []\n",
    "    for k in range(1, n_features + 1):\n",
    "        pca_k = PCAFromScratch(n_components=k)\n",
    "        X_proj = pca_k.fit_transform(X_noisy)\n",
    "        X_recon = pca_k.inverse_transform(X_proj)\n",
    "        error = np.linalg.norm(X_clean - X_recon, 'fro') / np.linalg.norm(X_clean, 'fro')\n",
    "        errors.append(error)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Denoising error vs rank\n",
    "    axes[0].plot(range(1, n_features + 1), errors, 'o-', color=COLORS['blue'], linewidth=2)\n",
    "    axes[0].axvline(true_dim, color=COLORS['red'], linestyle='--',\n",
    "                     label=f'True dimension = {true_dim}')\n",
    "    axes[0].set_xlabel('Number of Components')\n",
    "    axes[0].set_ylabel('Relative Error vs Clean Data')\n",
    "    axes[0].set_title('PCA Denoising: Error vs Components')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    best_k = np.argmin(errors) + 1\n",
    "    axes[0].annotate(f'Best: k={best_k}', xy=(best_k, errors[best_k - 1]),\n",
    "                      xytext=(best_k + 2, errors[best_k - 1] + 0.1),\n",
    "                      arrowprops=dict(arrowstyle='->', color=COLORS['green']),\n",
    "                      fontsize=10, color=COLORS['green'])\n",
    "\n",
    "    # Explained variance (scree plot)\n",
    "    axes[1].bar(range(1, n_features + 1), pca_full.explained_variance_ratio * 100,\n",
    "                color=COLORS['purple'], alpha=0.7)\n",
    "    axes[1].axvline(true_dim + 0.5, color=COLORS['red'], linestyle='--',\n",
    "                     label='Signal / Noise boundary')\n",
    "    axes[1].set_xlabel('Principal Component')\n",
    "    axes[1].set_ylabel('Explained Variance (%)')\n",
    "    axes[1].set_title('Scree Plot (gap reveals true dimensionality)')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'True dimensionality: {true_dim}')\n",
    "    print(f'Best denoising rank: {best_k}')\n",
    "    print(f'Variance in top {true_dim} components: '\n",
    "          f'{pca_full.explained_variance_ratio[:true_dim].sum()*100:.1f}%')\n",
    "\n",
    "\n",
    "demonstrate_pca_denoising()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 4 — Evaluation & Analysis",
    "",
    "Let's evaluate our implementations, analyze numerical stability, and",
    "benchmark performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Numerical Stability: Condition Numbers",
    "",
    "The **condition number** $\\kappa(\\mathbf{A}) = \\sigma_{\\max} / \\sigma_{\\min}$",
    "measures how sensitive a matrix computation is to small perturbations.",
    "Large condition numbers mean numerical instability."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_condition_numbers() -> None:\n",
    "    \"\"\"Demonstrate condition numbers and their effect on computations.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    # Well-conditioned matrix\n",
    "    A_good = rng.randn(5, 5)\n",
    "    A_good = A_good.T @ A_good + 0.1 * np.eye(5)  # Positive definite\n",
    "\n",
    "    # Ill-conditioned matrix (nearly singular)\n",
    "    A_bad = A_good.copy()\n",
    "    A_bad[4, :] = A_bad[3, :] + 1e-10 * rng.randn(5)  # Nearly duplicate row\n",
    "    A_bad = A_bad.T @ A_bad\n",
    "\n",
    "    kappa_good = np.linalg.cond(A_good)\n",
    "    kappa_bad = np.linalg.cond(A_bad)\n",
    "\n",
    "    print('=== Condition Number Analysis ===')\n",
    "    print(f'Well-conditioned matrix: κ = {kappa_good:.2f}')\n",
    "    print(f'Ill-conditioned matrix:  κ = {kappa_bad:.2e}')\n",
    "    print()\n",
    "\n",
    "    # Show impact: solving Ax = b with small perturbation\n",
    "    x_true = np.ones(5)\n",
    "    b_good = A_good @ x_true\n",
    "    b_bad = A_bad @ x_true\n",
    "\n",
    "    # Add tiny perturbation to b\n",
    "    perturbation = rng.randn(5) * 1e-10\n",
    "\n",
    "    x_good = np.linalg.solve(A_good, b_good + perturbation)\n",
    "    x_bad = np.linalg.solve(A_bad, b_bad + perturbation)\n",
    "\n",
    "    error_good = np.linalg.norm(x_good - x_true) / np.linalg.norm(x_true)\n",
    "    error_bad = np.linalg.norm(x_bad - x_true) / np.linalg.norm(x_true)\n",
    "\n",
    "    print(f'Effect of 1e-10 perturbation on solution:')\n",
    "    print(f'  Well-conditioned: relative error = {error_good:.2e}')\n",
    "    print(f'  Ill-conditioned:  relative error = {error_bad:.2e}')\n",
    "    print()\n",
    "\n",
    "    # Condition number reference table\n",
    "    ref = pd.DataFrame({\n",
    "        'Condition Number': ['κ ≈ 1', 'κ ≈ 10²', 'κ ≈ 10⁶', 'κ > 10¹⁰'],\n",
    "        'Stability': ['Excellent', 'Good', 'Concerning', 'Unstable'],\n",
    "        'ML Example': [\n",
    "            'Orthogonal features',\n",
    "            'Mildly correlated features',\n",
    "            'Highly correlated features',\n",
    "            'Multicollinear / near-singular',\n",
    "        ],\n",
    "        'Action': [\n",
    "            'No issues',\n",
    "            'Normal',\n",
    "            'Consider regularization or PCA',\n",
    "            'Must regularize or remove features',\n",
    "        ],\n",
    "    })\n",
    "    print('=== Condition Number Reference ===')\n",
    "    print(ref.to_string(index=False))\n",
    "\n",
    "\n",
    "analyze_condition_numbers()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PCA Impact on Downstream Tasks",
    "",
    "Let's measure how PCA dimensionality reduction affects prediction quality",
    "using a simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_pca_downstream() -> None:\n",
    "    \"\"\"Evaluate PCA's impact on downstream regression performance.\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_housing, test_size=0.2, random_state=SEED,\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Baseline: all features\n",
    "    model_full = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred_full = model_full.predict(X_test)\n",
    "    results.append({\n",
    "        'Components': 'All (8)',\n",
    "        'R²': r2_score(y_test, y_pred_full),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_full)),\n",
    "        'Features': 8,\n",
    "    })\n",
    "\n",
    "    # PCA at different component counts\n",
    "    for k in [1, 2, 3, 4, 5, 6, 7]:\n",
    "        pca_k = PCAFromScratch(n_components=k)\n",
    "        X_train_pca = pca_k.fit_transform(X_train)\n",
    "        X_test_pca = pca_k.transform(X_test)\n",
    "\n",
    "        model_k = LinearRegression().fit(X_train_pca, y_train)\n",
    "        y_pred_k = model_k.predict(X_test_pca)\n",
    "        results.append({\n",
    "            'Components': f'PCA-{k}',\n",
    "            'R²': r2_score(y_test, y_pred_k),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_k)),\n",
    "            'Features': k,\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['R²'] = results_df['R²'].round(4)\n",
    "    results_df['RMSE'] = results_df['RMSE'].round(4)\n",
    "    print('=== PCA Impact on Linear Regression ===')\n",
    "    print(results_df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    components = list(range(1, 8)) + [8]\n",
    "    r2_scores = results_df['R²'].values\n",
    "    rmse_scores = results_df['RMSE'].values\n",
    "\n",
    "    axes[0].plot(components, r2_scores, 'o-', color=COLORS['blue'], linewidth=2)\n",
    "    axes[0].axhline(r2_scores[-1], color=COLORS['red'], linestyle='--',\n",
    "                     alpha=0.5, label=f'Full model: {r2_scores[-1]:.4f}')\n",
    "    axes[0].set_xlabel('Number of Components')\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_title('R² vs PCA Components')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(components, rmse_scores, 'o-', color=COLORS['green'], linewidth=2)\n",
    "    axes[1].set_xlabel('Number of Components')\n",
    "    axes[1].set_ylabel('RMSE')\n",
    "    axes[1].set_title('RMSE vs PCA Components')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "evaluate_pca_downstream()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance Benchmarks",
    "",
    "Let's compare our implementations against NumPy/sklearn on different",
    "matrix sizes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def benchmark_decompositions() -> None:\n",
    "    \"\"\"Benchmark our implementations vs library implementations.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    results = []\n",
    "\n",
    "    for n in [50, 100, 500, 1000]:\n",
    "        X = rng.randn(n, min(n, 50))\n",
    "        cov = X.T @ X / n\n",
    "\n",
    "        # Our eigendecomposition (power iteration)\n",
    "        start = time.perf_counter()\n",
    "        eigendecomposition_power(cov, num_components=min(5, cov.shape[0]))\n",
    "        our_eig_time = time.perf_counter() - start\n",
    "\n",
    "        # NumPy eigendecomposition\n",
    "        start = time.perf_counter()\n",
    "        np.linalg.eigh(cov)\n",
    "        np_eig_time = time.perf_counter() - start\n",
    "\n",
    "        # Our PCA\n",
    "        start = time.perf_counter()\n",
    "        pca_ours = PCAFromScratch(n_components=5)\n",
    "        pca_ours.fit_transform(X)\n",
    "        our_pca_time = time.perf_counter() - start\n",
    "\n",
    "        # sklearn PCA\n",
    "        start = time.perf_counter()\n",
    "        pca_sk = SklearnPCA(n_components=5)\n",
    "        pca_sk.fit_transform(X)\n",
    "        sk_pca_time = time.perf_counter() - start\n",
    "\n",
    "        results.append({\n",
    "            'Matrix Size': f'{n}×{min(n, 50)}',\n",
    "            'Our Eigen': f'{our_eig_time*1000:.2f}ms',\n",
    "            'NumPy Eigen': f'{np_eig_time*1000:.2f}ms',\n",
    "            'Our PCA': f'{our_pca_time*1000:.2f}ms',\n",
    "            'sklearn PCA': f'{sk_pca_time*1000:.2f}ms',\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print('=== Decomposition Benchmarks ===')\n",
    "    print(results_df.to_string(index=False))\n",
    "    print()\n",
    "    print('NumPy uses LAPACK (Fortran) — optimized for decades.')\n",
    "    print('Our implementations are for learning, not production.')\n",
    "\n",
    "\n",
    "benchmark_decompositions()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Error Analysis: When PCA Fails",
    "",
    "PCA assumes linear relationships and is sensitive to outliers. Let's",
    "demonstrate cases where PCA doesn't capture the true structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_pca_limitations() -> None:\n",
    "    \"\"\"Show cases where PCA fails to capture data structure.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "    # Case 1: PCA works well — elliptical data\n",
    "    X_ellipse = rng.multivariate_normal([0, 0], [[3, 1.5], [1.5, 1]], 300)\n",
    "    pca_e = PCAFromScratch(n_components=1)\n",
    "    X_proj = pca_e.fit_transform(X_ellipse)\n",
    "    X_recon = pca_e.inverse_transform(X_proj)\n",
    "    axes[0, 0].scatter(X_ellipse[:, 0], X_ellipse[:, 1], s=10, alpha=0.5, color=COLORS['blue'])\n",
    "    axes[0, 0].scatter(X_recon[:, 0], X_recon[:, 1], s=10, alpha=0.5, color=COLORS['red'])\n",
    "    axes[0, 0].set_title(f'Elliptical: PC1 captures {pca_e.explained_variance_ratio[0]*100:.0f}% ✓')\n",
    "    axes[0, 0].set_aspect('equal')\n",
    "\n",
    "    # Case 2: PCA fails — circular data (no direction is better)\n",
    "    theta = rng.uniform(0, 2 * np.pi, 300)\n",
    "    r = 2 + rng.randn(300) * 0.3\n",
    "    X_circle = np.column_stack([r * np.cos(theta), r * np.sin(theta)])\n",
    "    pca_c = PCAFromScratch(n_components=1)\n",
    "    X_proj_c = pca_c.fit_transform(X_circle)\n",
    "    X_recon_c = pca_c.inverse_transform(X_proj_c)\n",
    "    axes[0, 1].scatter(X_circle[:, 0], X_circle[:, 1], s=10, alpha=0.5, color=COLORS['blue'])\n",
    "    axes[0, 1].scatter(X_recon_c[:, 0], X_recon_c[:, 1], s=10, alpha=0.5, color=COLORS['red'])\n",
    "    axes[0, 1].set_title(f'Circular: PC1 captures {pca_c.explained_variance_ratio[0]*100:.0f}% ✗')\n",
    "    axes[0, 1].set_aspect('equal')\n",
    "\n",
    "    # Case 3: PCA fails — moon-shaped data\n",
    "    from sklearn.datasets import make_moons\n",
    "    X_moon, _ = make_moons(n_samples=300, noise=0.1, random_state=SEED)\n",
    "    pca_m = PCAFromScratch(n_components=1)\n",
    "    X_proj_m = pca_m.fit_transform(X_moon)\n",
    "    X_recon_m = pca_m.inverse_transform(X_proj_m)\n",
    "    axes[0, 2].scatter(X_moon[:, 0], X_moon[:, 1], s=10, alpha=0.5, color=COLORS['blue'])\n",
    "    axes[0, 2].scatter(X_recon_m[:, 0], X_recon_m[:, 1], s=10, alpha=0.5, color=COLORS['red'])\n",
    "    axes[0, 2].set_title(f'Moons: PC1 captures {pca_m.explained_variance_ratio[0]*100:.0f}% (misleading) ✗')\n",
    "    axes[0, 2].set_aspect('equal')\n",
    "\n",
    "    # Case 4: Outlier sensitivity\n",
    "    X_clean = rng.multivariate_normal([0, 0], [[2, 1], [1, 1]], 300)\n",
    "    X_outlier = X_clean.copy()\n",
    "    X_outlier[:5] = rng.uniform(8, 12, (5, 2))  # Add 5 extreme outliers\n",
    "\n",
    "    pca_clean = PCAFromScratch(n_components=2)\n",
    "    pca_clean.fit(X_clean)\n",
    "    pca_outlier = PCAFromScratch(n_components=2)\n",
    "    pca_outlier.fit(X_outlier)\n",
    "\n",
    "    axes[1, 0].scatter(X_clean[:, 0], X_clean[:, 1], s=10, alpha=0.5, color=COLORS['blue'])\n",
    "    pc_clean = pca_clean.components[0]\n",
    "    axes[1, 0].annotate('', xy=pc_clean * 3, xytext=-pc_clean * 3,\n",
    "                          arrowprops=dict(arrowstyle='->', color=COLORS['red'], lw=2))\n",
    "    axes[1, 0].set_title('Clean Data: PC1 direction')\n",
    "    axes[1, 0].set_aspect('equal')\n",
    "\n",
    "    axes[1, 1].scatter(X_outlier[:, 0], X_outlier[:, 1], s=10, alpha=0.5, color=COLORS['blue'])\n",
    "    axes[1, 1].scatter(X_outlier[:5, 0], X_outlier[:5, 1], s=50, color=COLORS['red'],\n",
    "                         marker='x', linewidths=2, label='Outliers')\n",
    "    pc_out = pca_outlier.components[0]\n",
    "    axes[1, 1].annotate('', xy=pc_out * 3, xytext=-pc_out * 3,\n",
    "                          arrowprops=dict(arrowstyle='->', color=COLORS['red'], lw=2))\n",
    "    axes[1, 1].set_title('With Outliers: PC1 direction shifted!')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].set_aspect('equal')\n",
    "\n",
    "    # Angle difference\n",
    "    angle = np.degrees(np.arccos(np.clip(np.abs(pc_clean @ pc_out), 0, 1)))\n",
    "    axes[1, 2].bar(['Clean', 'With Outliers'], [0, angle],\n",
    "                    color=[COLORS['green'], COLORS['red']], alpha=0.7)\n",
    "    axes[1, 2].set_ylabel('PC1 Direction Shift (degrees)')\n",
    "    axes[1, 2].set_title(f'Outlier Impact: {angle:.1f}° rotation')\n",
    "\n",
    "    for ax in axes.ravel():\n",
    "        ax.grid(True, alpha=0.2)\n",
    "\n",
    "    plt.suptitle('When PCA Fails', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('PCA limitations:')\n",
    "    limitations = pd.DataFrame({\n",
    "        'Limitation': [\n",
    "            'Assumes linear structure',\n",
    "            'Sensitive to outliers',\n",
    "            'Sensitive to feature scaling',\n",
    "            'Variance ≠ importance',\n",
    "        ],\n",
    "        'When It Matters': [\n",
    "            'Non-linear manifolds (circles, moons)',\n",
    "            'Extreme values pull PC directions',\n",
    "            'Features with different units/scales',\n",
    "            'High-variance noise vs low-variance signal',\n",
    "        ],\n",
    "        'Solution': [\n",
    "            'Kernel PCA, t-SNE, UMAP (Module 3)',\n",
    "            'Robust PCA, remove outliers first',\n",
    "            'Always standardize before PCA',\n",
    "            'Domain knowledge, supervised methods',\n",
    "        ],\n",
    "    })\n",
    "    print(limitations.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_pca_limitations()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Linear Algebra Cheat Sheet for ML",
    "",
    "A comprehensive reference of the linear algebra operations used throughout",
    "this course."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_linalg_reference() -> None:\n",
    "    \"\"\"Create a linear algebra operations reference for the course.\"\"\"\n",
    "    reference = pd.DataFrame({\n",
    "        'Operation': [\n",
    "            'Matrix multiply', 'Transpose', 'Inverse',\n",
    "            'Determinant', 'Trace', 'Eigendecomposition',\n",
    "            'SVD', 'Pseudoinverse', 'Condition number',\n",
    "            'Frobenius norm', 'Rank',\n",
    "        ],\n",
    "        'NumPy': [\n",
    "            'A @ B', 'A.T', 'np.linalg.inv(A)',\n",
    "            'np.linalg.det(A)', 'np.trace(A)', 'np.linalg.eigh(A)',\n",
    "            'np.linalg.svd(A)', 'np.linalg.pinv(A)', 'np.linalg.cond(A)',\n",
    "            'np.linalg.norm(A, \"fro\")', 'np.linalg.matrix_rank(A)',\n",
    "        ],\n",
    "        'PyTorch': [\n",
    "            'A @ B', 'A.T / A.mT', 'torch.linalg.inv(A)',\n",
    "            'torch.linalg.det(A)', 'torch.trace(A)', 'torch.linalg.eigh(A)',\n",
    "            'torch.linalg.svd(A)', 'torch.linalg.pinv(A)', 'torch.linalg.cond(A)',\n",
    "            'torch.linalg.norm(A)', 'torch.linalg.matrix_rank(A)',\n",
    "        ],\n",
    "        'Used In': [\n",
    "            'Neural nets, attention', 'Gradients, covariance',\n",
    "            'Analytical solutions', 'Volume change',\n",
    "            'Regularization', 'PCA, spectral methods',\n",
    "            'PCA, compression', 'Least squares',\n",
    "            'Numerical stability', 'Matrix complexity',\n",
    "            'Intrinsic dimensionality',\n",
    "        ],\n",
    "    })\n",
    "    print('=== Linear Algebra for ML — Reference ===')\n",
    "    print(reference.to_string(index=False))\n",
    "\n",
    "\n",
    "build_linalg_reference()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 5 — Summary & Lessons Learned",
    "",
    "### Key Takeaways",
    "",
    "1. **Matrix decompositions reveal hidden structure.** Eigendecomposition shows",
    "   the principal axes of variation in symmetric matrices (like covariance).",
    "   SVD generalizes this to any matrix.",
    "",
    "2. **SVD is the Swiss Army knife of linear algebra.** It enables PCA,",
    "   low-rank approximation, pseudoinverses, and noise reduction — all",
    "   from one decomposition.",
    "",
    "3. **PCA = SVD of centered data.** Project onto the top-$k$ right singular",
    "   vectors to reduce dimensionality while preserving maximum variance.",
    "   Always standardize features before PCA.",
    "",
    "4. **Low-rank approximation is optimal.** The Eckart-Young theorem guarantees",
    "   that truncated SVD gives the best rank-$k$ approximation in Frobenius norm.",
    "   This powers compression, denoising, and dimensionality reduction.",
    "",
    "5. **Check condition numbers for stability.** Large condition numbers",
    "   ($\\kappa > 10^6$) signal that computations may be numerically unstable.",
    "   Regularization (adding $\\lambda \\mathbf{I}$) or PCA can help.",
    "",
    "### What's Next",
    "",
    "→ **01-07 (Probability & Statistics for ML)** covers distributions, MLE,",
    "  MAP estimation, and Bayes' theorem — the probabilistic foundation for",
    "  understanding loss functions and generalization.",
    "",
    "### Going Further",
    "",
    "- [Linear Algebra Done Right (Axler)](https://linear.axler.net/) — Rigorous",
    "  treatment of eigenvalues, singular values, and spectral theory",
    "- [Matrix Computations (Golub & Van Loan)](https://www.cs.cornell.edu/cv/GVL4/) —",
    "  The definitive reference for numerical linear algebra",
    "- [Essence of Linear Algebra (3Blue1Brown)](https://www.3blue1brown.com/topics/linear-algebra) —",
    "  Superb geometric intuition for all concepts in this notebook"
   ]
  }
 ]
}