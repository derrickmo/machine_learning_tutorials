{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations",
    "## 01-05: Data Loading with PyTorch",
    "",
    "**Objective:** Master PyTorch's Dataset/DataLoader pipeline — the standard",
    "interface for feeding data into neural networks efficiently and reproducibly.",
    "",
    "**Prerequisites:** 01-01 (Python, NumPy & Tensor Speed), 01-02 (Advanced NumPy & PyTorch Operations), 01-03 (Pandas for Tabular Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 0 — Setup & Prerequisites",
    "",
    "Every deep learning pipeline starts with data loading. PyTorch provides a",
    "clean two-component abstraction:",
    "",
    "- **`Dataset`** — defines how to access individual samples (mapping from index to data)",
    "- **`DataLoader`** — handles batching, shuffling, and parallel loading",
    "",
    "This notebook teaches:",
    "- **Map-style Datasets** — subclass `Dataset` with `__len__` + `__getitem__`",
    "- **Transforms** — composable data preprocessing pipelines",
    "- **DataLoader mechanics** — batching, shuffling, collation, worker processes",
    "- **Splitting strategies** — train/val/test splits with reproducible seeds",
    "- **Built-in datasets** — torchvision, torchtext, torchaudio ecosystem",
    "",
    "We use FashionMNIST and synthetic data to demonstrate all concepts.",
    "",
    "**Prerequisites:** 01-01, 01-02, 01-03 (Pandas for Tabular Data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, TensorDataset, Subset,\n",
    "    random_split, ConcatDataset, StackDataset,\n",
    ")\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Torchvision: {torchvision.__version__}')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA: {torch.version.cuda}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "import random\n",
    "\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0          # 0 for Colab compatibility\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "DATA_DIR = '../data'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading",
    "",
    "We download FashionMNIST — a standard image classification dataset — and",
    "load the California Housing dataset for tabular examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FashionMNIST — built-in torchvision dataset\n",
    "fashion_train = torchvision.datasets.FashionMNIST(\n",
    "    root=DATA_DIR, train=True, download=True,\n",
    "    transform=T.ToTensor(),\n",
    ")\n",
    "fashion_test = torchvision.datasets.FashionMNIST(\n",
    "    root=DATA_DIR, train=False, download=True,\n",
    "    transform=T.ToTensor(),\n",
    ")\n",
    "\n",
    "CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f'FashionMNIST train: {len(fashion_train)} samples')\n",
    "print(f'FashionMNIST test:  {len(fashion_test)} samples')\n",
    "print(f'Image shape: {fashion_train[0][0].shape}')\n",
    "print(f'Label range: {min(t for _, t in fashion_train)} - {max(t for _, t in fashion_train)}')\n",
    "\n",
    "# California Housing — tabular data\n",
    "housing = fetch_california_housing()\n",
    "housing_X = housing.data.astype(np.float32)\n",
    "housing_y = housing.target.astype(np.float32)\n",
    "print(f'\\nCalifornia Housing: {housing_X.shape[0]} samples, {housing_X.shape[1]} features')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick EDA: FashionMNIST Samples",
    "",
    "Let's visualize sample images and the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def show_fashion_eda(dataset: Dataset, class_names: list[str]) -> None:\n",
    "    \"\"\"Display sample images and class distribution for FashionMNIST.\n",
    "\n",
    "    Args:\n",
    "        dataset: FashionMNIST dataset.\n",
    "        class_names: List of class label names.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "    # Show sample images\n",
    "    for idx in range(8):\n",
    "        img, label = dataset[idx]\n",
    "        axes[0, idx].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[0, idx].set_title(class_names[label], fontsize=8)\n",
    "        axes[0, idx].axis('off')\n",
    "\n",
    "    # Show random images\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    random_indices = rng.choice(len(dataset), 8, replace=False)\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        img, label = dataset[idx]\n",
    "        axes[1, i].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[1, i].set_title(class_names[label], fontsize=8)\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.suptitle('FashionMNIST — Sample Images', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Class distribution\n",
    "    labels = [t for _, t in dataset]\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    counts = pd.Series(labels).value_counts().sort_index()\n",
    "    ax.bar(range(10), counts.values, color='#1E88E5', alpha=0.7)\n",
    "    ax.set_xticks(range(10))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Class Distribution')\n",
    "    for i, c in enumerate(counts.values):\n",
    "        ax.text(i, c + 100, str(c), ha='center', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_fashion_eda(fashion_train, CLASS_NAMES)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 1 — Dataset & DataLoader from Scratch",
    "",
    "Before using PyTorch's built-in abstractions, let's understand the pattern",
    "by building it ourselves. The key insight is that data loading has two",
    "orthogonal concerns:",
    "",
    "1. **Data access** — given an index $i$, return the $i$-th sample $(\\mathbf{x}_i, y_i)$",
    "2. **Batch construction** — group samples into batches, shuffle order, handle last incomplete batch",
    "",
    "PyTorch's `Dataset` handles concern 1, and `DataLoader` handles concern 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Building a Dataset from Scratch",
    "",
    "A PyTorch Dataset must implement two methods:",
    "- `__len__()` — returns the total number of samples",
    "- `__getitem__(index)` — returns one sample (features, label) by index",
    "",
    "This simple protocol lets PyTorch handle everything else."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    \"\"\"A simple Dataset wrapping NumPy arrays.\n",
    "\n",
    "    This is the most common pattern for tabular data: store features and\n",
    "    labels as NumPy arrays, convert to tensors on access.\n",
    "\n",
    "    Attributes:\n",
    "        features: Feature matrix as float32 tensor.\n",
    "        labels: Label vector as float32 tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize from NumPy arrays.\n",
    "\n",
    "        Args:\n",
    "            features: Feature matrix of shape (n_samples, n_features).\n",
    "            labels: Label vector of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of samples.\"\"\"\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return (features, label) for the given index.\n",
    "\n",
    "        Args:\n",
    "            index: Sample index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (feature_vector, label).\n",
    "        \"\"\"\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "\n",
    "# Test with California Housing\n",
    "housing_dataset = NumpyDataset(housing_X, housing_y)\n",
    "print(f'Dataset length: {len(housing_dataset)}')\n",
    "sample_x, sample_y = housing_dataset[0]\n",
    "assert sample_x.shape == (8,), f'Expected (8,), got {sample_x.shape}'\n",
    "assert sample_y.shape == (), f'Expected scalar, got {sample_y.shape}'\n",
    "print(f'Sample features shape: {sample_x.shape}')\n",
    "print(f'Sample label: {sample_y.item():.4f}')\n",
    "print(f'Feature dtypes: {sample_x.dtype}, Label dtype: {sample_y.dtype}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Building a DataLoader from Scratch",
    "",
    "The DataLoader's job is to:",
    "1. Decide the order of samples (sequential or shuffled)",
    "2. Group indices into batches",
    "3. Fetch samples from the Dataset and collate them into tensors",
    "",
    "Let's build a minimal version to understand the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SimpleDataLoader:\n",
    "    \"\"\"A minimal DataLoader implementation for understanding the mechanics.\n",
    "\n",
    "    Supports batching, shuffling, and drop_last. Does not support\n",
    "    multi-worker loading or pinned memory.\n",
    "\n",
    "    Attributes:\n",
    "        dataset: The source Dataset.\n",
    "        batch_size: Number of samples per batch.\n",
    "        shuffle: Whether to shuffle indices each epoch.\n",
    "        drop_last: Whether to drop the last incomplete batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        batch_size: int = 32,\n",
    "        shuffle: bool = False,\n",
    "        drop_last: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the SimpleDataLoader.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset to load from.\n",
    "            batch_size: Samples per batch.\n",
    "            shuffle: Randomize sample order each epoch.\n",
    "            drop_last: Drop last batch if smaller than batch_size.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of batches.\"\"\"\n",
    "        n = len(self.dataset)\n",
    "        if self.drop_last:\n",
    "            return n // self.batch_size\n",
    "        return (n + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield batches of (features, labels) tensors.\"\"\"\n",
    "        n = len(self.dataset)\n",
    "        indices = list(range(n))\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(indices)\n",
    "\n",
    "        # Group into batches\n",
    "        for start in range(0, n, self.batch_size):\n",
    "            batch_indices = indices[start:start + self.batch_size]\n",
    "\n",
    "            if self.drop_last and len(batch_indices) < self.batch_size:\n",
    "                break\n",
    "\n",
    "            # Fetch and collate\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            for idx in batch_indices:\n",
    "                feat, lab = self.dataset[idx]\n",
    "                batch_features.append(feat)\n",
    "                batch_labels.append(lab)\n",
    "\n",
    "            yield torch.stack(batch_features), torch.stack(batch_labels)\n",
    "\n",
    "\n",
    "# Test our SimpleDataLoader\n",
    "simple_loader = SimpleDataLoader(housing_dataset, batch_size=32, shuffle=True)\n",
    "print(f'Number of batches: {len(simple_loader)}')\n",
    "\n",
    "first_batch_x, first_batch_y = next(iter(simple_loader))\n",
    "assert first_batch_x.shape == (32, 8), f'Expected (32, 8), got {first_batch_x.shape}'\n",
    "assert first_batch_y.shape == (32,), f'Expected (32,), got {first_batch_y.shape}'\n",
    "print(f'Batch features shape: {first_batch_x.shape}')\n",
    "print(f'Batch labels shape: {first_batch_y.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Understanding Shuffling and Reproducibility",
    "",
    "Shuffling is critical for training — it prevents the model from learning",
    "the order of samples. But we need **reproducible** shuffling for debugging.",
    "",
    "PyTorch's DataLoader uses a `Generator` object to control the random state",
    "independently of the global seed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_shuffling_reproducibility() -> None:\n",
    "    \"\"\"Show how Generator objects make shuffled loading reproducible.\"\"\"\n",
    "    small_dataset = TensorDataset(\n",
    "        torch.arange(10, dtype=torch.float32).unsqueeze(1),\n",
    "        torch.arange(10, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    # Without generator — different order each time\n",
    "    loader_no_gen = DataLoader(small_dataset, batch_size=3, shuffle=True)\n",
    "    run1 = [y.tolist() for _, y in loader_no_gen]\n",
    "    run2 = [y.tolist() for _, y in loader_no_gen]\n",
    "    print('Without Generator (may differ):')\n",
    "    print(f'  Run 1 batches: {run1}')\n",
    "    print(f'  Run 2 batches: {run2}')\n",
    "    print(f'  Same order? {run1 == run2}')\n",
    "    print()\n",
    "\n",
    "    # With generator — same order when re-seeded\n",
    "    gen = torch.Generator().manual_seed(SEED)\n",
    "    loader_gen = DataLoader(small_dataset, batch_size=3, shuffle=True, generator=gen)\n",
    "    run3 = [y.tolist() for _, y in loader_gen]\n",
    "\n",
    "    gen = torch.Generator().manual_seed(SEED)  # Re-seed\n",
    "    loader_gen2 = DataLoader(small_dataset, batch_size=3, shuffle=True, generator=gen)\n",
    "    run4 = [y.tolist() for _, y in loader_gen2]\n",
    "\n",
    "    print('With Generator (same seed → same order):')\n",
    "    print(f'  Run 3 batches: {run3}')\n",
    "    print(f'  Run 4 batches: {run4}')\n",
    "    print(f'  Same order? {run3 == run4}')\n",
    "\n",
    "\n",
    "demonstrate_shuffling_reproducibility()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Transforms: Composable Data Preprocessing",
    "",
    "Transforms are functions applied to each sample when it's loaded. PyTorch",
    "uses `torchvision.transforms` for images, but the pattern is general.",
    "",
    "Key transforms for images:",
    "- `ToTensor()` — convert PIL Image/NumPy to `(C, H, W)` float tensor, scale to `[0, 1]`",
    "- `Normalize(mean, std)` — channel-wise normalization",
    "- `Compose([...])` — chain multiple transforms",
    "- `RandomHorizontalFlip()`, `RandomCrop()` — data augmentation (training only)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_transforms() -> None:\n",
    "    \"\"\"Show how torchvision transforms work step by step.\"\"\"\n",
    "    # Load raw dataset (no transform) to show the conversion\n",
    "    raw_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root=DATA_DIR, train=True, download=True,\n",
    "        transform=None,  # Returns PIL Image\n",
    "    )\n",
    "\n",
    "    raw_img, raw_label = raw_dataset[0]\n",
    "    print(f'Raw image type: {type(raw_img)}')\n",
    "    print(f'Raw image size: {raw_img.size}')\n",
    "    print(f'Raw label: {raw_label} ({CLASS_NAMES[raw_label]})')\n",
    "    print()\n",
    "\n",
    "    # Step 1: ToTensor — PIL → Tensor, scale 0-255 → 0.0-1.0\n",
    "    to_tensor = T.ToTensor()\n",
    "    tensor_img = to_tensor(raw_img)\n",
    "    print(f'After ToTensor:')\n",
    "    print(f'  Shape: {tensor_img.shape}  (C, H, W format)')\n",
    "    print(f'  Range: [{tensor_img.min():.3f}, {tensor_img.max():.3f}]')\n",
    "    print(f'  Dtype: {tensor_img.dtype}')\n",
    "    print()\n",
    "\n",
    "    # Step 2: Normalize — standardize channels\n",
    "    normalize = T.Normalize(mean=[0.2860], std=[0.3530])  # FashionMNIST stats\n",
    "    norm_img = normalize(tensor_img)\n",
    "    print(f'After Normalize:')\n",
    "    print(f'  Range: [{norm_img.min():.3f}, {norm_img.max():.3f}]')\n",
    "    print(f'  Mean: {norm_img.mean():.3f}')\n",
    "    print()\n",
    "\n",
    "    # Step 3: Compose — chain transforms\n",
    "    train_transform = T.Compose([\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=10),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.2860], std=[0.3530]),\n",
    "    ])\n",
    "    print('Composed transform pipeline:')\n",
    "    for i, t in enumerate(train_transform.transforms):\n",
    "        print(f'  {i+1}. {t}')\n",
    "\n",
    "    # Visualize augmentation effects\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "    fig.suptitle('Data Augmentation: Same Image, 5 Random Augmentations', fontsize=12)\n",
    "\n",
    "    # Original\n",
    "    for idx in range(5):\n",
    "        axes[0, idx].imshow(np.array(raw_img), cmap='gray')\n",
    "        axes[0, idx].set_title('Original', fontsize=8)\n",
    "        axes[0, idx].axis('off')\n",
    "\n",
    "        aug_img = train_transform(raw_img)\n",
    "        axes[1, idx].imshow(aug_img.squeeze(), cmap='gray')\n",
    "        axes[1, idx].set_title(f'Augmented #{idx+1}', fontsize=8)\n",
    "        axes[1, idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "demonstrate_transforms()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Writing Custom Transforms",
    "",
    "You can write your own transforms as callable classes. This is useful for",
    "domain-specific preprocessing that torchvision doesn't cover."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class AddGaussianNoise:\n",
    "    \"\"\"Add Gaussian noise to a tensor — a simple data augmentation.\n",
    "\n",
    "    Attributes:\n",
    "        mean: Mean of the Gaussian noise.\n",
    "        std: Standard deviation of the Gaussian noise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean: float = 0.0, std: float = 0.1) -> None:\n",
    "        \"\"\"Initialize noise parameters.\n",
    "\n",
    "        Args:\n",
    "            mean: Mean of the noise distribution.\n",
    "            std: Standard deviation of the noise distribution.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add noise to input tensor.\n",
    "\n",
    "        Args:\n",
    "            tensor: Input image tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor with added Gaussian noise.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
    "        return tensor + noise\n",
    "\n",
    "\n",
    "class MinMaxScale:\n",
    "    \"\"\"Scale tensor values to [0, 1] range.\"\"\"\n",
    "\n",
    "    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Scale tensor to [0, 1].\n",
    "\n",
    "        Args:\n",
    "            tensor: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Scaled tensor with values in [0, 1].\n",
    "        \"\"\"\n",
    "        t_min = tensor.min()\n",
    "        t_max = tensor.max()\n",
    "        if t_max - t_min > 0:\n",
    "            return (tensor - t_min) / (t_max - t_min)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "# Test custom transforms\n",
    "noisy_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    AddGaussianNoise(mean=0.0, std=0.2),\n",
    "    MinMaxScale(),\n",
    "])\n",
    "\n",
    "raw_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=DATA_DIR, train=True, download=True, transform=None,\n",
    ")\n",
    "raw_img, label = raw_dataset[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "axes[0].imshow(np.array(raw_img), cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "for idx in range(1, 4):\n",
    "    noisy_img = noisy_transform(raw_img)\n",
    "    axes[idx].imshow(noisy_img.squeeze(), cmap='gray')\n",
    "    axes[idx].set_title(f'Noisy #{idx}')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Custom Transform: Gaussian Noise + MinMax Scaling', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Dataset for Tabular Data",
    "",
    "Image datasets are common, but many ML tasks use tabular data. Let's build",
    "a Dataset that handles feature scaling and conversion from Pandas/NumPy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Dataset for tabular data with optional feature scaling.\n",
    "\n",
    "    Converts Pandas/NumPy data to tensors and applies optional StandardScaler.\n",
    "\n",
    "    Attributes:\n",
    "        features: Scaled feature tensor.\n",
    "        labels: Label tensor.\n",
    "        scaler: Fitted StandardScaler instance (or None).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "        scaler: StandardScaler | None = None,\n",
    "        fit_scaler: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize tabular dataset.\n",
    "\n",
    "        Args:\n",
    "            features: Feature matrix of shape (n_samples, n_features).\n",
    "            labels: Label array of shape (n_samples,).\n",
    "            scaler: Pre-fitted scaler. If None and fit_scaler is True, fits a new one.\n",
    "            fit_scaler: Whether to fit a new StandardScaler.\n",
    "        \"\"\"\n",
    "        if fit_scaler and scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            features = self.scaler.fit_transform(features)\n",
    "        elif scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            features = self.scaler.transform(features)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of samples.\"\"\"\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return (features, label) at index.\n",
    "\n",
    "        Args:\n",
    "            index: Sample index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (feature_vector, label).\n",
    "        \"\"\"\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "\n",
    "# Split first, then create datasets (scaler fitted on train only)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    housing_X, housing_y, test_size=0.2, random_state=SEED,\n",
    ")\n",
    "\n",
    "train_tab_ds = TabularDataset(X_train, y_train, fit_scaler=True)\n",
    "test_tab_ds = TabularDataset(X_test, y_test, scaler=train_tab_ds.scaler)\n",
    "\n",
    "print(f'Train dataset: {len(train_tab_ds)} samples')\n",
    "print(f'Test dataset:  {len(test_tab_ds)} samples')\n",
    "sample_x, sample_y = train_tab_ds[0]\n",
    "print(f'Features (scaled): mean={sample_x.mean():.3f}, std={sample_x.std():.3f}')\n",
    "print(f'Label: {sample_y.item():.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 2 — Putting It All Together: DataPipeline Class",
    "",
    "Now we combine Dataset, transforms, and splitting into a reusable",
    "`DataPipeline` class that produces ready-to-use DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Complete data pipeline: split → transform → DataLoader.\n",
    "\n",
    "    Handles the full workflow from raw data to training-ready DataLoaders\n",
    "    with proper splitting and reproducibility.\n",
    "\n",
    "    Attributes:\n",
    "        train_loader: DataLoader for training set.\n",
    "        val_loader: DataLoader for validation set.\n",
    "        test_loader: DataLoader for test set.\n",
    "        train_set: Training Dataset.\n",
    "        val_set: Validation Dataset.\n",
    "        test_set: Test Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        batch_size: int = 64,\n",
    "        val_ratio: float = 0.1,\n",
    "        test_ratio: float = 0.1,\n",
    "        seed: int = SEED,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Create train/val/test splits and DataLoaders.\n",
    "\n",
    "        Args:\n",
    "            dataset: Full dataset to split.\n",
    "            batch_size: Samples per batch.\n",
    "            val_ratio: Fraction for validation.\n",
    "            test_ratio: Fraction for test.\n",
    "            seed: Random seed for reproducible splits.\n",
    "            num_workers: Number of data loading workers.\n",
    "            pin_memory: Whether to pin memory for GPU transfer.\n",
    "        \"\"\"\n",
    "        train_ratio = 1.0 - val_ratio - test_ratio\n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "        self.train_set, self.val_set, self.test_set = random_split(\n",
    "            dataset, [train_ratio, val_ratio, test_ratio], generator=generator,\n",
    "        )\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_set, batch_size=batch_size, shuffle=True,\n",
    "            num_workers=num_workers, pin_memory=pin_memory,\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_set, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=pin_memory,\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_set, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=pin_memory,\n",
    "        )\n",
    "\n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Return a summary DataFrame of split sizes.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with split names, sizes, ratios, and batch counts.\n",
    "        \"\"\"\n",
    "        total = len(self.train_set) + len(self.val_set) + len(self.test_set)\n",
    "        rows = []\n",
    "        for name, ds, loader in [\n",
    "            ('Train', self.train_set, self.train_loader),\n",
    "            ('Val', self.val_set, self.val_loader),\n",
    "            ('Test', self.test_set, self.test_loader),\n",
    "        ]:\n",
    "            rows.append({\n",
    "                'Split': name,\n",
    "                'Samples': len(ds),\n",
    "                'Ratio': f'{len(ds)/total:.1%}',\n",
    "                'Batches': len(loader),\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Sanity check with FashionMNIST\n",
    "pipeline = DataPipeline(\n",
    "    fashion_train, batch_size=BATCH_SIZE,\n",
    "    val_ratio=0.1, test_ratio=0.1,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    ")\n",
    "\n",
    "print('=== DataPipeline Summary ===')\n",
    "print(pipeline.summary().to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Verify batch shapes\n",
    "batch_x, batch_y = next(iter(pipeline.train_loader))\n",
    "assert batch_x.shape[0] == BATCH_SIZE, f'Expected batch size {BATCH_SIZE}, got {batch_x.shape[0]}'\n",
    "assert batch_x.shape[1:] == (1, 28, 28), f'Expected image shape (1,28,28), got {batch_x.shape[1:]}'\n",
    "print(f'Batch features shape: {batch_x.shape}')\n",
    "print(f'Batch labels shape: {batch_y.shape}')\n",
    "print(f'Batch features range: [{batch_x.min():.3f}, {batch_x.max():.3f}]')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that splits are reproducible across different pipeline instances."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def verify_split_reproducibility() -> None:\n",
    "    \"\"\"Verify that DataPipeline produces identical splits with the same seed.\"\"\"\n",
    "    small_data = TensorDataset(\n",
    "        torch.randn(100, 5),\n",
    "        torch.randint(0, 3, (100,)),\n",
    "    )\n",
    "\n",
    "    pipe1 = DataPipeline(small_data, batch_size=16, seed=SEED)\n",
    "    pipe2 = DataPipeline(small_data, batch_size=16, seed=SEED)\n",
    "    pipe3 = DataPipeline(small_data, batch_size=16, seed=42)  # Different seed\n",
    "\n",
    "    # Compare train indices\n",
    "    indices1 = pipe1.train_set.indices\n",
    "    indices2 = pipe2.train_set.indices\n",
    "    indices3 = pipe3.train_set.indices\n",
    "\n",
    "    print(f'Same seed (SEED vs SEED): indices match = {indices1 == indices2}')\n",
    "    print(f'Diff seed (SEED vs 42):   indices match = {indices1 == indices3}')\n",
    "    print(f'Train sizes: {len(indices1)}, {len(indices2)}, {len(indices3)}')\n",
    "\n",
    "\n",
    "verify_split_reproducibility()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 3 — Application: Real-World Data Loading Patterns",
    "",
    "Now we apply these concepts to realistic scenarios that appear throughout",
    "the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TensorDataset: The Quick Path",
    "",
    "When your data is already in tensors or NumPy arrays, `TensorDataset` is a",
    "one-liner alternative to writing a custom Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_tensor_dataset() -> None:\n",
    "    \"\"\"Show TensorDataset as a quick alternative to custom Datasets.\"\"\"\n",
    "    # Convert housing data to tensors\n",
    "    X_tensor = torch.tensor(housing_X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(housing_y, dtype=torch.float32)\n",
    "\n",
    "    # One-liner Dataset\n",
    "    tensor_ds = TensorDataset(X_tensor, y_tensor)\n",
    "    print(f'TensorDataset length: {len(tensor_ds)}')\n",
    "\n",
    "    sample = tensor_ds[0]\n",
    "    print(f'Sample: {len(sample)} elements')\n",
    "    print(f'  Features: shape={sample[0].shape}, dtype={sample[0].dtype}')\n",
    "    print(f'  Label: {sample[1].item():.4f}')\n",
    "    print()\n",
    "\n",
    "    # Use with DataLoader directly\n",
    "    loader = DataLoader(tensor_ds, batch_size=32, shuffle=True)\n",
    "    batch_x, batch_y = next(iter(loader))\n",
    "    print(f'Batch from TensorDataset:')\n",
    "    print(f'  Features: {batch_x.shape}')\n",
    "    print(f'  Labels: {batch_y.shape}')\n",
    "    print()\n",
    "\n",
    "    # Compare: NumpyDataset vs TensorDataset (identical behavior)\n",
    "    manual_ds = NumpyDataset(housing_X, housing_y)\n",
    "    manual_sample = manual_ds[0]\n",
    "    tensor_sample = tensor_ds[0]\n",
    "    print('NumpyDataset vs TensorDataset:')\n",
    "    print(f'  Features match: {torch.allclose(manual_sample[0], tensor_sample[0])}')\n",
    "    print(f'  Labels match: {torch.allclose(manual_sample[1], tensor_sample[1])}')\n",
    "\n",
    "\n",
    "demonstrate_tensor_dataset()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Working with Official Train/Test Splits",
    "",
    "Many datasets (CIFAR-10, MNIST, etc.) come with predefined train/test splits.",
    "The standard approach is to use the official splits and only split the",
    "training set into train/val."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_official_splits() -> None:\n",
    "    \"\"\"Show the standard pattern for datasets with official train/test splits.\"\"\"\n",
    "    # FashionMNIST has official train (60K) and test (10K) splits\n",
    "    full_train = torchvision.datasets.FashionMNIST(\n",
    "        root=DATA_DIR, train=True, download=True, transform=T.ToTensor(),\n",
    "    )\n",
    "    test_set = torchvision.datasets.FashionMNIST(\n",
    "        root=DATA_DIR, train=False, download=True, transform=T.ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Split official training set into train/val (90/10)\n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "    train_size = int(0.9 * len(full_train))\n",
    "    val_size = len(full_train) - train_size\n",
    "    train_set, val_set = random_split(\n",
    "        full_train, [train_size, val_size], generator=generator,\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Split': ['Train', 'Val', 'Test'],\n",
    "        'Source': ['Official train (90%)', 'Official train (10%)', 'Official test'],\n",
    "        'Samples': [len(train_set), len(val_set), len(test_set)],\n",
    "        'Batches': [len(train_loader), len(val_loader), len(test_loader)],\n",
    "    })\n",
    "    print('=== Official Splits Pattern ===')\n",
    "    print(summary.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Verify no overlap\n",
    "    train_indices = set(train_set.indices)\n",
    "    val_indices = set(val_set.indices)\n",
    "    overlap = train_indices & val_indices\n",
    "    print(f'Train/Val overlap: {len(overlap)} samples (should be 0)')\n",
    "\n",
    "\n",
    "demonstrate_official_splits()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Subset, Concat, and Dataset Composition",
    "",
    "PyTorch provides utilities for manipulating datasets without copying data:",
    "- **`Subset`** — select a subset of indices (useful for debugging with small data)",
    "- **`ConcatDataset`** — combine multiple datasets end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_dataset_composition() -> None:\n",
    "    \"\"\"Show Subset and ConcatDataset for dataset manipulation.\"\"\"\n",
    "    full_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root=DATA_DIR, train=True, download=True, transform=T.ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Subset — quick debugging with small data\n",
    "    debug_indices = list(range(100))\n",
    "    debug_dataset = Subset(full_dataset, debug_indices)\n",
    "    print(f'Full dataset: {len(full_dataset)} → Debug subset: {len(debug_dataset)}')\n",
    "\n",
    "    # Class-specific subset\n",
    "    class_0_indices = [i for i, (_, label) in enumerate(full_dataset) if label == 0]\n",
    "    class_0_dataset = Subset(full_dataset, class_0_indices[:500])\n",
    "    print(f'Class 0 (T-shirt) subset: {len(class_0_dataset)} samples')\n",
    "    print()\n",
    "\n",
    "    # ConcatDataset — combine train and test for cross-validation\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(\n",
    "        root=DATA_DIR, train=False, download=True, transform=T.ToTensor(),\n",
    "    )\n",
    "    combined = ConcatDataset([full_dataset, test_dataset])\n",
    "    print(f'Train ({len(full_dataset)}) + Test ({len(test_dataset)}) = Combined ({len(combined)})')\n",
    "\n",
    "    # Verify access still works\n",
    "    img, label = combined[0]\n",
    "    print(f'First sample: shape={img.shape}, label={label}')\n",
    "    img_from_test, label_from_test = combined[len(full_dataset)]\n",
    "    print(f'First test sample (via concat): shape={img_from_test.shape}, label={label_from_test}')\n",
    "\n",
    "\n",
    "demonstrate_dataset_composition()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Custom Collate Functions",
    "",
    "The default `collate_fn` stacks samples into uniform batches. But sometimes",
    "samples have different sizes (e.g., variable-length text). A custom collate",
    "function handles this by padding sequences to the maximum length in the batch."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class VariableLengthDataset(Dataset):\n",
    "    \"\"\"Dataset with variable-length sequences (simulating text data).\n",
    "\n",
    "    Attributes:\n",
    "        sequences: List of 1D tensors with different lengths.\n",
    "        labels: Tensor of integer labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_samples: int = 200, max_length: int = 50) -> None:\n",
    "        \"\"\"Generate random variable-length sequences.\n",
    "\n",
    "        Args:\n",
    "            num_samples: Number of sequences to generate.\n",
    "            max_length: Maximum sequence length.\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(SEED)\n",
    "        lengths = rng.randint(5, max_length + 1, size=num_samples)\n",
    "        self.sequences = [\n",
    "            torch.randint(0, 100, (length,)) for length in lengths\n",
    "        ]\n",
    "        self.labels = torch.randint(0, 3, (num_samples,))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of sequences.\"\"\"\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return (sequence, label) at index.\n",
    "\n",
    "        Args:\n",
    "            index: Sample index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (sequence_tensor, label_tensor).\n",
    "        \"\"\"\n",
    "        return self.sequences[index], self.labels[index]\n",
    "\n",
    "\n",
    "def pad_collate_fn(\n",
    "    batch: list[tuple[torch.Tensor, torch.Tensor]],\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Collate variable-length sequences by padding to max length.\n",
    "\n",
    "    Args:\n",
    "        batch: List of (sequence, label) tuples.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (padded_sequences, labels, lengths).\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "    # Pad all sequences to the longest in this batch\n",
    "    max_len = lengths.max().item()\n",
    "    padded = torch.zeros(len(sequences), max_len, dtype=sequences[0].dtype)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "\n",
    "    return padded, torch.stack(list(labels)), lengths\n",
    "\n",
    "\n",
    "# Test variable-length loading\n",
    "var_dataset = VariableLengthDataset(num_samples=200, max_length=50)\n",
    "print(f'Sample lengths: {[len(var_dataset[i][0]) for i in range(5)]}')\n",
    "\n",
    "var_loader = DataLoader(\n",
    "    var_dataset, batch_size=8, shuffle=False, collate_fn=pad_collate_fn,\n",
    ")\n",
    "\n",
    "padded_batch, label_batch, length_batch = next(iter(var_loader))\n",
    "print(f'\\nPadded batch shape: {padded_batch.shape}')\n",
    "print(f'Labels shape: {label_batch.shape}')\n",
    "print(f'Actual lengths: {length_batch.tolist()}')\n",
    "print(f'Max length in batch: {length_batch.max().item()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Library Comparison: Our Implementation vs PyTorch DataLoader",
    "",
    "Let's verify that our `SimpleDataLoader` produces the same results as",
    "PyTorch's official `DataLoader` and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compare_dataloaders() -> None:\n",
    "    \"\"\"Compare SimpleDataLoader vs PyTorch DataLoader.\"\"\"\n",
    "    test_data = TensorDataset(\n",
    "        torch.randn(1000, 8),\n",
    "        torch.randint(0, 5, (1000,), dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    # Our implementation\n",
    "    ours = SimpleDataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # PyTorch's implementation\n",
    "    official = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Compare outputs (no shuffling for deterministic comparison)\n",
    "    all_match = True\n",
    "    for (our_x, our_y), (off_x, off_y) in zip(ours, official):\n",
    "        if not (torch.allclose(our_x, off_x) and torch.allclose(our_y, off_y)):\n",
    "            all_match = False\n",
    "            break\n",
    "\n",
    "    print(f'Output match (no shuffle): {all_match}')\n",
    "    print(f'Number of batches — ours: {len(ours)}, official: {len(official)}')\n",
    "    print()\n",
    "\n",
    "    # Performance comparison\n",
    "    n_iters = 50\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_iters):\n",
    "        for batch in ours:\n",
    "            _ = batch  # Consume batch (timing only)\n",
    "    our_time = time.perf_counter() - start\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_iters):\n",
    "        for batch in official:\n",
    "            _ = batch  # Consume batch (timing only)\n",
    "    official_time = time.perf_counter() - start\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'Implementation': ['SimpleDataLoader (ours)', 'PyTorch DataLoader'],\n",
    "        f'Time ({n_iters} epochs)': [f'{our_time:.3f}s', f'{official_time:.3f}s'],\n",
    "        'Per Epoch': [f'{our_time/n_iters*1000:.1f}ms', f'{official_time/n_iters*1000:.1f}ms'],\n",
    "    })\n",
    "    print('=== Performance Comparison ===')\n",
    "    print(results.to_string(index=False))\n",
    "    print()\n",
    "    print('PyTorch DataLoader is optimized in C++ with multiprocessing support.')\n",
    "    print('Always use it in practice — our SimpleDataLoader is for learning only.')\n",
    "\n",
    "\n",
    "compare_dataloaders()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 4 — Evaluation & Analysis",
    "",
    "Let's analyze the DataLoader's behavior in detail — batch size effects,",
    "drop_last behavior, and performance characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Batch Size Analysis",
    "",
    "Batch size affects both training dynamics and performance. Let's measure",
    "the loading overhead for different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_batch_sizes() -> None:\n",
    "    \"\"\"Analyze how batch size affects loading performance and memory.\"\"\"\n",
    "    dataset = torchvision.datasets.FashionMNIST(\n",
    "        root=DATA_DIR, train=True, download=True, transform=T.ToTensor(),\n",
    "    )\n",
    "\n",
    "    batch_sizes = [16, 32, 64, 128, 256, 512]\n",
    "    results = []\n",
    "\n",
    "    for bs in batch_sizes:\n",
    "        loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "        num_batches = len(loader)\n",
    "\n",
    "        # Measure one epoch loading time\n",
    "        start = time.perf_counter()\n",
    "        for batch_x, batch_y in loader:\n",
    "            _ = batch_x  # Consume batch (timing only)\n",
    "        elapsed = time.perf_counter() - start\n",
    "\n",
    "        # Last batch size\n",
    "        last_batch_size = len(dataset) % bs if len(dataset) % bs != 0 else bs\n",
    "\n",
    "        results.append({\n",
    "            'Batch Size': bs,\n",
    "            'Num Batches': num_batches,\n",
    "            'Last Batch': last_batch_size,\n",
    "            'Load Time': f'{elapsed:.3f}s',\n",
    "            'Per Batch': f'{elapsed/num_batches*1000:.2f}ms',\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print('=== Batch Size Analysis (FashionMNIST, 60K samples) ===')\n",
    "    print(results_df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    load_times = [float(r['Load Time'].rstrip('s')) for r in results]\n",
    "    axes[0].plot(batch_sizes, load_times, 'o-', color='#1E88E5', linewidth=2)\n",
    "    axes[0].set_xlabel('Batch Size')\n",
    "    axes[0].set_ylabel('Load Time (s)')\n",
    "    axes[0].set_title('Epoch Loading Time vs Batch Size')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xscale('log', base=2)\n",
    "\n",
    "    num_batches_list = [r['Num Batches'] for r in results]\n",
    "    axes[1].bar(range(len(batch_sizes)), num_batches_list, color='#43A047', alpha=0.7)\n",
    "    axes[1].set_xticks(range(len(batch_sizes)))\n",
    "    axes[1].set_xticklabels(batch_sizes)\n",
    "    axes[1].set_xlabel('Batch Size')\n",
    "    axes[1].set_ylabel('Number of Batches')\n",
    "    axes[1].set_title('Batches per Epoch vs Batch Size')\n",
    "    for i, nb in enumerate(num_batches_list):\n",
    "        axes[1].text(i, nb + 10, str(nb), ha='center', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_batch_sizes()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 drop_last Behavior",
    "",
    "The `drop_last` parameter controls whether the last incomplete batch is",
    "included or discarded. This matters for:",
    "- **BatchNorm layers** — need consistent batch sizes to compute statistics",
    "- **Distributed training** — all processes need the same number of batches"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_drop_last() -> None:\n",
    "    \"\"\"Show the effect of drop_last on batch counts and data usage.\"\"\"\n",
    "    dataset_size = 100\n",
    "    test_ds = TensorDataset(\n",
    "        torch.randn(dataset_size, 5),\n",
    "        torch.randint(0, 3, (dataset_size,)),\n",
    "    )\n",
    "\n",
    "    batch_sizes = [16, 32, 64]\n",
    "    results = []\n",
    "\n",
    "    for bs in batch_sizes:\n",
    "        loader_keep = DataLoader(test_ds, batch_size=bs, drop_last=False)\n",
    "        loader_drop = DataLoader(test_ds, batch_size=bs, drop_last=True)\n",
    "\n",
    "        keep_sizes = [len(batch[0]) for batch in loader_keep]\n",
    "        drop_sizes = [len(batch[0]) for batch in loader_drop]\n",
    "\n",
    "        results.append({\n",
    "            'Batch Size': bs,\n",
    "            'drop_last=False': f'{len(keep_sizes)} batches, last={keep_sizes[-1]}',\n",
    "            'Samples Used (keep)': sum(keep_sizes),\n",
    "            'drop_last=True': f'{len(drop_sizes)} batches, all={bs}',\n",
    "            'Samples Used (drop)': sum(drop_sizes),\n",
    "            'Samples Lost': sum(keep_sizes) - sum(drop_sizes),\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f'=== drop_last Analysis (dataset_size={dataset_size}) ===')\n",
    "    print(results_df.to_string(index=False))\n",
    "    print()\n",
    "    print('Rule of thumb:')\n",
    "    print('  - Training: drop_last=True if using BatchNorm or distributed training')\n",
    "    print('  - Validation/Test: always drop_last=False (evaluate every sample)')\n",
    "\n",
    "\n",
    "analyze_drop_last()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 PyTorch Dataset Ecosystem Reference",
    "",
    "PyTorch provides hundreds of built-in datasets across three libraries.",
    "Here's a reference of the ones used in this course."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_dataset_reference() -> None:\n",
    "    \"\"\"Create a comprehensive reference of PyTorch dataset ecosystem.\"\"\"\n",
    "    reference = pd.DataFrame({\n",
    "        'Library': [\n",
    "            'torchvision', 'torchvision', 'torchvision', 'torchvision',\n",
    "            'torchvision', 'torchvision', 'torchvision',\n",
    "            'torchtext', 'torchtext', 'torchtext',\n",
    "            'torchaudio', 'sklearn',\n",
    "        ],\n",
    "        'Dataset': [\n",
    "            'MNIST', 'FashionMNIST', 'CIFAR-10', 'CIFAR-100',\n",
    "            'STL10', 'CelebA', 'VOCDetection',\n",
    "            'AG_NEWS', 'WikiText-2', 'SST-2',\n",
    "            'SPEECHCOMMANDS', 'California Housing',\n",
    "        ],\n",
    "        'Task': [\n",
    "            'Digit classification', 'Clothing classification',\n",
    "            'Image classification', 'Fine-grained classification',\n",
    "            'Self-supervised learning', 'Face attributes',\n",
    "            'Object detection',\n",
    "            'News classification', 'Language modeling',\n",
    "            'Sentiment analysis',\n",
    "            'Audio classification', 'Housing price regression',\n",
    "        ],\n",
    "        'Samples': [\n",
    "            '70K', '70K', '60K', '60K',\n",
    "            '13K', '202K', '~17K',\n",
    "            '120K', '~2M tokens', '68K',\n",
    "            '105K', '20.6K',\n",
    "        ],\n",
    "        'Used In': [\n",
    "            'Mod 3,5,11', 'Mod 1,5,6', 'Mod 6,9,11', 'Mod 9',\n",
    "            'Mod 11', 'Mod 11', 'Mod 9',\n",
    "            'Mod 7,10', 'Mod 7,8,17', 'Mod 10,13',\n",
    "            'Mod 12,19', 'Mod 1,2,4,19',\n",
    "        ],\n",
    "    })\n",
    "    print('=== PyTorch Dataset Ecosystem ===')\n",
    "    print(reference.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # DataLoader parameter reference\n",
    "    params = pd.DataFrame({\n",
    "        'Parameter': [\n",
    "            'batch_size', 'shuffle', 'num_workers', 'pin_memory',\n",
    "            'drop_last', 'collate_fn', 'generator', 'sampler',\n",
    "        ],\n",
    "        'Default': [\n",
    "            '1', 'False', '0', 'False',\n",
    "            'False', 'default', 'None', 'None',\n",
    "        ],\n",
    "        'Course Standard': [\n",
    "            '64', 'True (train only)', '0 (Colab)', 'cuda.is_available()',\n",
    "            'False', 'default', 'Generator(SEED)', 'None',\n",
    "        ],\n",
    "        'Note': [\n",
    "            'Power of 2 recommended', 'Never shuffle val/test',\n",
    "            '0 for Colab, 4+ for local', 'Faster GPU transfer',\n",
    "            'True for BatchNorm', 'Custom for var-length',\n",
    "            'For reproducible shuffling', 'For custom sampling',\n",
    "        ],\n",
    "    })\n",
    "    print('=== DataLoader Parameters ===')\n",
    "    print(params.to_string(index=False))\n",
    "\n",
    "\n",
    "build_dataset_reference()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Error Analysis: Common Data Loading Mistakes",
    "",
    "Data loading bugs are subtle and can silently degrade training. Let's",
    "demonstrate the most common mistakes and how to detect them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_common_mistakes() -> None:\n",
    "    \"\"\"Show common data loading mistakes and their consequences.\"\"\"\n",
    "    print('=== Common Data Loading Mistakes ===')\n",
    "    print()\n",
    "\n",
    "    # Mistake 1: Fitting scaler on test data (data leakage)\n",
    "    print('1. DATA LEAKAGE: Fitting scaler on all data (including test)')\n",
    "    scaler_wrong = StandardScaler()\n",
    "    X_all_scaled = scaler_wrong.fit_transform(housing_X)  # BAD: includes test data\n",
    "\n",
    "    scaler_right = StandardScaler()\n",
    "    X_train_scaled = scaler_right.fit_transform(X_train)   # GOOD: train only\n",
    "    X_test_scaled = scaler_right.transform(X_test)          # Transform test with train stats\n",
    "\n",
    "    print(f'  Wrong (fit on all):   test mean = {X_all_scaled[-len(X_test):].mean():.4f}')\n",
    "    print(f'  Right (fit on train): test mean = {X_test_scaled.mean():.4f}')\n",
    "    print(f'  The \"right\" way has non-zero test mean — this is correct!')\n",
    "    print()\n",
    "\n",
    "    # Mistake 2: Forgetting to shuffle training data\n",
    "    print('2. FORGETTING TO SHUFFLE: Model sees same order every epoch')\n",
    "    sorted_data = TensorDataset(\n",
    "        torch.randn(100, 5),\n",
    "        torch.tensor([0]*50 + [1]*50, dtype=torch.long),  # Sorted labels\n",
    "    )\n",
    "    loader_noshuffle = DataLoader(sorted_data, batch_size=20, shuffle=False)\n",
    "    first_batch_labels = next(iter(loader_noshuffle))[1]\n",
    "    print(f'  No shuffle — first batch labels: {first_batch_labels.tolist()}')\n",
    "    print(f'  All zeros! Model sees pure class-0 batches then pure class-1.')\n",
    "\n",
    "    loader_shuffle = DataLoader(sorted_data, batch_size=20, shuffle=True)\n",
    "    first_batch_labels_s = next(iter(loader_shuffle))[1]\n",
    "    print(f'  Shuffled — first batch labels:   {first_batch_labels_s.tolist()}')\n",
    "    print(f'  Mixed! Each batch has representative class distribution.')\n",
    "    print()\n",
    "\n",
    "    # Mistake 3: Wrong normalization values\n",
    "    print('3. WRONG NORMALIZATION: Using ImageNet stats for non-ImageNet data')\n",
    "    imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    fashion_mean, fashion_std = [0.2860], [0.3530]\n",
    "    print(f'  ImageNet stats (3-channel RGB): mean={imagenet_mean}, std={imagenet_std}')\n",
    "    print(f'  FashionMNIST stats (1-channel): mean={fashion_mean}, std={fashion_std}')\n",
    "    print(f'  Using ImageNet stats on FashionMNIST would shift the distribution!')\n",
    "    print()\n",
    "\n",
    "    # Mistake 4: Augmenting validation/test data\n",
    "    print('4. AUGMENTING VAL/TEST: Apply augmentation only to training data')\n",
    "    train_tf = T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomRotation(10),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.2860], std=[0.3530]),\n",
    "    ])\n",
    "    val_tf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.2860], std=[0.3530]),\n",
    "    ])\n",
    "    print(f'  Train transforms: {len(train_tf.transforms)} steps (with augmentation)')\n",
    "    print(f'  Val transforms:   {len(val_tf.transforms)} steps (no augmentation)')\n",
    "\n",
    "    # Summary\n",
    "    print()\n",
    "    mistakes_df = pd.DataFrame({\n",
    "        'Mistake': [\n",
    "            'Fit scaler on all data',\n",
    "            'Forget to shuffle training',\n",
    "            'Wrong normalization stats',\n",
    "            'Augment validation data',\n",
    "            'Shuffle validation/test',\n",
    "        ],\n",
    "        'Consequence': [\n",
    "            'Data leakage → inflated metrics',\n",
    "            'Ordered batches → poor gradients',\n",
    "            'Shifted distribution → slow convergence',\n",
    "            'Noisy evaluation → unreliable metrics',\n",
    "            'Non-reproducible evaluation',\n",
    "        ],\n",
    "        'Fix': [\n",
    "            'Fit on train, transform test',\n",
    "            'shuffle=True for train loader',\n",
    "            'Compute stats from your dataset',\n",
    "            'Separate train/val transforms',\n",
    "            'shuffle=False for val/test',\n",
    "        ],\n",
    "    })\n",
    "    print('=== Data Loading Mistakes Summary ===')\n",
    "    print(mistakes_df.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_common_mistakes()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Computing Dataset Statistics",
    "",
    "To use `Normalize` correctly, you need to compute the mean and standard",
    "deviation of your training data. Here's the standard approach."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_dataset_statistics(dataset: Dataset) -> tuple[list[float], list[float]]:\n",
    "    \"\"\"Compute channel-wise mean and std for an image dataset.\n",
    "\n",
    "    Uses Welford's online algorithm to avoid loading all images into memory.\n",
    "\n",
    "    Args:\n",
    "        dataset: Image dataset where __getitem__ returns (image_tensor, label).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (channel_means, channel_stds).\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "    n_channels = dataset[0][0].shape[0]\n",
    "    pixel_sum = torch.zeros(n_channels)\n",
    "    pixel_sq_sum = torch.zeros(n_channels)\n",
    "    n_pixels = 0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        batch_size = images.shape[0]\n",
    "        n_per_image = images.shape[2] * images.shape[3]\n",
    "\n",
    "        # Sum over batch, height, width — keep channels\n",
    "        pixel_sum += images.sum(dim=[0, 2, 3])\n",
    "        pixel_sq_sum += (images ** 2).sum(dim=[0, 2, 3])\n",
    "        n_pixels += batch_size * n_per_image\n",
    "\n",
    "    mean = pixel_sum / n_pixels\n",
    "    std = torch.sqrt(pixel_sq_sum / n_pixels - mean ** 2)\n",
    "\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "\n",
    "# Compute stats for FashionMNIST\n",
    "raw_fashion = torchvision.datasets.FashionMNIST(\n",
    "    root=DATA_DIR, train=True, download=True, transform=T.ToTensor(),\n",
    ")\n",
    "\n",
    "means, stds = compute_dataset_statistics(raw_fashion)\n",
    "print(f'FashionMNIST statistics:')\n",
    "print(f'  Mean: {[f\"{m:.4f}\" for m in means]}')\n",
    "print(f'  Std:  {[f\"{s:.4f}\" for s in stds]}')\n",
    "print()\n",
    "print('Use these values in T.Normalize():')\n",
    "print(f'  T.Normalize(mean={[round(m, 4) for m in means]}, std={[round(s, 4) for s in stds]})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 End-to-End Training Demo",
    "",
    "Let's use our DataPipeline to train a simple model, demonstrating the",
    "complete workflow from data loading to training curves."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def end_to_end_training_demo() -> None:\n",
    "    \"\"\"Demonstrate complete pipeline from DataLoader to training curves.\"\"\"\n",
    "    # Prepare tabular data pipeline\n",
    "    full_ds = TensorDataset(\n",
    "        torch.tensor(housing_X, dtype=torch.float32),\n",
    "        torch.tensor(housing_y, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        full_ds, [0.8, 0.1, 0.1], generator=generator,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')\n",
    "\n",
    "    # Simple linear model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(8, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 1),\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    train_losses: list[float] = []\n",
    "    val_losses: list[float] = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_samples = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_x).squeeze()\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "            n_samples += batch_x.size(0)\n",
    "        train_losses.append(epoch_loss / n_samples)\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                predictions = model(batch_x).squeeze()\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item() * batch_x.size(0)\n",
    "                val_samples += batch_x.size(0)\n",
    "        val_losses.append(val_loss / val_samples)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs} | '\n",
    "                  f'Train Loss: {train_losses[-1]:.4f} | '\n",
    "                  f'Val Loss: {val_losses[-1]:.4f}')\n",
    "\n",
    "    # Plot training curves\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', linewidth=1.5)\n",
    "    ax.plot(range(1, num_epochs + 1), val_losses, label='Val Loss', linewidth=1.5)\n",
    "    best_epoch = np.argmin(val_losses) + 1\n",
    "    ax.axvline(best_epoch, color='gray', linestyle='--', alpha=0.5,\n",
    "               label=f'Best: epoch {best_epoch}')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MSE Loss')\n",
    "    ax.set_title('Training Curves — DataLoader → Model → Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            predictions = model(batch_x).squeeze()\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            test_loss += loss.item() * batch_x.size(0)\n",
    "            test_samples += batch_x.size(0)\n",
    "    test_mse = test_loss / test_samples\n",
    "    print(f'\\nTest MSE: {test_mse:.4f}')\n",
    "    print(f'Test RMSE: {np.sqrt(test_mse):.4f}')\n",
    "\n",
    "\n",
    "end_to_end_training_demo()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 5 — Summary & Lessons Learned",
    "",
    "### Key Takeaways",
    "",
    "1. **Dataset defines data access, DataLoader handles batching.** Always subclass",
    "   `Dataset` with `__len__` + `__getitem__`, then wrap with `DataLoader` for",
    "   automatic batching, shuffling, and parallel loading.",
    "",
    "2. **Transforms are composable preprocessing.** Use `T.Compose([...])` to chain",
    "   transforms. Apply augmentation only to training data, not validation/test.",
    "",
    "3. **Always shuffle training, never shuffle evaluation.** Shuffling prevents the",
    "   model from learning data order. Use `Generator().manual_seed(SEED)` for",
    "   reproducible shuffling.",
    "",
    "4. **Fit preprocessing on training data only.** Scalers, normalizers, and",
    "   statistics must be computed from training data and applied to val/test.",
    "   Fitting on all data causes data leakage.",
    "",
    "5. **Use official splits when available.** Datasets like CIFAR-10 and FashionMNIST",
    "   have standard train/test splits. Split only the training portion into train/val.",
    "",
    "### What's Next",
    "",
    "→ **01-06 (Linear Algebra for Machine Learning)** covers eigendecomposition, SVD,",
    "  and low-rank approximation — the mathematical tools behind PCA, dimensionality",
    "  reduction, and matrix factorization methods.",
    "",
    "### Going Further",
    "",
    "- [PyTorch Data Loading Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) —",
    "  Official tutorial with more advanced patterns",
    "- [Writing Custom Datasets](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) —",
    "  PyTorch basics on Dataset and DataLoader"
   ]
  }
 ]
}