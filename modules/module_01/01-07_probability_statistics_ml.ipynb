{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations",
    "## 01-07: Probability & Statistics for ML",
    "",
    "**Objective:** Build the probabilistic foundation for machine learning —",
    "distributions, likelihood, MLE, MAP estimation, and Bayes' theorem —",
    "from scratch with visual intuition and numerical experiments.",
    "",
    "**Prerequisites:** 01-01 (Python, NumPy & Tensor Speed), 01-06 (Linear Algebra for ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 0 — Setup & Prerequisites",
    "",
    "Probability is the language of uncertainty, and almost every ML algorithm",
    "is either explicitly or implicitly probabilistic. Loss functions are",
    "negative log-likelihoods, regularization is a prior, and generalization",
    "is a statement about population distributions.",
    "",
    "This notebook covers:",
    "- **Probability axioms and rules** — independence, conditioning, marginalization",
    "- **Common distributions** — Bernoulli, Binomial, Gaussian, Categorical",
    "- **Maximum Likelihood Estimation (MLE)** — fitting distributions to data",
    "- **Maximum A Posteriori (MAP)** — MLE with priors (= regularization)",
    "- **Bayes' theorem** — updating beliefs with evidence",
    "- **Central Limit Theorem** — why Gaussians appear everywhere",
    "- **Hypothesis testing basics** — p-values, confidence intervals",
    "",
    "We use synthetic data and the California Housing dataset to ground every",
    "concept in practical ML applications.",
    "",
    "**Prerequisites:** 01-01, 01-06 (Linear Algebra for ML)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy import stats as sp_stats\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA: {torch.version.cuda}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "import random\n",
    "\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "FIGSIZE = (10, 6)\n",
    "COLORS = {\n",
    "    'blue': '#1E88E5',\n",
    "    'red': '#E53935',\n",
    "    'green': '#43A047',\n",
    "    'orange': '#FF9800',\n",
    "    'purple': '#9C27B0',\n",
    "    'teal': '#00897B',\n",
    "    'gray': '#757575',\n",
    "}\n",
    "COLOR_LIST = list(COLORS.values())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading",
    "",
    "We load the California Housing dataset to demonstrate statistical concepts",
    "on real-world data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# California Housing\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data.astype(np.float64)\n",
    "y_housing = housing.target.astype(np.float64)\n",
    "feature_names = housing.feature_names\n",
    "print(f'California Housing: {X_housing.shape}')\n",
    "print(f'Target (MedHouseVal): mean={y_housing.mean():.3f}, std={y_housing.std():.3f}')\n",
    "print(f'Features: {feature_names}')\n",
    "\n",
    "# Quick distribution check\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].hist(y_housing, bins=50, color=COLORS['blue'], alpha=0.7, edgecolor='white')\n",
    "axes[0].set_xlabel('Median House Value ($100K)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Target Distribution')\n",
    "\n",
    "axes[1].hist(X_housing[:, 0], bins=50, color=COLORS['green'], alpha=0.7, edgecolor='white')\n",
    "axes[1].set_xlabel('Median Income')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('MedInc Distribution')\n",
    "\n",
    "axes[2].hist(X_housing[:, 1], bins=50, color=COLORS['orange'], alpha=0.7, edgecolor='white')\n",
    "axes[2].set_xlabel('House Age')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('HouseAge Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 1 — Probability Foundations from Scratch",
    "",
    "We build up from probability axioms to the key distributions and",
    "estimation methods used in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Discrete Distributions",
    "",
    "**Bernoulli** — a single coin flip with probability $p$:",
    "$$P(X = 1) = p, \\quad P(X = 0) = 1 - p$$",
    "",
    "**Binomial** — $n$ independent coin flips:",
    "$$P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}$$",
    "",
    "**Categorical** — generalization to $K$ classes:",
    "$$P(X = k) = p_k, \\quad \\sum_{k=1}^{K} p_k = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def bernoulli_pmf(k: np.ndarray, p: float) -> np.ndarray:\n",
    "    \"\"\"Compute Bernoulli PMF.\n",
    "\n",
    "    Args:\n",
    "        k: Array of outcomes (0 or 1).\n",
    "        p: Probability of success.\n",
    "\n",
    "    Returns:\n",
    "        Array of probabilities.\n",
    "    \"\"\"\n",
    "    return p ** k * (1 - p) ** (1 - k)\n",
    "\n",
    "\n",
    "def binomial_pmf(k: np.ndarray, n: int, p: float) -> np.ndarray:\n",
    "    \"\"\"Compute Binomial PMF.\n",
    "\n",
    "    Args:\n",
    "        k: Array of number of successes.\n",
    "        n: Number of trials.\n",
    "        p: Probability of success per trial.\n",
    "\n",
    "    Returns:\n",
    "        Array of probabilities.\n",
    "    \"\"\"\n",
    "    from math import comb\n",
    "    coeffs = np.array([comb(n, int(ki)) for ki in k])\n",
    "    return coeffs * p ** k * (1 - p) ** (n - k)\n",
    "\n",
    "\n",
    "def categorical_sample(\n",
    "    probs: np.ndarray,\n",
    "    n_samples: int,\n",
    "    rng: np.random.RandomState,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Sample from a categorical distribution.\n",
    "\n",
    "    Args:\n",
    "        probs: Probability vector of shape (K,), must sum to 1.\n",
    "        n_samples: Number of samples to draw.\n",
    "        rng: Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Array of sampled class indices of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    assert np.isclose(probs.sum(), 1.0), f'Probabilities must sum to 1, got {probs.sum()}'\n",
    "    cumsum = np.cumsum(probs)\n",
    "    uniforms = rng.uniform(0, 1, n_samples)\n",
    "    return np.searchsorted(cumsum, uniforms)\n",
    "\n",
    "\n",
    "# Visualize discrete distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Bernoulli\n",
    "for p, color in [(0.3, COLORS['blue']), (0.5, COLORS['green']), (0.8, COLORS['red'])]:\n",
    "    k = np.array([0, 1])\n",
    "    axes[0].bar(k + (p - 0.5) * 0.25, bernoulli_pmf(k, p), width=0.2,\n",
    "                color=color, alpha=0.7, label=f'p={p}')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xlabel('k')\n",
    "axes[0].set_ylabel('P(X=k)')\n",
    "axes[0].set_title('Bernoulli Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Binomial\n",
    "n_trials = 20\n",
    "k_vals = np.arange(0, n_trials + 1)\n",
    "for p, color in [(0.3, COLORS['blue']), (0.5, COLORS['green']), (0.7, COLORS['red'])]:\n",
    "    axes[1].plot(k_vals, binomial_pmf(k_vals, n_trials, p), 'o-',\n",
    "                 color=color, markersize=3, label=f'n={n_trials}, p={p}')\n",
    "axes[1].set_xlabel('k (successes)')\n",
    "axes[1].set_ylabel('P(X=k)')\n",
    "axes[1].set_title('Binomial Distribution')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "# Categorical\n",
    "rng = np.random.RandomState(SEED)\n",
    "probs = np.array([0.1, 0.3, 0.15, 0.25, 0.2])\n",
    "samples = categorical_sample(probs, 5000, rng)\n",
    "counts = np.bincount(samples, minlength=len(probs))\n",
    "empirical = counts / counts.sum()\n",
    "x_pos = np.arange(len(probs))\n",
    "axes[2].bar(x_pos - 0.15, probs, width=0.3, color=COLORS['blue'], alpha=0.7, label='True')\n",
    "axes[2].bar(x_pos + 0.15, empirical, width=0.3, color=COLORS['red'], alpha=0.7, label='Empirical (5K)')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('Probability')\n",
    "axes[2].set_title('Categorical Distribution')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Continuous Distributions",
    "",
    "The **Gaussian (Normal)** distribution is the most important in ML:",
    "",
    "$$p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$",
    "",
    "The **Multivariate Gaussian** generalizes to $d$ dimensions:",
    "",
    "$$p(\\mathbf{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "    \"\"\"Compute univariate Gaussian PDF.\n",
    "\n",
    "    Args:\n",
    "        x: Points at which to evaluate the PDF.\n",
    "        mu: Mean of the distribution.\n",
    "        sigma: Standard deviation of the distribution.\n",
    "\n",
    "    Returns:\n",
    "        Array of density values.\n",
    "    \"\"\"\n",
    "    coeff = 1.0 / (sigma * np.sqrt(2 * np.pi))\n",
    "    exponent = -0.5 * ((x - mu) / sigma) ** 2\n",
    "    return coeff * np.exp(exponent)\n",
    "\n",
    "\n",
    "def multivariate_gaussian_pdf(\n",
    "    X: np.ndarray,\n",
    "    mu: np.ndarray,\n",
    "    cov: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute multivariate Gaussian PDF.\n",
    "\n",
    "    Args:\n",
    "        X: Data points of shape (n_samples, d).\n",
    "        mu: Mean vector of shape (d,).\n",
    "        cov: Covariance matrix of shape (d, d).\n",
    "\n",
    "    Returns:\n",
    "        Array of density values of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    d = len(mu)\n",
    "    diff = X - mu\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    det = np.linalg.det(cov)\n",
    "\n",
    "    coeff = 1.0 / (np.sqrt((2 * np.pi) ** d * det))\n",
    "    exponent = -0.5 * np.sum(diff @ cov_inv * diff, axis=1)\n",
    "    return coeff * np.exp(exponent)\n",
    "\n",
    "\n",
    "# Visualize Gaussian distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "x_range = np.linspace(-6, 6, 300)\n",
    "\n",
    "# Different means\n",
    "for mu, color in [(-2, COLORS['blue']), (0, COLORS['green']), (2, COLORS['red'])]:\n",
    "    y = gaussian_pdf(x_range, mu, 1.0)\n",
    "    axes[0].plot(x_range, y, color=color, linewidth=2, label=f'μ={mu}, σ=1')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('p(x)')\n",
    "axes[0].set_title('Varying Mean')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "# Different variances\n",
    "for sigma, color in [(0.5, COLORS['blue']), (1.0, COLORS['green']), (2.0, COLORS['red'])]:\n",
    "    y = gaussian_pdf(x_range, 0, sigma)\n",
    "    axes[1].plot(x_range, y, color=color, linewidth=2, label=f'μ=0, σ={sigma}')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('p(x)')\n",
    "axes[1].set_title('Varying Standard Deviation')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "# Verify against scipy\n",
    "our_pdf = gaussian_pdf(x_range, 1.0, 2.0)\n",
    "scipy_pdf = sp_stats.norm.pdf(x_range, 1.0, 2.0)\n",
    "axes[2].plot(x_range, our_pdf, color=COLORS['blue'], linewidth=2, label='Ours')\n",
    "axes[2].plot(x_range, scipy_pdf, '--', color=COLORS['red'], linewidth=2, label='SciPy')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('p(x)')\n",
    "axes[2].set_title(f'Verification: max diff = {np.max(np.abs(our_pdf - scipy_pdf)):.2e}')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Other Distributions in ML",
    "",
    "Beyond the Gaussian, several distributions appear frequently:",
    "- **Uniform** — uninformative priors, random initialization",
    "- **Exponential** — waiting times, learning rate schedules",
    "- **Beta** — priors on probabilities (Bayesian classification)",
    "- **Poisson** — count data (word frequencies, event counts)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_other_distributions() -> None:\n",
    "    \"\"\"Visualize distributions commonly used in ML.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(13, 10))\n",
    "    x = np.linspace(0.01, 10, 300)\n",
    "    x_01 = np.linspace(0.001, 0.999, 300)\n",
    "\n",
    "    # Uniform\n",
    "    for a, b, color in [(0, 1, COLORS['blue']), (2, 5, COLORS['green']), (0, 10, COLORS['red'])]:\n",
    "        y = np.where((x >= a) & (x <= b), 1.0 / (b - a), 0.0)\n",
    "        axes[0, 0].plot(x, y, color=color, linewidth=2, label=f'U({a},{b})')\n",
    "    axes[0, 0].set_xlabel('x')\n",
    "    axes[0, 0].set_ylabel('p(x)')\n",
    "    axes[0, 0].set_title('Uniform Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.2)\n",
    "\n",
    "    # Exponential\n",
    "    for lam, color in [(0.5, COLORS['blue']), (1.0, COLORS['green']), (2.0, COLORS['red'])]:\n",
    "        y = lam * np.exp(-lam * x)\n",
    "        axes[0, 1].plot(x, y, color=color, linewidth=2, label=f'λ={lam}')\n",
    "    axes[0, 1].set_xlabel('x')\n",
    "    axes[0, 1].set_ylabel('p(x)')\n",
    "    axes[0, 1].set_title('Exponential Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.2)\n",
    "\n",
    "    # Beta\n",
    "    for a, b, color in [(0.5, 0.5, COLORS['blue']), (2, 5, COLORS['green']),\n",
    "                         (5, 2, COLORS['red']), (2, 2, COLORS['orange'])]:\n",
    "        y = sp_stats.beta.pdf(x_01, a, b)\n",
    "        axes[1, 0].plot(x_01, y, color=color, linewidth=2, label=f'α={a}, β={b}')\n",
    "    axes[1, 0].set_xlabel('x')\n",
    "    axes[1, 0].set_ylabel('p(x)')\n",
    "    axes[1, 0].set_title('Beta Distribution')\n",
    "    axes[1, 0].legend(fontsize=9)\n",
    "    axes[1, 0].grid(True, alpha=0.2)\n",
    "\n",
    "    # Poisson\n",
    "    k_vals = np.arange(0, 20)\n",
    "    for lam, color in [(1, COLORS['blue']), (4, COLORS['green']), (10, COLORS['red'])]:\n",
    "        y = sp_stats.poisson.pmf(k_vals, lam)\n",
    "        axes[1, 1].plot(k_vals, y, 'o-', color=color, markersize=4,\n",
    "                         linewidth=1.5, label=f'λ={lam}')\n",
    "    axes[1, 1].set_xlabel('k')\n",
    "    axes[1, 1].set_ylabel('P(X=k)')\n",
    "    axes[1, 1].set_title('Poisson Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.2)\n",
    "\n",
    "    plt.suptitle('Common Distributions in ML', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "demonstrate_other_distributions()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Maximum Likelihood Estimation (MLE)",
    "",
    "Given observed data $\\mathcal{D} = \\{x_1, \\ldots, x_n\\}$, MLE finds the",
    "parameter $\\theta$ that maximizes the likelihood:",
    "",
    "$$\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta \\prod_{i=1}^{n} p(x_i | \\theta)$$",
    "",
    "In practice, we maximize the **log-likelihood** (sum instead of product):",
    "",
    "$$\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta \\sum_{i=1}^{n} \\log p(x_i | \\theta)$$",
    "",
    "**Connection to ML:** Minimizing cross-entropy loss = maximizing log-likelihood",
    "of the data under the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def gaussian_log_likelihood(\n",
    "    data: np.ndarray,\n",
    "    mu: float,\n",
    "    sigma: float,\n",
    ") -> float:\n",
    "    \"\"\"Compute log-likelihood of data under a Gaussian model.\n",
    "\n",
    "    Args:\n",
    "        data: Observed values of shape (n,).\n",
    "        mu: Mean parameter.\n",
    "        sigma: Standard deviation parameter.\n",
    "\n",
    "    Returns:\n",
    "        Total log-likelihood value.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    ll = -0.5 * n * np.log(2 * np.pi)\n",
    "    ll -= n * np.log(sigma)\n",
    "    ll -= 0.5 * np.sum((data - mu) ** 2) / sigma ** 2\n",
    "    return ll\n",
    "\n",
    "\n",
    "def mle_gaussian(data: np.ndarray) -> tuple[float, float]:\n",
    "    \"\"\"Compute MLE for Gaussian parameters (closed-form).\n",
    "\n",
    "    For a Gaussian, the MLE solutions are:\n",
    "    - μ_MLE = sample mean\n",
    "    - σ²_MLE = sample variance (biased, dividing by n)\n",
    "\n",
    "    Args:\n",
    "        data: Observed values of shape (n,).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (mu_mle, sigma_mle).\n",
    "    \"\"\"\n",
    "    mu_mle = np.mean(data)\n",
    "    sigma_mle = np.sqrt(np.mean((data - mu_mle) ** 2))  # Biased (MLE)\n",
    "    return mu_mle, sigma_mle\n",
    "\n",
    "\n",
    "# Generate data from a known Gaussian\n",
    "rng = np.random.RandomState(SEED)\n",
    "true_mu, true_sigma = 3.0, 1.5\n",
    "data_mle = rng.normal(true_mu, true_sigma, size=500)\n",
    "\n",
    "mu_hat, sigma_hat = mle_gaussian(data_mle)\n",
    "print(f'True parameters:  μ={true_mu}, σ={true_sigma}')\n",
    "print(f'MLE estimates:    μ̂={mu_hat:.4f}, σ̂={sigma_hat:.4f}')\n",
    "print(f'Log-likelihood at MLE: {gaussian_log_likelihood(data_mle, mu_hat, sigma_hat):.4f}')\n",
    "print(f'Log-likelihood at true: {gaussian_log_likelihood(data_mle, true_mu, true_sigma):.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how the log-likelihood surface looks and how MLE finds",
    "the peak."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_mle_surface(data: np.ndarray, true_mu: float, true_sigma: float) -> None:\n",
    "    \"\"\"Visualize the log-likelihood surface for Gaussian MLE.\n",
    "\n",
    "    Args:\n",
    "        data: Observed data.\n",
    "        true_mu: True mean.\n",
    "        true_sigma: True standard deviation.\n",
    "    \"\"\"\n",
    "    mu_hat, sigma_hat = mle_gaussian(data)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # 1D slice: LL vs mu (sigma fixed at MLE)\n",
    "    mu_range = np.linspace(true_mu - 2, true_mu + 2, 200)\n",
    "    ll_mu = [gaussian_log_likelihood(data, m, sigma_hat) for m in mu_range]\n",
    "    axes[0].plot(mu_range, ll_mu, color=COLORS['blue'], linewidth=2)\n",
    "    axes[0].axvline(mu_hat, color=COLORS['red'], linestyle='--',\n",
    "                     label=f'μ̂_MLE = {mu_hat:.3f}')\n",
    "    axes[0].axvline(true_mu, color=COLORS['green'], linestyle=':',\n",
    "                     label=f'μ_true = {true_mu}')\n",
    "    axes[0].set_xlabel('μ')\n",
    "    axes[0].set_ylabel('Log-likelihood')\n",
    "    axes[0].set_title('LL vs μ (σ fixed at MLE)')\n",
    "    axes[0].legend(fontsize=9)\n",
    "    axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "    # 1D slice: LL vs sigma (mu fixed at MLE)\n",
    "    sigma_range = np.linspace(0.5, 3.5, 200)\n",
    "    ll_sigma = [gaussian_log_likelihood(data, mu_hat, s) for s in sigma_range]\n",
    "    axes[1].plot(sigma_range, ll_sigma, color=COLORS['blue'], linewidth=2)\n",
    "    axes[1].axvline(sigma_hat, color=COLORS['red'], linestyle='--',\n",
    "                     label=f'σ̂_MLE = {sigma_hat:.3f}')\n",
    "    axes[1].axvline(true_sigma, color=COLORS['green'], linestyle=':',\n",
    "                     label=f'σ_true = {true_sigma}')\n",
    "    axes[1].set_xlabel('σ')\n",
    "    axes[1].set_ylabel('Log-likelihood')\n",
    "    axes[1].set_title('LL vs σ (μ fixed at MLE)')\n",
    "    axes[1].legend(fontsize=9)\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    # 2D contour\n",
    "    mu_grid = np.linspace(true_mu - 1.5, true_mu + 1.5, 100)\n",
    "    sigma_grid = np.linspace(0.8, 2.5, 100)\n",
    "    ll_surface = np.zeros((len(sigma_grid), len(mu_grid)))\n",
    "    for i, s in enumerate(sigma_grid):\n",
    "        for j, m in enumerate(mu_grid):\n",
    "            ll_surface[i, j] = gaussian_log_likelihood(data, m, s)\n",
    "\n",
    "    axes[2].contourf(mu_grid, sigma_grid, ll_surface, levels=30, cmap='viridis')\n",
    "    axes[2].plot(mu_hat, sigma_hat, 'r*', markersize=15, label='MLE')\n",
    "    axes[2].plot(true_mu, true_sigma, 'g^', markersize=10, label='True')\n",
    "    axes[2].set_xlabel('μ')\n",
    "    axes[2].set_ylabel('σ')\n",
    "    axes[2].set_title('Log-Likelihood Surface')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.suptitle('Maximum Likelihood Estimation', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_mle_surface(data_mle, true_mu, true_sigma)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 MLE for Bernoulli (Classification Connection)",
    "",
    "For binary classification, the model outputs $p(y=1|\\mathbf{x})$.",
    "The MLE for Bernoulli is simply the sample proportion:",
    "",
    "$$\\hat{p}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$$",
    "",
    "The negative log-likelihood is the **binary cross-entropy loss**:",
    "",
    "$$\\mathcal{L} = -\\frac{1}{n}\\sum_i \\left[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def bernoulli_nll(labels: np.ndarray, predicted_probs: np.ndarray) -> float:\n",
    "    \"\"\"Compute binary cross-entropy (negative log-likelihood for Bernoulli).\n",
    "\n",
    "    Args:\n",
    "        labels: True binary labels of shape (n,).\n",
    "        predicted_probs: Predicted probabilities of shape (n,).\n",
    "\n",
    "    Returns:\n",
    "        Average negative log-likelihood (binary cross-entropy).\n",
    "    \"\"\"\n",
    "    eps = 1e-15  # Numerical stability\n",
    "    predicted_probs = np.clip(predicted_probs, eps, 1 - eps)\n",
    "    nll = -(labels * np.log(predicted_probs) + (1 - labels) * np.log(1 - predicted_probs))\n",
    "    return nll.mean()\n",
    "\n",
    "\n",
    "# Demonstrate connection: MLE of coin flips\n",
    "rng = np.random.RandomState(SEED)\n",
    "true_p = 0.7\n",
    "coin_flips = rng.binomial(1, true_p, size=1000)\n",
    "\n",
    "p_hat = coin_flips.mean()  # MLE\n",
    "print(f'Coin flips: n={len(coin_flips)}, sum={coin_flips.sum()}')\n",
    "print(f'True p = {true_p}')\n",
    "print(f'MLE p̂ = {p_hat:.4f}')\n",
    "print()\n",
    "\n",
    "# BCE at different p values\n",
    "p_range = np.linspace(0.01, 0.99, 200)\n",
    "bce_values = [bernoulli_nll(coin_flips, np.full(len(coin_flips), p)) for p in p_range]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(p_range, bce_values, color=COLORS['blue'], linewidth=2)\n",
    "ax.axvline(p_hat, color=COLORS['red'], linestyle='--', label=f'MLE p̂ = {p_hat:.4f}')\n",
    "ax.axvline(true_p, color=COLORS['green'], linestyle=':', label=f'True p = {true_p}')\n",
    "ax.set_xlabel('p')\n",
    "ax.set_ylabel('Binary Cross-Entropy')\n",
    "ax.set_title('BCE vs p — Minimum = MLE')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'BCE at MLE:  {bernoulli_nll(coin_flips, np.full(len(coin_flips), p_hat)):.6f}')\n",
    "print(f'BCE at true: {bernoulli_nll(coin_flips, np.full(len(coin_flips), true_p)):.6f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Maximum A Posteriori (MAP) Estimation",
    "",
    "MAP adds a **prior** distribution $p(\\theta)$ to MLE:",
    "",
    "$$\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\left[\\sum_i \\log p(x_i | \\theta) + \\log p(\\theta)\\right]$$",
    "",
    "**The key insight:** MAP = MLE + regularization.",
    "- Gaussian prior on weights → L2 regularization (Ridge)",
    "- Laplace prior on weights → L1 regularization (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def map_gaussian_with_prior(\n",
    "    data: np.ndarray,\n",
    "    prior_mu: float,\n",
    "    prior_sigma: float,\n",
    "    data_sigma: float,\n",
    ") -> float:\n",
    "    \"\"\"Compute MAP estimate for Gaussian mean with Gaussian prior.\n",
    "\n",
    "    Closed-form: MAP = weighted average of prior mean and MLE.\n",
    "\n",
    "    Args:\n",
    "        data: Observed values.\n",
    "        prior_mu: Prior mean for the parameter.\n",
    "        prior_sigma: Prior standard deviation (confidence in prior).\n",
    "        data_sigma: Known data standard deviation.\n",
    "\n",
    "    Returns:\n",
    "        MAP estimate of the mean.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    data_mean = np.mean(data)\n",
    "    prior_precision = 1.0 / prior_sigma ** 2\n",
    "    data_precision = n / data_sigma ** 2\n",
    "\n",
    "    # MAP is a precision-weighted average\n",
    "    map_mu = (prior_precision * prior_mu + data_precision * data_mean) / (\n",
    "        prior_precision + data_precision\n",
    "    )\n",
    "    return map_mu\n",
    "\n",
    "\n",
    "def visualize_map_vs_mle() -> None:\n",
    "    \"\"\"Show how MAP interpolates between prior and MLE.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    true_mu = 5.0\n",
    "    data_sigma = 2.0\n",
    "    prior_mu = 0.0  # Prior centered at 0\n",
    "\n",
    "    sample_sizes = [1, 5, 10, 50, 200, 1000]\n",
    "    prior_sigmas = [1.0, 2.0, 5.0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Effect of sample size\n",
    "    for ps, color in zip(prior_sigmas, [COLORS['blue'], COLORS['green'], COLORS['red']]):\n",
    "        map_estimates = []\n",
    "        mle_estimates = []\n",
    "        for n in sample_sizes:\n",
    "            data = rng.normal(true_mu, data_sigma, n)\n",
    "            map_est = map_gaussian_with_prior(data, prior_mu, ps, data_sigma)\n",
    "            mle_est = np.mean(data)\n",
    "            map_estimates.append(map_est)\n",
    "            mle_estimates.append(mle_est)\n",
    "\n",
    "        axes[0].plot(sample_sizes, map_estimates, 'o-', color=color,\n",
    "                      linewidth=2, label=f'MAP (σ_prior={ps})')\n",
    "\n",
    "    axes[0].plot(sample_sizes, mle_estimates, 's--', color=COLORS['gray'],\n",
    "                  linewidth=1.5, label='MLE (no prior)')\n",
    "    axes[0].axhline(true_mu, color='black', linestyle=':', alpha=0.5, label=f'True μ={true_mu}')\n",
    "    axes[0].axhline(prior_mu, color=COLORS['orange'], linestyle=':', alpha=0.5, label=f'Prior μ={prior_mu}')\n",
    "    axes[0].set_xlabel('Sample Size (n)')\n",
    "    axes[0].set_ylabel('Estimated μ')\n",
    "    axes[0].set_title('MAP vs MLE: Effect of Sample Size')\n",
    "    axes[0].legend(fontsize=8)\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "    # Regularization connection\n",
    "    lambda_range = np.linspace(0, 5, 200)\n",
    "    data_small = rng.normal(true_mu, data_sigma, 10)\n",
    "    x_bar = np.mean(data_small)\n",
    "    n = len(data_small)\n",
    "\n",
    "    # Ridge regression analogy: θ_MAP = (X^TX + λI)^{-1} X^Ty\n",
    "    # For 1D: θ_MAP = n·x_bar / (n + λ·σ²) when prior_mu = 0\n",
    "    map_lambda = [n * x_bar / (n + lam * data_sigma ** 2) for lam in lambda_range]\n",
    "\n",
    "    axes[1].plot(lambda_range, map_lambda, color=COLORS['blue'], linewidth=2)\n",
    "    axes[1].axhline(x_bar, color=COLORS['green'], linestyle='--',\n",
    "                     label=f'MLE = {x_bar:.2f}')\n",
    "    axes[1].axhline(0, color=COLORS['red'], linestyle=':', label='Prior (λ→∞)')\n",
    "    axes[1].set_xlabel('Regularization Strength (λ = 1/σ²_prior)')\n",
    "    axes[1].set_ylabel('MAP Estimate')\n",
    "    axes[1].set_title('MAP ↔ Regularization Connection')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('Key insight:')\n",
    "    print('  - MAP with Gaussian prior = Ridge regression (L2 penalty)')\n",
    "    print('  - MAP with Laplace prior = Lasso (L1 penalty)')\n",
    "    print('  - More data → MAP approaches MLE (data overwhelms prior)')\n",
    "\n",
    "\n",
    "visualize_map_vs_mle()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Bayes' Theorem",
    "",
    "Bayes' theorem relates conditional probabilities:",
    "",
    "$$P(\\theta | \\mathcal{D}) = \\frac{P(\\mathcal{D} | \\theta) \\cdot P(\\theta)}{P(\\mathcal{D})}$$",
    "",
    "- $P(\\theta | \\mathcal{D})$ — **posterior** (updated belief after seeing data)",
    "- $P(\\mathcal{D} | \\theta)$ — **likelihood** (how probable the data is under $\\theta$)",
    "- $P(\\theta)$ — **prior** (belief before seeing data)",
    "- $P(\\mathcal{D})$ — **evidence** (normalization constant)",
    "",
    "MAP finds the **mode** of the posterior. Full Bayesian inference computes",
    "the **entire posterior distribution**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def bayesian_coin_inference() -> None:\n",
    "    \"\"\"Demonstrate Bayesian updating for coin fairness.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    true_p = 0.65\n",
    "    all_flips = rng.binomial(1, true_p, size=200)\n",
    "\n",
    "    # Prior: Beta(2, 2) — mild belief that coin is fair\n",
    "    alpha_prior, beta_prior = 2, 2\n",
    "    p_grid = np.linspace(0, 1, 500)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "    axes = axes.ravel()\n",
    "    checkpoints = [0, 1, 5, 20, 50, 200]\n",
    "\n",
    "    for idx, n_obs in enumerate(checkpoints):\n",
    "        observed = all_flips[:n_obs]\n",
    "        heads = observed.sum() if n_obs > 0 else 0\n",
    "        tails = n_obs - heads\n",
    "\n",
    "        # Posterior: Beta(alpha + heads, beta + tails)\n",
    "        alpha_post = alpha_prior + heads\n",
    "        beta_post = beta_prior + tails\n",
    "        posterior = sp_stats.beta.pdf(p_grid, alpha_post, beta_post)\n",
    "        prior = sp_stats.beta.pdf(p_grid, alpha_prior, beta_prior)\n",
    "\n",
    "        # Plot\n",
    "        if n_obs == 0:\n",
    "            axes[idx].plot(p_grid, prior, color=COLORS['blue'], linewidth=2, label='Prior')\n",
    "        else:\n",
    "            axes[idx].fill_between(p_grid, posterior, alpha=0.3, color=COLORS['blue'])\n",
    "            axes[idx].plot(p_grid, posterior, color=COLORS['blue'], linewidth=2, label='Posterior')\n",
    "            axes[idx].plot(p_grid, prior, '--', color=COLORS['gray'],\n",
    "                            linewidth=1, label='Prior', alpha=0.5)\n",
    "\n",
    "        axes[idx].axvline(true_p, color=COLORS['red'], linestyle=':',\n",
    "                           label=f'True p={true_p}', alpha=0.7)\n",
    "\n",
    "        if n_obs > 0:\n",
    "            mle_p = heads / n_obs\n",
    "            map_p = (alpha_post - 1) / (alpha_post + beta_post - 2)\n",
    "            axes[idx].axvline(mle_p, color=COLORS['green'], linestyle='--',\n",
    "                               label=f'MLE={mle_p:.2f}', alpha=0.7)\n",
    "\n",
    "        axes[idx].set_xlabel('p')\n",
    "        axes[idx].set_ylabel('Density')\n",
    "        title = f'n={n_obs}' if n_obs > 0 else 'Prior (n=0)'\n",
    "        if n_obs > 0:\n",
    "            title += f' | H={heads}, T={tails}'\n",
    "        axes[idx].set_title(title)\n",
    "        axes[idx].legend(fontsize=7)\n",
    "        axes[idx].set_xlim(0, 1)\n",
    "\n",
    "    plt.suptitle('Bayesian Coin Inference: Posterior Update', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "bayesian_coin_inference()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Multivariate Gaussian Visualization",
    "",
    "The multivariate Gaussian is the cornerstone of many ML algorithms",
    "(Gaussian Mixture Models, Gaussian Processes, Kalman Filters). Let's",
    "visualize how the covariance matrix shapes the distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_multivariate_gaussians() -> None:\n",
    "    \"\"\"Visualize 2D Gaussians with different covariance matrices.\"\"\"\n",
    "    configs = [\n",
    "        ('Spherical', np.array([[1, 0], [0, 1]]), np.array([0, 0])),\n",
    "        ('Diagonal', np.array([[3, 0], [0, 0.5]]), np.array([0, 0])),\n",
    "        ('Correlated', np.array([[2, 1.5], [1.5, 2]]), np.array([0, 0])),\n",
    "        ('Anti-correlated', np.array([[2, -1.5], [-1.5, 2]]), np.array([0, 0])),\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    x_grid = np.linspace(-5, 5, 100)\n",
    "    y_grid = np.linspace(-5, 5, 100)\n",
    "    xx, yy = np.meshgrid(x_grid, y_grid)\n",
    "    grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "    for idx, (name, cov, mu) in enumerate(configs):\n",
    "        # Compute PDF on grid\n",
    "        pdf_vals = multivariate_gaussian_pdf(grid_points, mu, cov)\n",
    "        pdf_grid = pdf_vals.reshape(xx.shape)\n",
    "\n",
    "        # Contour plot\n",
    "        axes[idx].contourf(xx, yy, pdf_grid, levels=20, cmap='viridis', alpha=0.8)\n",
    "        axes[idx].contour(xx, yy, pdf_grid, levels=5, colors='white', linewidths=0.5)\n",
    "\n",
    "        # Draw samples\n",
    "        rng_mv = np.random.RandomState(SEED)\n",
    "        samples = rng_mv.multivariate_normal(mu, cov, 200)\n",
    "        axes[idx].scatter(samples[:, 0], samples[:, 1], s=5, color='white', alpha=0.5)\n",
    "\n",
    "        # Show eigenvectors of covariance\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "        for i in range(2):\n",
    "            vec = eigvecs[:, i] * np.sqrt(eigvals[i]) * 2\n",
    "            axes[idx].annotate('', xy=mu + vec, xytext=mu,\n",
    "                                arrowprops=dict(arrowstyle='->', color=COLORS['red'], lw=2))\n",
    "\n",
    "        axes[idx].set_xlabel('$x_1$')\n",
    "        axes[idx].set_ylabel('$x_2$')\n",
    "        axes[idx].set_title(f'{name}\\nΣ = {cov.tolist()}')\n",
    "        axes[idx].set_aspect('equal')\n",
    "        axes[idx].set_xlim(-5, 5)\n",
    "        axes[idx].set_ylim(-5, 5)\n",
    "\n",
    "    plt.suptitle('Multivariate Gaussian: Effect of Covariance', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_multivariate_gaussians()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 2 — Putting It All Together: Statistical Toolkit",
    "",
    "Let's assemble our probability primitives into a reusable toolkit for",
    "statistical analysis in ML."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class StatisticalToolkit:\n",
    "    \"\"\"Reusable statistical analysis toolkit for ML.\n",
    "\n",
    "    Provides methods for distribution fitting, hypothesis testing,\n",
    "    confidence intervals, and distribution comparison.\n",
    "\n",
    "    Attributes:\n",
    "        data: The data array being analyzed.\n",
    "        n: Number of samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray) -> None:\n",
    "        \"\"\"Initialize with data.\n",
    "\n",
    "        Args:\n",
    "            data: 1D array of observations.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.n = len(data)\n",
    "\n",
    "    def descriptive_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"Compute comprehensive descriptive statistics.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with statistic names and values.\n",
    "        \"\"\"\n",
    "        stats_dict = {\n",
    "            'Count': self.n,\n",
    "            'Mean': np.mean(self.data),\n",
    "            'Std': np.std(self.data, ddof=1),\n",
    "            'Min': np.min(self.data),\n",
    "            'Q1 (25%)': np.percentile(self.data, 25),\n",
    "            'Median': np.median(self.data),\n",
    "            'Q3 (75%)': np.percentile(self.data, 75),\n",
    "            'Max': np.max(self.data),\n",
    "            'Skewness': sp_stats.skew(self.data),\n",
    "            'Kurtosis': sp_stats.kurtosis(self.data),\n",
    "            'IQR': np.percentile(self.data, 75) - np.percentile(self.data, 25),\n",
    "        }\n",
    "        return pd.DataFrame({\n",
    "            'Statistic': list(stats_dict.keys()),\n",
    "            'Value': [f'{v:.4f}' if isinstance(v, float) else str(v)\n",
    "                      for v in stats_dict.values()],\n",
    "        })\n",
    "\n",
    "    def confidence_interval(\n",
    "        self,\n",
    "        confidence: float = 0.95,\n",
    "    ) -> tuple[float, float]:\n",
    "        \"\"\"Compute confidence interval for the mean.\n",
    "\n",
    "        Uses the t-distribution for small samples.\n",
    "\n",
    "        Args:\n",
    "            confidence: Confidence level (e.g., 0.95 for 95%).\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (lower_bound, upper_bound).\n",
    "        \"\"\"\n",
    "        mean = np.mean(self.data)\n",
    "        se = sp_stats.sem(self.data)\n",
    "        alpha = 1 - confidence\n",
    "        t_crit = sp_stats.t.ppf(1 - alpha / 2, df=self.n - 1)\n",
    "        return (mean - t_crit * se, mean + t_crit * se)\n",
    "\n",
    "    def normality_test(self) -> dict[str, float]:\n",
    "        \"\"\"Test if data follows a normal distribution.\n",
    "\n",
    "        Uses the Shapiro-Wilk test (best for n < 5000).\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with test statistic and p-value.\n",
    "        \"\"\"\n",
    "        sample = self.data[:5000] if self.n > 5000 else self.data\n",
    "        stat, p_value = sp_stats.shapiro(sample)\n",
    "        return {'statistic': stat, 'p_value': p_value}\n",
    "\n",
    "    def fit_gaussian(self) -> tuple[float, float]:\n",
    "        \"\"\"Fit Gaussian distribution via MLE.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (mu_mle, sigma_mle).\n",
    "        \"\"\"\n",
    "        return mle_gaussian(self.data)\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "toolkit = StatisticalToolkit(y_housing)\n",
    "print('=== California Housing Target — Descriptive Statistics ===')\n",
    "print(toolkit.descriptive_stats().to_string(index=False))\n",
    "print()\n",
    "\n",
    "ci_low, ci_high = toolkit.confidence_interval(0.95)\n",
    "print(f'95% CI for mean: [{ci_low:.4f}, {ci_high:.4f}]')\n",
    "\n",
    "norm_test = toolkit.normality_test()\n",
    "print(f'\\nNormality test: statistic={norm_test[\"statistic\"]:.4f}, '\n",
    "      f'p-value={norm_test[\"p_value\"]:.4e}')\n",
    "print(f'Normal? {\"Yes\" if norm_test[\"p_value\"] > 0.05 else \"No\"} (at α=0.05)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 3 — Application: Statistical Analysis of Housing Data",
    "",
    "Let's apply our tools to analyze the California Housing dataset",
    "and demonstrate how probability connects to ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Central Limit Theorem Demonstration",
    "",
    "The CLT says that sample means converge to a Gaussian, regardless",
    "of the underlying distribution. This is why Gaussian assumptions",
    "work surprisingly well in practice."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_clt() -> None:\n",
    "    \"\"\"Demonstrate the Central Limit Theorem with different source distributions.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    distributions = {\n",
    "        'Exponential (λ=1)': lambda n: rng.exponential(1.0, n),\n",
    "        'Uniform (0, 1)': lambda n: rng.uniform(0, 1, n),\n",
    "        'Bernoulli (p=0.3)': lambda n: rng.binomial(1, 0.3, n),\n",
    "    }\n",
    "\n",
    "    sample_sizes = [1, 5, 30, 100]\n",
    "    n_simulations = 5000\n",
    "\n",
    "    fig, axes = plt.subplots(len(distributions), len(sample_sizes),\n",
    "                              figsize=(16, 10), sharex=False)\n",
    "\n",
    "    for row, (dist_name, sampler) in enumerate(distributions.items()):\n",
    "        for col, n in enumerate(sample_sizes):\n",
    "            means = [sampler(n).mean() for _ in range(n_simulations)]\n",
    "\n",
    "            axes[row, col].hist(means, bins=40, density=True,\n",
    "                                 color=COLOR_LIST[row], alpha=0.7, edgecolor='white')\n",
    "\n",
    "            # Overlay theoretical Gaussian\n",
    "            grand_mean = np.mean(means)\n",
    "            grand_std = np.std(means)\n",
    "            x_plot = np.linspace(grand_mean - 4 * grand_std, grand_mean + 4 * grand_std, 200)\n",
    "            axes[row, col].plot(x_plot, gaussian_pdf(x_plot, grand_mean, grand_std),\n",
    "                                 'k--', linewidth=1.5, label='Gaussian fit')\n",
    "\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(f'n = {n}', fontsize=11)\n",
    "            if col == 0:\n",
    "                axes[row, col].set_ylabel(dist_name, fontsize=10)\n",
    "\n",
    "    plt.suptitle('Central Limit Theorem: Sample Mean Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('Observation: Even n=30 gives approximately Gaussian sample means,')\n",
    "    print('regardless of the source distribution.')\n",
    "\n",
    "\n",
    "demonstrate_clt()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Law of Large Numbers",
    "",
    "The LLN states that the sample mean converges to the true mean as",
    "$n \\to \\infty$. This is why SGD works — each mini-batch gradient",
    "is a noisy estimate that averages out over many steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_law_of_large_numbers() -> None:\n",
    "    \"\"\"Visualize how running averages converge to the true mean.\"\"\"\n",
    "    rng_lln = np.random.RandomState(SEED)\n",
    "\n",
    "    distributions = {\n",
    "        'Gaussian (μ=5)': (rng_lln.normal, {'loc': 5.0, 'scale': 2.0}),\n",
    "        'Exponential (λ=0.5)': (rng_lln.exponential, {'scale': 2.0}),\n",
    "        'Uniform (0, 10)': (rng_lln.uniform, {'low': 0, 'high': 10}),\n",
    "    }\n",
    "    true_means = [5.0, 2.0, 5.0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    n_max = 5000\n",
    "\n",
    "    for idx, ((name, (sampler, params)), true_mean) in enumerate(\n",
    "        zip(distributions.items(), true_means)\n",
    "    ):\n",
    "        samples = sampler(size=n_max, **params)\n",
    "        running_mean = np.cumsum(samples) / np.arange(1, n_max + 1)\n",
    "\n",
    "        axes[idx].plot(running_mean, color=COLOR_LIST[idx], linewidth=1, alpha=0.8)\n",
    "        axes[idx].axhline(true_mean, color=COLORS['red'], linestyle='--',\n",
    "                           label=f'True μ = {true_mean}', linewidth=1.5)\n",
    "        axes[idx].fill_between(\n",
    "            range(n_max),\n",
    "            true_mean - 2 * np.std(samples) / np.sqrt(np.arange(1, n_max + 1)),\n",
    "            true_mean + 2 * np.std(samples) / np.sqrt(np.arange(1, n_max + 1)),\n",
    "            alpha=0.1, color=COLORS['red'],\n",
    "        )\n",
    "        axes[idx].set_xlabel('Number of Samples')\n",
    "        axes[idx].set_ylabel('Running Mean')\n",
    "        axes[idx].set_title(name)\n",
    "        axes[idx].legend(fontsize=9)\n",
    "        axes[idx].grid(True, alpha=0.2)\n",
    "\n",
    "    plt.suptitle('Law of Large Numbers: Running Mean → True Mean', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "demonstrate_law_of_large_numbers()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Distribution Fitting on Real Data",
    "",
    "Let's fit different distributions to housing features and compare",
    "their log-likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def fit_distributions_to_housing() -> None:\n",
    "    \"\"\"Fit multiple distributions to housing features and compare.\"\"\"\n",
    "    features_to_test = {\n",
    "        'MedInc': X_housing[:, 0],\n",
    "        'HouseAge': X_housing[:, 1],\n",
    "        'MedHouseVal': y_housing,\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for idx, (name, data) in enumerate(features_to_test.items()):\n",
    "        axes[idx].hist(data, bins=50, density=True, color=COLORS['gray'],\n",
    "                        alpha=0.5, edgecolor='white', label='Data')\n",
    "\n",
    "        x_range = np.linspace(data.min(), data.max(), 200)\n",
    "\n",
    "        # Fit Gaussian\n",
    "        mu, sigma = mle_gaussian(data)\n",
    "        axes[idx].plot(x_range, gaussian_pdf(x_range, mu, sigma),\n",
    "                        color=COLORS['blue'], linewidth=2, label=f'Gaussian (μ={mu:.2f})')\n",
    "\n",
    "        # Fit Log-normal (for positive data)\n",
    "        if data.min() > 0:\n",
    "            log_data = np.log(data)\n",
    "            mu_ln, sigma_ln = mle_gaussian(log_data)\n",
    "            lognorm_pdf = gaussian_pdf(np.log(x_range), mu_ln, sigma_ln) / x_range\n",
    "            axes[idx].plot(x_range, lognorm_pdf,\n",
    "                            color=COLORS['red'], linewidth=2, label='Log-Normal')\n",
    "\n",
    "        axes[idx].set_xlabel(name)\n",
    "        axes[idx].set_ylabel('Density')\n",
    "        axes[idx].set_title(f'{name} — Distribution Fit')\n",
    "        axes[idx].legend(fontsize=9)\n",
    "        axes[idx].grid(True, alpha=0.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "fit_distributions_to_housing()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Bootstrap Confidence Intervals",
    "",
    "The bootstrap is a powerful non-parametric method for estimating",
    "uncertainty. Instead of assuming a distribution, it resamples the data",
    "with replacement to build an empirical sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def bootstrap_confidence_interval(\n",
    "    data: np.ndarray,\n",
    "    statistic_fn: callable,\n",
    "    n_bootstrap: int = 5000,\n",
    "    confidence: float = 0.95,\n",
    ") -> tuple[float, float, np.ndarray]:\n",
    "    \"\"\"Compute bootstrap confidence interval for any statistic.\n",
    "\n",
    "    Args:\n",
    "        data: Input data array.\n",
    "        statistic_fn: Function that computes the statistic from data.\n",
    "        n_bootstrap: Number of bootstrap samples.\n",
    "        confidence: Confidence level.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (lower, upper, bootstrap_distribution).\n",
    "    \"\"\"\n",
    "    rng_bs = np.random.RandomState(SEED)\n",
    "    n = len(data)\n",
    "    bootstrap_stats = np.zeros(n_bootstrap)\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        resample = data[rng_bs.choice(n, size=n, replace=True)]\n",
    "        bootstrap_stats[i] = statistic_fn(resample)\n",
    "\n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(bootstrap_stats, 100 * alpha / 2)\n",
    "    upper = np.percentile(bootstrap_stats, 100 * (1 - alpha / 2))\n",
    "    return lower, upper, bootstrap_stats\n",
    "\n",
    "\n",
    "# Bootstrap CI for mean, median, and std of housing prices\n",
    "stats_to_test = {\n",
    "    'Mean': np.mean,\n",
    "    'Median': np.median,\n",
    "    'Std': lambda x: np.std(x, ddof=1),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for idx, (name, fn) in enumerate(stats_to_test.items()):\n",
    "    low, high, boot_dist = bootstrap_confidence_interval(y_housing, fn)\n",
    "    point_est = fn(y_housing)\n",
    "\n",
    "    axes[idx].hist(boot_dist, bins=50, color=COLOR_LIST[idx], alpha=0.7, edgecolor='white')\n",
    "    axes[idx].axvline(point_est, color='black', linestyle='-', linewidth=2, label=f'{name}={point_est:.4f}')\n",
    "    axes[idx].axvline(low, color=COLORS['red'], linestyle='--', label=f'95% CI: [{low:.4f}')\n",
    "    axes[idx].axvline(high, color=COLORS['red'], linestyle='--', label=f'         {high:.4f}]')\n",
    "    axes[idx].set_xlabel(name)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].set_title(f'Bootstrap {name}')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Bootstrap Confidence Intervals — MedHouseVal', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Covariance and Correlation",
    "",
    "The covariance matrix captures linear relationships between features.",
    "Correlation normalizes by standard deviations for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_covariance_from_scratch(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute covariance matrix from scratch.\n",
    "\n",
    "    Args:\n",
    "        X: Data matrix of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix of shape (n_features, n_features).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    X_centered = X - X.mean(axis=0)\n",
    "    return (X_centered.T @ X_centered) / (n - 1)\n",
    "\n",
    "\n",
    "def compute_correlation_from_scratch(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute Pearson correlation matrix from scratch.\n",
    "\n",
    "    Args:\n",
    "        X: Data matrix of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        Correlation matrix of shape (n_features, n_features).\n",
    "    \"\"\"\n",
    "    cov = compute_covariance_from_scratch(X)\n",
    "    stds = np.sqrt(np.diag(cov))\n",
    "    return cov / np.outer(stds, stds)\n",
    "\n",
    "\n",
    "# Compute and verify\n",
    "our_cov = compute_covariance_from_scratch(X_housing)\n",
    "np_cov = np.cov(X_housing.T)\n",
    "assert our_cov.shape == (8, 8), f'Expected (8, 8), got {our_cov.shape}'\n",
    "print(f'Covariance match: {np.allclose(our_cov, np_cov)}')\n",
    "\n",
    "our_corr = compute_correlation_from_scratch(X_housing)\n",
    "np_corr = np.corrcoef(X_housing.T)\n",
    "print(f'Correlation match: {np.allclose(our_corr, np_corr)}')\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "im = ax.imshow(our_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(8))\n",
    "ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax.set_yticks(range(8))\n",
    "ax.set_yticklabels(feature_names)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        ax.text(j, i, f'{our_corr[i,j]:.2f}', ha='center', va='center', fontsize=8)\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "ax.set_title('Feature Correlation Matrix — California Housing')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 4 — Evaluation & Analysis",
    "",
    "Let's evaluate our statistical tools, examine hypothesis testing,",
    "and build a comprehensive reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MLE Convergence: Effect of Sample Size",
    "",
    "How quickly does MLE converge to the true parameters? Let's measure",
    "this empirically."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_mle_convergence() -> None:\n",
    "    \"\"\"Show how MLE estimates improve with more data.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    true_mu, true_sigma = 3.0, 1.5\n",
    "\n",
    "    sample_sizes = [5, 10, 25, 50, 100, 250, 500, 1000, 5000]\n",
    "    n_trials = 200\n",
    "\n",
    "    mu_errors = []\n",
    "    sigma_errors = []\n",
    "\n",
    "    for n in sample_sizes:\n",
    "        mu_errs = []\n",
    "        sigma_errs = []\n",
    "        for _ in range(n_trials):\n",
    "            data = rng.normal(true_mu, true_sigma, n)\n",
    "            mu_hat, sigma_hat = mle_gaussian(data)\n",
    "            mu_errs.append(abs(mu_hat - true_mu))\n",
    "            sigma_errs.append(abs(sigma_hat - true_sigma))\n",
    "        mu_errors.append(np.mean(mu_errs))\n",
    "        sigma_errors.append(np.mean(sigma_errs))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].loglog(sample_sizes, mu_errors, 'o-', color=COLORS['blue'], linewidth=2, label='|μ̂ - μ|')\n",
    "    # Theoretical: error ∝ 1/√n\n",
    "    theoretical = mu_errors[0] * np.sqrt(sample_sizes[0]) / np.sqrt(sample_sizes)\n",
    "    axes[0].loglog(sample_sizes, theoretical, '--', color=COLORS['red'], label='O(1/√n)')\n",
    "    axes[0].set_xlabel('Sample Size (n)')\n",
    "    axes[0].set_ylabel('Mean Absolute Error')\n",
    "    axes[0].set_title('MLE Convergence: Mean')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "    axes[1].loglog(sample_sizes, sigma_errors, 'o-', color=COLORS['green'], linewidth=2, label='|σ̂ - σ|')\n",
    "    theoretical_s = sigma_errors[0] * np.sqrt(sample_sizes[0]) / np.sqrt(sample_sizes)\n",
    "    axes[1].loglog(sample_sizes, theoretical_s, '--', color=COLORS['red'], label='O(1/√n)')\n",
    "    axes[1].set_xlabel('Sample Size (n)')\n",
    "    axes[1].set_ylabel('Mean Absolute Error')\n",
    "    axes[1].set_title('MLE Convergence: Std Dev')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    plt.suptitle('MLE Convergence Rate', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('MLE converges at rate O(1/√n) — doubling precision needs 4× more data.')\n",
    "\n",
    "\n",
    "analyze_mle_convergence()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hypothesis Testing for Model Comparison",
    "",
    "In ML, we often need to determine if one model is significantly better",
    "than another. The paired t-test compares two sets of scores."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_hypothesis_testing() -> None:\n",
    "    \"\"\"Show hypothesis testing for model comparison.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    # Simulate cross-validation scores for two models\n",
    "    n_folds = 30\n",
    "    model_a_scores = rng.normal(0.85, 0.03, n_folds)  # Mean 85%\n",
    "    model_b_scores = rng.normal(0.87, 0.03, n_folds)  # Mean 87%\n",
    "    model_c_scores = rng.normal(0.855, 0.03, n_folds)  # Mean 85.5% (not significant)\n",
    "\n",
    "    # Paired t-test: A vs B\n",
    "    t_stat_ab, p_val_ab = sp_stats.ttest_rel(model_a_scores, model_b_scores)\n",
    "    t_stat_ac, p_val_ac = sp_stats.ttest_rel(model_a_scores, model_c_scores)\n",
    "\n",
    "    print('=== Model Comparison via Paired t-test ===')\n",
    "    results = pd.DataFrame({\n",
    "        'Comparison': ['A vs B', 'A vs C'],\n",
    "        'Mean A': [f'{model_a_scores.mean():.4f}', f'{model_a_scores.mean():.4f}'],\n",
    "        'Mean Other': [f'{model_b_scores.mean():.4f}', f'{model_c_scores.mean():.4f}'],\n",
    "        'Diff': [f'{(model_b_scores - model_a_scores).mean():.4f}',\n",
    "                 f'{(model_c_scores - model_a_scores).mean():.4f}'],\n",
    "        't-stat': [f'{t_stat_ab:.3f}', f'{t_stat_ac:.3f}'],\n",
    "        'p-value': [f'{p_val_ab:.4f}', f'{p_val_ac:.4f}'],\n",
    "        'Significant?': [\n",
    "            'Yes' if p_val_ab < 0.05 else 'No',\n",
    "            'Yes' if p_val_ac < 0.05 else 'No',\n",
    "        ],\n",
    "    })\n",
    "    print(results.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Box plot comparison\n",
    "    bp = axes[0].boxplot([model_a_scores, model_b_scores, model_c_scores],\n",
    "                          labels=['Model A', 'Model B', 'Model C'], patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], [COLORS['blue'], COLORS['green'], COLORS['orange']]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Cross-Validation Score Distributions')\n",
    "    axes[0].grid(True, axis='y', alpha=0.2)\n",
    "\n",
    "    # Difference distribution\n",
    "    diff_ab = model_b_scores - model_a_scores\n",
    "    diff_ac = model_c_scores - model_a_scores\n",
    "    axes[1].hist(diff_ab, bins=15, color=COLORS['green'], alpha=0.6, label='B - A', edgecolor='white')\n",
    "    axes[1].hist(diff_ac, bins=15, color=COLORS['orange'], alpha=0.6, label='C - A', edgecolor='white')\n",
    "    axes[1].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_xlabel('Score Difference')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Score Differences')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('Interpretation:')\n",
    "    print('  p < 0.05 → reject null hypothesis (models perform differently)')\n",
    "    print('  p > 0.05 → cannot reject null (no significant difference)')\n",
    "\n",
    "\n",
    "demonstrate_hypothesis_testing()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Error Analysis: Common Probabilistic Mistakes in ML",
    "",
    "Let's examine common mistakes when applying probability in ML."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_common_mistakes() -> None:\n",
    "    \"\"\"Show common probabilistic mistakes and their consequences.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    print('=== Common Probabilistic Mistakes in ML ===')\n",
    "    print()\n",
    "\n",
    "    # 1. Confusing P(A|B) with P(B|A) — base rate fallacy\n",
    "    print('1. BASE RATE FALLACY: P(disease|positive) ≠ P(positive|disease)')\n",
    "    sensitivity = 0.99  # P(positive|disease)\n",
    "    prevalence = 0.001  # P(disease)\n",
    "    false_positive_rate = 0.05  # P(positive|healthy)\n",
    "\n",
    "    p_positive = sensitivity * prevalence + false_positive_rate * (1 - prevalence)\n",
    "    p_disease_given_positive = (sensitivity * prevalence) / p_positive\n",
    "\n",
    "    print(f'  P(positive|disease) = {sensitivity:.0%} (sensitivity)')\n",
    "    print(f'  P(disease) = {prevalence:.1%} (prevalence)')\n",
    "    print(f'  P(positive|healthy) = {false_positive_rate:.0%} (false positive rate)')\n",
    "    print(f'  P(disease|positive) = {p_disease_given_positive:.1%} (much lower than 99%!)')\n",
    "    print()\n",
    "\n",
    "    # 2. Multiple comparisons problem\n",
    "    print('2. MULTIPLE COMPARISONS: Testing many models inflates false positives')\n",
    "    n_models = 20\n",
    "    p_values = rng.uniform(0, 1, n_models)  # All models are equally bad\n",
    "    significant = np.sum(p_values < 0.05)\n",
    "    print(f'  {n_models} random models tested at α=0.05')\n",
    "    print(f'  \"Significant\" results: {significant} (expected ≈ {n_models * 0.05:.0f})')\n",
    "    print(f'  Bonferroni correction: α = 0.05/{n_models} = {0.05/n_models:.4f}')\n",
    "    bonferroni_sig = np.sum(p_values < 0.05 / n_models)\n",
    "    print(f'  Significant after correction: {bonferroni_sig}')\n",
    "    print()\n",
    "\n",
    "    # 3. Independence assumption\n",
    "    print('3. INDEPENDENCE ASSUMPTION: Most real features are not independent')\n",
    "    corr_target = compute_correlation_from_scratch(\n",
    "        np.column_stack([X_housing[:, 0], X_housing[:, 5], y_housing])\n",
    "    )\n",
    "    print(f'  Corr(MedInc, AveOccup) = {corr_target[0, 1]:.4f}')\n",
    "    print(f'  Naive Bayes assumes independence — works despite this!')\n",
    "    print()\n",
    "\n",
    "    # Summary table\n",
    "    mistakes = pd.DataFrame({\n",
    "        'Mistake': [\n",
    "            'Base rate fallacy',\n",
    "            'Multiple comparisons',\n",
    "            'Independence assumption',\n",
    "            'Small sample inference',\n",
    "            'Ignoring priors',\n",
    "        ],\n",
    "        'Description': [\n",
    "            'Confusing P(A|B) with P(B|A)',\n",
    "            'Testing many hypotheses without correction',\n",
    "            'Assuming features are independent',\n",
    "            'Drawing conclusions from n < 30',\n",
    "            'Using flat priors when domain knowledge exists',\n",
    "        ],\n",
    "        'ML Impact': [\n",
    "            'Misinterpreting classifier outputs',\n",
    "            'Overfitting to validation set',\n",
    "            'Naive Bayes, feature selection bias',\n",
    "            'Unreliable cross-validation scores',\n",
    "            'Missing regularization opportunity',\n",
    "        ],\n",
    "    })\n",
    "    print('=== Summary ===')\n",
    "    print(mistakes.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_common_mistakes()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Probability & Statistics Reference for ML",
    "",
    "A comprehensive reference mapping probability concepts to their ML applications."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_probability_reference() -> None:\n",
    "    \"\"\"Create a probability/statistics reference for the course.\"\"\"\n",
    "    reference = pd.DataFrame({\n",
    "        'Concept': [\n",
    "            'Gaussian distribution', 'Bernoulli/Categorical',\n",
    "            'Maximum Likelihood', 'MAP estimation',\n",
    "            'Bayes theorem', 'Central Limit Theorem',\n",
    "            'Covariance/Correlation', 'Confidence intervals',\n",
    "            'Hypothesis testing', 'KL divergence',\n",
    "        ],\n",
    "        'ML Application': [\n",
    "            'Weight initialization, noise models',\n",
    "            'Classification outputs, sampling',\n",
    "            'Training = maximizing likelihood',\n",
    "            'Regularization = adding a prior',\n",
    "            'Posterior estimation, Naive Bayes',\n",
    "            'Why batch means are Gaussian',\n",
    "            'Feature selection, PCA (Module 1-06)',\n",
    "            'Uncertainty quantification',\n",
    "            'Model comparison, A/B testing',\n",
    "            'Loss functions, VAEs (see 01-08)',\n",
    "        ],\n",
    "        'Where Used': [\n",
    "            'Modules 1-20', 'Modules 2, 5, 7, 10',\n",
    "            'Modules 2-20', 'Modules 2, 4, 13',\n",
    "            'Modules 2, 4, 7', 'Modules 4, 16',\n",
    "            'Modules 1, 3, 4', 'Modules 4, 20',\n",
    "            'Modules 4, 20', 'Modules 1, 5, 11',\n",
    "        ],\n",
    "    })\n",
    "    print('=== Probability & Statistics for ML — Reference ===')\n",
    "    print(reference.to_string(index=False))\n",
    "\n",
    "\n",
    "build_probability_reference()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 5 — Summary & Lessons Learned",
    "",
    "### Key Takeaways",
    "",
    "1. **Training = Maximum Likelihood.** Minimizing cross-entropy loss is exactly",
    "   maximizing the log-likelihood of the data under the model. Understanding",
    "   this connection demystifies why loss functions look the way they do.",
    "",
    "2. **Regularization = Prior belief.** MAP estimation adds a prior to MLE.",
    "   Gaussian prior → L2 (Ridge), Laplace prior → L1 (Lasso). More data",
    "   reduces the influence of the prior.",
    "",
    "3. **Bayes' theorem updates beliefs.** The posterior combines prior knowledge",
    "   with evidence. This is the foundation of Bayesian methods, Naive Bayes",
    "   classification, and probabilistic graphical models.",
    "",
    "4. **The CLT explains Gaussian ubiquity.** Sample means converge to Gaussians",
    "   regardless of the source distribution. This justifies many Gaussian",
    "   assumptions in ML (batch statistics, noise models, initialization).",
    "",
    "5. **Statistical testing prevents false discoveries.** Use paired t-tests",
    "   for model comparison, apply Bonferroni correction for multiple tests,",
    "   and be aware of base rate fallacies when interpreting probabilities.",
    "",
    "### What's Next",
    "",
    "→ **01-08 (Information Theory for ML)** covers entropy, cross-entropy,",
    "  KL divergence, and mutual information — the information-theoretic",
    "  perspective on the same loss functions we derived probabilistically here.",
    "",
    "### Going Further",
    "",
    "- [Pattern Recognition and Machine Learning (Bishop)](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/) —",
    "  The classic probabilistic ML textbook",
    "- [Probabilistic Machine Learning (Murphy)](https://probml.github.io/pml-book/) —",
    "  Modern treatment with deep learning connections",
    "- [Seeing Theory](https://seeing-theory.brown.edu/) — Interactive probability",
    "  visualizations"
   ]
  }
 ]
}