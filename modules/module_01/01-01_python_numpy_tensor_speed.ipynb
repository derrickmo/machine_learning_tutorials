{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations\n",
    "## 01-01: Python, NumPy & Tensor Speed\n",
    "\n",
    "**Objective:** Understand *why* vectorized array operations are orders of magnitude faster\n",
    "than plain Python loops, and learn to write NumPy/PyTorch code that exploits this speed.\n",
    "\n",
    "**Prerequisites:** Basic Python (lists, loops, functions). No prior NumPy or PyTorch experience required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 0 — Setup & Prerequisites\n",
    "\n",
    "This notebook is the very first in the course. We start with the most fundamental question\n",
    "in scientific computing: **why are some code patterns 100× faster than others on the same data?**\n",
    "\n",
    "We will build a benchmarking framework from scratch, systematically measure Python loops vs\n",
    "NumPy vectorization vs PyTorch tensors, explore broadcasting rules, and study how memory\n",
    "layout affects performance. By the end, you will have a deep intuition for writing fast\n",
    "numerical code — a skill that underpins every notebook in this course.\n",
    "\n",
    "**Prerequisites:** None — this is the entry point to the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'NumPy: {np.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "import random\n",
    "\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "# Benchmark parameters\n",
    "SMALL_SIZE = 1_000\n",
    "MEDIUM_SIZE = 100_000\n",
    "LARGE_SIZE = 1_000_000\n",
    "XL_SIZE = 10_000_000\n",
    "\n",
    "# Timing parameters\n",
    "NUM_WARMUP = 2       # Warmup runs before timing\n",
    "NUM_TIMED = 5        # Timed runs to average\n",
    "\n",
    "# Visualization\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 — Vectorization from Scratch\n",
    "\n",
    "### Why Vectorization Matters\n",
    "\n",
    "Python is an interpreted, dynamically typed language. Every time you write `a + b` in a\n",
    "Python loop, the interpreter must:\n",
    "\n",
    "1. Look up the types of `a` and `b`\n",
    "2. Find the appropriate `__add__` method\n",
    "3. Check for overflow, type coercion, etc.\n",
    "4. Allocate memory for the result\n",
    "5. Return the result as a new Python object\n",
    "\n",
    "This overhead costs ~100 nanoseconds **per operation**. For 10 million elements, that adds up\n",
    "to ~1 second of pure overhead — before any actual math happens.\n",
    "\n",
    "**Vectorized libraries** like NumPy bypass this by:\n",
    "- Storing data in contiguous C arrays (not Python objects)\n",
    "- Dispatching a single C/Fortran function call that loops over the entire array\n",
    "- Using SIMD (Single Instruction, Multiple Data) CPU instructions\n",
    "- Avoiding Python object creation for intermediate results\n",
    "\n",
    "The result? The same operation can be **50–500× faster**. Let's measure it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Building a Benchmarking Framework\n",
    "\n",
    "Before we can compare approaches, we need a reliable way to measure execution time.\n",
    "We'll build a `measure_time` function that handles warmup runs (to prime CPU caches)\n",
    "and multiple timed runs (to reduce noise from OS scheduling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(\n",
    "    func: callable,\n",
    "    num_warmup: int = NUM_WARMUP,\n",
    "    num_timed: int = NUM_TIMED,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Measure execution time of a zero-argument callable.\n",
    "\n",
    "    Runs warmup iterations first (to prime CPU caches), then times\n",
    "    multiple runs and returns the mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        func: Zero-argument callable to benchmark.\n",
    "        num_warmup: Number of warmup runs before timing.\n",
    "        num_timed: Number of timed runs to average.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (mean_seconds, std_seconds).\n",
    "    \"\"\"\n",
    "    # Warmup: bring data into CPU cache\n",
    "    for _ in range(num_warmup):\n",
    "        func()\n",
    "\n",
    "    # Timed runs\n",
    "    times: list[float] = []\n",
    "    for _ in range(num_timed):\n",
    "        start = time.perf_counter()\n",
    "        func()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "\n",
    "    return float(np.mean(times)), float(np.std(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our timing function works correctly by measuring a known operation — sleeping\n",
    "for a fixed duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: time.sleep(0.01) should take ~10 ms\n",
    "mean_t, std_t = measure_time(lambda: time.sleep(0.01), num_warmup=1, num_timed=3)\n",
    "print(f'time.sleep(0.01): {mean_t*1000:.1f} ± {std_t*1000:.1f} ms (expected ~10 ms)')\n",
    "assert 8.0 < mean_t * 1000 < 15.0, f'Timing sanity check failed: {mean_t*1000:.1f} ms'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Speed Gap: Python Loops vs NumPy\n",
    "\n",
    "Let's start with the most basic operation: **element-wise addition** of two arrays.\n",
    "We'll implement it three ways and measure the speed difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_add(a: list[float], b: list[float]) -> list[float]:\n",
    "    \"\"\"Element-wise addition using a Python for loop.\n",
    "\n",
    "    Args:\n",
    "        a: First list of numbers.\n",
    "        b: Second list of numbers.\n",
    "\n",
    "    Returns:\n",
    "        List of element-wise sums.\n",
    "    \"\"\"\n",
    "    result = [0.0] * len(a)\n",
    "    for i in range(len(a)):\n",
    "        result[i] = a[i] + b[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "def python_add_comprehension(a: list[float], b: list[float]) -> list[float]:\n",
    "    \"\"\"Element-wise addition using a list comprehension.\n",
    "\n",
    "    Args:\n",
    "        a: First list of numbers.\n",
    "        b: Second list of numbers.\n",
    "\n",
    "    Returns:\n",
    "        List of element-wise sums.\n",
    "    \"\"\"\n",
    "    return [x + y for x, y in zip(a, b)]\n",
    "\n",
    "\n",
    "def numpy_add(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Element-wise addition using NumPy vectorization.\n",
    "\n",
    "    Args:\n",
    "        a: First NumPy array.\n",
    "        b: Second NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of element-wise sums.\n",
    "    \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll benchmark all three on the same data. We create lists and arrays of the same\n",
    "size and measure how long each approach takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "np.random.seed(SEED)\n",
    "size = MEDIUM_SIZE\n",
    "\n",
    "list_a = list(np.random.randn(size))\n",
    "list_b = list(np.random.randn(size))\n",
    "arr_a = np.array(list_a)\n",
    "arr_b = np.array(list_b)\n",
    "\n",
    "print(f'Array size: {size:,} elements')\n",
    "print(f'Data type: float64 ({arr_a.dtype})')\n",
    "print()\n",
    "\n",
    "# Benchmark each approach\n",
    "t_loop, s_loop = measure_time(lambda: python_add(list_a, list_b))\n",
    "t_comp, s_comp = measure_time(lambda: python_add_comprehension(list_a, list_b))\n",
    "t_numpy, s_numpy = measure_time(lambda: numpy_add(arr_a, arr_b))\n",
    "\n",
    "print(f'Python for-loop:       {t_loop*1000:8.2f} ± {s_loop*1000:.2f} ms')\n",
    "print(f'List comprehension:    {t_comp*1000:8.2f} ± {s_comp*1000:.2f} ms')\n",
    "print(f'NumPy vectorized:      {t_numpy*1000:8.2f} ± {s_numpy*1000:.2f} ms')\n",
    "print()\n",
    "print(f'Speedup (loop → NumPy):          {t_loop / t_numpy:6.1f}×')\n",
    "print(f'Speedup (comprehension → NumPy): {t_comp / t_numpy:6.1f}×')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy version is dramatically faster — typically **50–200×** compared to pure Python.\n",
    "The list comprehension is somewhat faster than the explicit loop (less Python overhead\n",
    "per iteration), but still nowhere near NumPy's speed.\n",
    "\n",
    "**Why?** NumPy's `+` operator dispatches to a compiled C function that processes the entire\n",
    "array in a single call. Python never touches individual elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Scaling: How Speed Varies with Array Size\n",
    "\n",
    "The speedup ratio isn't constant — it depends on the array size. For very small arrays,\n",
    "NumPy's function call overhead can make it slower than a simple loop. For large arrays,\n",
    "the vectorized advantage grows. Let's map out this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_addition_scaling() -> pd.DataFrame:\n",
    "    \"\"\"Benchmark element-wise addition at various array sizes.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing results for each approach and size.\n",
    "    \"\"\"\n",
    "    sizes = [10, 100, 1_000, 10_000, 100_000, 1_000_000]\n",
    "    records: list[dict] = []\n",
    "\n",
    "    for n in sizes:\n",
    "        np.random.seed(SEED)\n",
    "        lst_a = list(np.random.randn(n))\n",
    "        lst_b = list(np.random.randn(n))\n",
    "        npa = np.array(lst_a)\n",
    "        npb = np.array(lst_b)\n",
    "\n",
    "        t_loop, _ = measure_time(lambda: python_add(lst_a, lst_b), num_warmup=1, num_timed=3)\n",
    "        t_comp, _ = measure_time(lambda: python_add_comprehension(lst_a, lst_b), num_warmup=1, num_timed=3)\n",
    "        t_np, _ = measure_time(lambda: numpy_add(npa, npb), num_warmup=1, num_timed=3)\n",
    "\n",
    "        records.append({\n",
    "            'Size': n,\n",
    "            'Python Loop (ms)': t_loop * 1000,\n",
    "            'List Comp (ms)': t_comp * 1000,\n",
    "            'NumPy (ms)': t_np * 1000,\n",
    "            'Loop/NumPy': t_loop / t_np,\n",
    "        })\n",
    "        print(f'  n={n:>10,}: loop={t_loop*1000:.3f}ms, numpy={t_np*1000:.4f}ms, '\n",
    "              f'speedup={t_loop/t_np:.1f}×')\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "print('Benchmarking addition at various sizes...')\n",
    "scaling_df = benchmark_addition_scaling()\n",
    "print()\n",
    "print(scaling_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this scaling relationship. The log-log plot reveals the **crossover point**\n",
    "where NumPy starts winning, and how the speedup grows with array size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: absolute times\n",
    "axes[0].loglog(scaling_df['Size'], scaling_df['Python Loop (ms)'], 'o-',\n",
    "               label='Python Loop', color='#E53935', linewidth=2, markersize=7)\n",
    "axes[0].loglog(scaling_df['Size'], scaling_df['List Comp (ms)'], 's-',\n",
    "               label='List Comprehension', color='#FF9800', linewidth=2, markersize=7)\n",
    "axes[0].loglog(scaling_df['Size'], scaling_df['NumPy (ms)'], '^-',\n",
    "               label='NumPy Vectorized', color='#1E88E5', linewidth=2, markersize=7)\n",
    "axes[0].set_xlabel('Array Size (n)')\n",
    "axes[0].set_ylabel('Time (ms, log scale)')\n",
    "axes[0].set_title('Element-wise Addition: Absolute Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: speedup ratio\n",
    "axes[1].semilogx(scaling_df['Size'], scaling_df['Loop/NumPy'], 'o-',\n",
    "                 color='#43A047', linewidth=2, markersize=7)\n",
    "axes[1].set_xlabel('Array Size (n)')\n",
    "axes[1].set_ylabel('Speedup (Python Loop / NumPy)')\n",
    "axes[1].set_title('Vectorization Speedup vs Array Size')\n",
    "axes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Break-even')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Maximum observed speedup: {scaling_df[\"Loop/NumPy\"].max():.1f}× '\n",
    "      f'at n={scaling_df.loc[scaling_df[\"Loop/NumPy\"].idxmax(), \"Size\"]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observations:**\n",
    "- For n < ~50, Python loops can be comparable or faster (NumPy has function call overhead)\n",
    "- For n > 1,000, NumPy is consistently 10–100× faster\n",
    "- The speedup generally plateaus once the array is large enough to saturate memory bandwidth\n",
    "\n",
    "This is the **first rule of numerical Python:** always vectorize when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Beyond Addition: Common Operations\n",
    "\n",
    "Addition is the simplest case. Let's verify that the speedup pattern holds across\n",
    "different mathematical operations — dot products, element-wise multiplication,\n",
    "aggregations (sum, mean), and mathematical functions (exp, sin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_dot(a: list[float], b: list[float]) -> float:\n",
    "    \"\"\"Compute dot product using a Python for loop.\n",
    "\n",
    "    Args:\n",
    "        a: First vector as a list.\n",
    "        b: Second vector as a list.\n",
    "\n",
    "    Returns:\n",
    "        Scalar dot product.\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    for i in range(len(a)):\n",
    "        total += a[i] * b[i]\n",
    "    return total\n",
    "\n",
    "\n",
    "def python_sum(a: list[float]) -> float:\n",
    "    \"\"\"Compute sum using a Python for loop.\n",
    "\n",
    "    Args:\n",
    "        a: List of numbers.\n",
    "\n",
    "    Returns:\n",
    "        Sum of all elements.\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    for val in a:\n",
    "        total += val\n",
    "    return total\n",
    "\n",
    "\n",
    "def python_elementwise_mul(a: list[float], b: list[float]) -> list[float]:\n",
    "    \"\"\"Element-wise multiplication using a Python for loop.\n",
    "\n",
    "    Args:\n",
    "        a: First list of numbers.\n",
    "        b: Second list of numbers.\n",
    "\n",
    "    Returns:\n",
    "        List of element-wise products.\n",
    "    \"\"\"\n",
    "    result = [0.0] * len(a)\n",
    "    for i in range(len(a)):\n",
    "        result[i] = a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "def python_exp(a: list[float]) -> list[float]:\n",
    "    \"\"\"Compute element-wise exp using Python's math module.\n",
    "\n",
    "    Args:\n",
    "        a: List of numbers.\n",
    "\n",
    "    Returns:\n",
    "        List of exp(x) for each element.\n",
    "    \"\"\"\n",
    "    import math\n",
    "    return [math.exp(x) for x in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a comprehensive benchmark across all these operations, comparing Python\n",
    "loops against their NumPy equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "np.random.seed(SEED)\n",
    "n_ops = MEDIUM_SIZE\n",
    "list_x = list(np.random.randn(n_ops))\n",
    "list_y = list(np.random.randn(n_ops))\n",
    "arr_x = np.array(list_x)\n",
    "arr_y = np.array(list_y)\n",
    "\n",
    "# Define operation pairs: (name, python_func, numpy_func)\n",
    "operations = [\n",
    "    ('Addition',\n",
    "     lambda: python_add(list_x, list_y),\n",
    "     lambda: arr_x + arr_y),\n",
    "    ('Multiplication',\n",
    "     lambda: python_elementwise_mul(list_x, list_y),\n",
    "     lambda: arr_x * arr_y),\n",
    "    ('Dot Product',\n",
    "     lambda: python_dot(list_x, list_y),\n",
    "     lambda: np.dot(arr_x, arr_y)),\n",
    "    ('Sum',\n",
    "     lambda: python_sum(list_x),\n",
    "     lambda: np.sum(arr_x)),\n",
    "    ('Exp',\n",
    "     lambda: python_exp(list_x),\n",
    "     lambda: np.exp(arr_x)),\n",
    "]\n",
    "\n",
    "op_records: list[dict] = []\n",
    "for name, py_func, np_func in operations:\n",
    "    t_py, _ = measure_time(py_func)\n",
    "    t_np, _ = measure_time(np_func)\n",
    "    speedup = t_py / t_np\n",
    "    op_records.append({\n",
    "        'Operation': name,\n",
    "        'Python (ms)': t_py * 1000,\n",
    "        'NumPy (ms)': t_np * 1000,\n",
    "        'Speedup': speedup,\n",
    "    })\n",
    "\n",
    "ops_df = pd.DataFrame(op_records)\n",
    "print(f'Operation benchmarks (n={n_ops:,}):')\n",
    "print(ops_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize operation speedups\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#1E88E5', '#E53935', '#43A047', '#FF9800', '#9C27B0']\n",
    "bars = ax.barh(ops_df['Operation'], ops_df['Speedup'], color=colors)\n",
    "ax.set_xlabel('Speedup (Python Loop / NumPy)')\n",
    "ax.set_title(f'Vectorization Speedup by Operation (n={n_ops:,})')\n",
    "ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, ops_df['Speedup']):\n",
    "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{val:.0f}×', va='center', fontweight='bold')\n",
    "\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every operation shows significant speedup. The exact ratio varies:\n",
    "- **Sum and dot product** tend to show the highest speedups because they reduce to a single\n",
    "  scalar, avoiding memory allocation for a result array.\n",
    "- **Exp** is compute-heavy, so the SIMD advantage is particularly large.\n",
    "- **Addition and multiplication** are memory-bound — the speedup is limited by how fast\n",
    "  data can be read from RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Matrix Multiplication: Where Vectorization Shines Brightest\n",
    "\n",
    "Matrix multiplication is the most important operation in machine learning. A single\n",
    "forward pass through a neural network is a chain of matrix multiplications. Let's\n",
    "implement it from scratch and compare against NumPy's optimized BLAS (Basic Linear\n",
    "Algebra Subprograms) implementation.\n",
    "\n",
    "For matrices $\\mathbf{A} \\in \\mathbb{R}^{m \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times n}$:\n",
    "\n",
    "$$C_{ij} = \\sum_{p=1}^{k} A_{ip} \\cdot B_{pj}$$\n",
    "\n",
    "The naive implementation requires three nested loops — $O(m \\cdot k \\cdot n)$ operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_matmul(\n",
    "    a: list[list[float]],\n",
    "    b: list[list[float]],\n",
    ") -> list[list[float]]:\n",
    "    \"\"\"Matrix multiplication using triple nested Python loops.\n",
    "\n",
    "    Args:\n",
    "        a: Matrix of shape (m, k) as nested lists.\n",
    "        b: Matrix of shape (k, n) as nested lists.\n",
    "\n",
    "    Returns:\n",
    "        Result matrix of shape (m, n) as nested lists.\n",
    "    \"\"\"\n",
    "    m = len(a)\n",
    "    k = len(a[0])\n",
    "    n = len(b[0])\n",
    "\n",
    "    result = [[0.0] * n for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            total = 0.0\n",
    "            for p in range(k):\n",
    "                total += a[i][p] * b[p][j]\n",
    "            result[i][j] = total\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark this against NumPy's `@` operator (which calls BLAS under the hood)\n",
    "at several matrix sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul_scaling() -> pd.DataFrame:\n",
    "    \"\"\"Benchmark matrix multiplication at various sizes.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing results.\n",
    "    \"\"\"\n",
    "    sizes = [10, 25, 50, 100, 200]\n",
    "    records: list[dict] = []\n",
    "\n",
    "    for n in sizes:\n",
    "        np.random.seed(SEED)\n",
    "        np_a = np.random.randn(n, n)\n",
    "        np_b = np.random.randn(n, n)\n",
    "        py_a = np_a.tolist()\n",
    "        py_b = np_b.tolist()\n",
    "\n",
    "        t_py, _ = measure_time(lambda: python_matmul(py_a, py_b), num_warmup=1, num_timed=2)\n",
    "        t_np, _ = measure_time(lambda: np_a @ np_b, num_warmup=1, num_timed=3)\n",
    "\n",
    "        # Verify correctness\n",
    "        py_result = np.array(python_matmul(py_a, py_b))\n",
    "        np_result = np_a @ np_b\n",
    "        max_error = np.max(np.abs(py_result - np_result))\n",
    "\n",
    "        records.append({\n",
    "            'Size': f'{n}×{n}',\n",
    "            'Python (ms)': t_py * 1000,\n",
    "            'NumPy (ms)': t_np * 1000,\n",
    "            'Speedup': t_py / t_np,\n",
    "            'Max Error': max_error,\n",
    "        })\n",
    "        print(f'  {n:>3}×{n:<3}: python={t_py*1000:>10.2f}ms, '\n",
    "              f'numpy={t_np*1000:>8.4f}ms, '\n",
    "              f'speedup={t_py/t_np:>8.0f}×, error={max_error:.2e}')\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "print('Benchmarking matrix multiplication...')\n",
    "matmul_df = benchmark_matmul_scaling()\n",
    "print()\n",
    "print(matmul_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speedup for matrix multiplication is **enormous** — often 1,000× or more for larger\n",
    "matrices. This is because NumPy calls optimized BLAS libraries (OpenBLAS or MKL) that use:\n",
    "\n",
    "- **Cache-aware blocking:** Splits the matrix into tiles that fit in L1/L2 cache\n",
    "- **SIMD instructions:** Processes 4–8 floats per CPU instruction\n",
    "- **Multi-threading:** Uses multiple CPU cores in parallel\n",
    "\n",
    "This is why `np.dot()` or `@` should **always** be used instead of manual loops for\n",
    "matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Broadcasting: Vectorization Without Shape Matching\n",
    "\n",
    "One of NumPy's most powerful features is **broadcasting** — the ability to perform\n",
    "operations on arrays with different shapes. Broadcasting eliminates the need to\n",
    "manually replicate data, saving both memory and computation time.\n",
    "\n",
    "#### Broadcasting Rules\n",
    "\n",
    "When operating on two arrays, NumPy compares their shapes element-wise, starting from\n",
    "the trailing (rightmost) dimensions:\n",
    "\n",
    "1. If two dimensions are equal, they're compatible\n",
    "2. If one of them is 1, it's broadcast (stretched) to match the other\n",
    "3. If neither is 1 and they differ, it's an error\n",
    "\n",
    "If the arrays have different numbers of dimensions, the smaller array is padded with\n",
    "size-1 dimensions on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_broadcasting() -> None:\n",
    "    \"\"\"Show broadcasting rules with concrete examples and shape annotations.\"\"\"\n",
    "    print('=== Broadcasting Examples ===')\n",
    "    print()\n",
    "\n",
    "    # Example 1: Scalar + Array\n",
    "    a1 = np.array([1, 2, 3, 4])\n",
    "    b1 = 10\n",
    "    result1 = a1 + b1\n",
    "    print(f'1. Scalar + Array')\n",
    "    print(f'   {a1} + {b1} = {result1}')\n",
    "    print(f'   Shapes: {a1.shape} + () → {result1.shape}')\n",
    "    print()\n",
    "\n",
    "    # Example 2: Row vector + Column vector → Matrix\n",
    "    row = np.array([[1, 2, 3]])          # Shape: (1, 3)\n",
    "    col = np.array([[10], [20], [30]])   # Shape: (3, 1)\n",
    "    result2 = row + col\n",
    "    print(f'2. Row vector + Column vector → Matrix')\n",
    "    print(f'   row shape: {row.shape}, col shape: {col.shape}')\n",
    "    print(f'   Result shape: {result2.shape}')\n",
    "    print(f'   Result:\\n{result2}')\n",
    "    print()\n",
    "\n",
    "    # Example 3: Matrix + Row vector (common in ML: adding bias)\n",
    "    matrix = np.ones((3, 4))\n",
    "    bias = np.array([1, 2, 3, 4])       # Shape: (4,)\n",
    "    result3 = matrix + bias\n",
    "    print(f'3. Matrix + Row vector (bias addition)')\n",
    "    print(f'   matrix shape: {matrix.shape}, bias shape: {bias.shape}')\n",
    "    print(f'   Result shape: {result3.shape}')\n",
    "    print(f'   Result:\\n{result3}')\n",
    "    print()\n",
    "\n",
    "    # Example 4: 3D tensor broadcasting (batch operations)\n",
    "    batch = np.random.randn(2, 3, 4)     # (batch, rows, cols)\n",
    "    scale = np.array([1, 2, 3, 4])       # (cols,)\n",
    "    result4 = batch * scale\n",
    "    print(f'4. Batch tensor × Row vector')\n",
    "    print(f'   batch shape: {batch.shape}, scale shape: {scale.shape}')\n",
    "    print(f'   Result shape: {result4.shape}')\n",
    "    print(f'   (Each row in each batch sample is scaled element-wise)')\n",
    "\n",
    "\n",
    "demonstrate_broadcasting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting Speed: Why It Matters\n",
    "\n",
    "Broadcasting isn't just convenient — it's also **faster** than the manual alternative.\n",
    "Without broadcasting, you'd have to use `np.tile()` or `np.repeat()` to manually expand\n",
    "the smaller array, which allocates a large temporary copy. Broadcasting avoids this copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: manual expansion vs broadcasting\n",
    "np.random.seed(SEED)\n",
    "n_rows, n_cols = 10_000, 1_000\n",
    "matrix = np.random.randn(n_rows, n_cols)\n",
    "bias = np.random.randn(n_cols)\n",
    "\n",
    "def add_with_tile() -> np.ndarray:\n",
    "    \"\"\"Add bias to matrix by tiling the bias vector.\"\"\"\n",
    "    bias_tiled = np.tile(bias, (n_rows, 1))  # Allocates a full copy\n",
    "    return matrix + bias_tiled\n",
    "\n",
    "def add_with_broadcast() -> np.ndarray:\n",
    "    \"\"\"Add bias to matrix using broadcasting (no copy).\"\"\"\n",
    "    return matrix + bias\n",
    "\n",
    "t_tile, _ = measure_time(add_with_tile)\n",
    "t_bcast, _ = measure_time(add_with_broadcast)\n",
    "\n",
    "# Verify same result\n",
    "assert np.allclose(add_with_tile(), add_with_broadcast())\n",
    "\n",
    "print(f'Matrix shape: {matrix.shape}, Bias shape: {bias.shape}')\n",
    "print(f'np.tile + add:  {t_tile*1000:.2f} ms')\n",
    "print(f'Broadcasting:   {t_bcast*1000:.2f} ms')\n",
    "print(f'Speedup: {t_tile/t_bcast:.1f}×')\n",
    "print(f'Memory saved: {n_rows * n_cols * 8 / 1024 / 1024:.1f} MB '\n",
    "      f'(avoided tiling a {n_rows}×{n_cols} temporary array)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Memory Layout: Row-Major vs Column-Major\n",
    "\n",
    "Computer memory is a flat 1D sequence of bytes. A 2D array must be **linearized** into\n",
    "this flat memory. There are two conventions:\n",
    "\n",
    "- **Row-major (C order):** Rows are stored contiguously. Element `A[i][j+1]` is next to\n",
    "  `A[i][j]` in memory. This is NumPy's default.\n",
    "- **Column-major (Fortran order):** Columns are stored contiguously. Element `A[i+1][j]`\n",
    "  is next to `A[i][j]` in memory.\n",
    "\n",
    "When you access elements sequentially in memory, CPU caches work efficiently (each cache\n",
    "line loads 64 bytes ≈ 8 float64 values). When you access elements with large strides\n",
    "(jumping across rows/columns), cache misses slow things down dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_memory_layout() -> pd.DataFrame:\n",
    "    \"\"\"Benchmark row-wise vs column-wise operations on C and F order arrays.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing results.\n",
    "    \"\"\"\n",
    "    n = 5_000\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    c_array = np.ascontiguousarray(np.random.randn(n, n))  # Row-major\n",
    "    f_array = np.asfortranarray(c_array)                     # Column-major\n",
    "\n",
    "    # Verify same data\n",
    "    assert np.allclose(c_array, f_array)\n",
    "\n",
    "    records: list[dict] = []\n",
    "\n",
    "    # Row-wise sum on each layout\n",
    "    t_c_row, _ = measure_time(lambda: c_array.sum(axis=1))\n",
    "    t_f_row, _ = measure_time(lambda: f_array.sum(axis=1))\n",
    "    records.append({\n",
    "        'Operation': 'Row sum (axis=1)',\n",
    "        'C-order (ms)': t_c_row * 1000,\n",
    "        'F-order (ms)': t_f_row * 1000,\n",
    "        'Winner': 'C-order' if t_c_row < t_f_row else 'F-order',\n",
    "    })\n",
    "\n",
    "    # Column-wise sum on each layout\n",
    "    t_c_col, _ = measure_time(lambda: c_array.sum(axis=0))\n",
    "    t_f_col, _ = measure_time(lambda: f_array.sum(axis=0))\n",
    "    records.append({\n",
    "        'Operation': 'Column sum (axis=0)',\n",
    "        'C-order (ms)': t_c_col * 1000,\n",
    "        'F-order (ms)': t_f_col * 1000,\n",
    "        'Winner': 'C-order' if t_c_col < t_f_col else 'F-order',\n",
    "    })\n",
    "\n",
    "    # Row-wise mean\n",
    "    t_c_rmean, _ = measure_time(lambda: c_array.mean(axis=1))\n",
    "    t_f_rmean, _ = measure_time(lambda: f_array.mean(axis=1))\n",
    "    records.append({\n",
    "        'Operation': 'Row mean (axis=1)',\n",
    "        'C-order (ms)': t_c_rmean * 1000,\n",
    "        'F-order (ms)': t_f_rmean * 1000,\n",
    "        'Winner': 'C-order' if t_c_rmean < t_f_rmean else 'F-order',\n",
    "    })\n",
    "\n",
    "    # Column-wise mean\n",
    "    t_c_cmean, _ = measure_time(lambda: c_array.mean(axis=0))\n",
    "    t_f_cmean, _ = measure_time(lambda: f_array.mean(axis=0))\n",
    "    records.append({\n",
    "        'Operation': 'Column mean (axis=0)',\n",
    "        'C-order (ms)': t_c_cmean * 1000,\n",
    "        'F-order (ms)': t_f_cmean * 1000,\n",
    "        'Winner': 'C-order' if t_c_cmean < t_f_cmean else 'F-order',\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "layout_df = benchmark_memory_layout()\n",
    "print(f'Memory Layout Benchmark (5000×5000 matrix):')\n",
    "print(layout_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** C-order (row-major) is faster for row operations, F-order (column-major)\n",
    "is faster for column operations. Since ML data is typically stored as *samples × features*\n",
    "(rows = samples), NumPy's default C-order is the right choice.\n",
    "\n",
    "When you see unexpectedly slow operations, check whether you're accessing data along the\n",
    "'wrong' axis relative to the memory layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Views vs Copies: Understanding Memory Ownership\n",
    "\n",
    "NumPy operations can return either a **view** (shared memory with the original) or a\n",
    "**copy** (independent memory). Understanding this distinction is critical for both\n",
    "correctness and performance:\n",
    "\n",
    "- **Views** are fast (no data copying) but mutations affect the original\n",
    "- **Copies** are safe (independent data) but use extra memory and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_views_and_copies() -> None:\n",
    "    \"\"\"Show which operations create views vs copies and their implications.\"\"\"\n",
    "    original = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "    print(f'Original: {original}')\n",
    "    print()\n",
    "\n",
    "    # Slicing creates a VIEW\n",
    "    view_slice = original[2:6]\n",
    "    print(f'Slice original[2:6]: {view_slice}')\n",
    "    print(f'  Is a view? {view_slice.base is original}')\n",
    "    print(f'  Shares memory? {np.shares_memory(original, view_slice)}')\n",
    "\n",
    "    # Modifying the view changes the original!\n",
    "    view_slice[0] = 999\n",
    "    print(f'  After view_slice[0] = 999:')\n",
    "    print(f'    view_slice: {view_slice}')\n",
    "    print(f'    original:   {original}  ← ALSO CHANGED!')\n",
    "    original[2] = 3  # Restore\n",
    "    print()\n",
    "\n",
    "    # Fancy indexing creates a COPY\n",
    "    fancy_idx = original[[0, 3, 7]]\n",
    "    print(f'Fancy index original[[0, 3, 7]]: {fancy_idx}')\n",
    "    print(f'  Shares memory? {np.shares_memory(original, fancy_idx)}')\n",
    "    fancy_idx[0] = 999\n",
    "    print(f'  After fancy_idx[0] = 999:')\n",
    "    print(f'    fancy_idx: {fancy_idx}')\n",
    "    print(f'    original:  {original}  ← NOT changed')\n",
    "    print()\n",
    "\n",
    "    # Reshape creates a VIEW (when possible)\n",
    "    matrix_view = original.reshape(2, 5)\n",
    "    print(f'Reshape to (2, 5):')\n",
    "    print(f'  Shares memory? {np.shares_memory(original, matrix_view)}')\n",
    "    print(f'  Strides: original={original.strides}, reshaped={matrix_view.strides}')\n",
    "    print()\n",
    "\n",
    "    # .copy() forces a copy\n",
    "    explicit_copy = original.copy()\n",
    "    print(f'Explicit .copy():')\n",
    "    print(f'  Shares memory? {np.shares_memory(original, explicit_copy)}')\n",
    "    print()\n",
    "\n",
    "    # Summary table\n",
    "    operations_table = pd.DataFrame({\n",
    "        'Operation': ['Slicing [a:b]', 'Reshape', 'Transpose (.T)',\n",
    "                      'Fancy indexing [list]', 'Boolean indexing [mask]',\n",
    "                      '.flatten()', '.ravel()', '.copy()'],\n",
    "        'Returns': ['View', 'View*', 'View', 'Copy', 'Copy',\n",
    "                    'Copy', 'View*', 'Copy'],\n",
    "        'Note': ['Always a view', 'View when contiguous possible',\n",
    "                 'Always a view', 'Always a copy',\n",
    "                 'Always a copy', 'Always a copy',\n",
    "                 'View when contiguous; copy otherwise',\n",
    "                 'Explicit copy'],\n",
    "    })\n",
    "    print('=== View vs Copy Summary ===')\n",
    "    print(operations_table.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_views_and_copies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding views is essential for two reasons:\n",
    "1. **Performance:** Avoid unnecessary copies when working with large datasets\n",
    "2. **Correctness:** Know when modifying a slice will affect the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — Putting It All Together: BenchmarkSuite Class\n",
    "\n",
    "We've built individual benchmarking functions. Now let's combine them into a reusable\n",
    "`BenchmarkSuite` class that can systematically compare any set of approaches across\n",
    "multiple array sizes and produce publication-quality visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkSuite:\n",
    "    \"\"\"Systematic benchmarking tool for comparing numerical computation approaches.\n",
    "\n",
    "    Supports registering multiple implementations for the same operation,\n",
    "    running them across various input sizes, and generating comparison\n",
    "    tables and plots.\n",
    "\n",
    "    Attributes:\n",
    "        results: Dictionary mapping experiment names to DataFrames of results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize with empty results.\"\"\"\n",
    "        self.results: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    def run_scaling_benchmark(\n",
    "        self,\n",
    "        name: str,\n",
    "        approaches: dict[str, callable],\n",
    "        data_factory: callable,\n",
    "        sizes: list[int],\n",
    "        num_warmup: int = NUM_WARMUP,\n",
    "        num_timed: int = NUM_TIMED,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Run a benchmark across multiple sizes for several approaches.\n",
    "\n",
    "        Args:\n",
    "            name: Name for this benchmark experiment.\n",
    "            approaches: Dictionary mapping approach names to callables.\n",
    "                Each callable takes data_factory output and returns a\n",
    "                zero-argument callable to benchmark.\n",
    "            data_factory: Callable that takes (size,) and returns data\n",
    "                to pass to each approach.\n",
    "            sizes: List of input sizes to benchmark.\n",
    "            num_warmup: Warmup runs before timing.\n",
    "            num_timed: Timed runs to average.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with timing results.\n",
    "        \"\"\"\n",
    "        records: list[dict] = []\n",
    "\n",
    "        for size in sizes:\n",
    "            data = data_factory(size)\n",
    "            row: dict[str, object] = {'Size': size}\n",
    "\n",
    "            for approach_name, make_func in approaches.items():\n",
    "                func = make_func(data)\n",
    "                mean_t, std_t = measure_time(func, num_warmup, num_timed)\n",
    "                row[f'{approach_name} (ms)'] = mean_t * 1000\n",
    "                row[f'{approach_name} (std)'] = std_t * 1000\n",
    "\n",
    "            records.append(row)\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        self.results[name] = df\n",
    "        return df\n",
    "\n",
    "    def plot_scaling(\n",
    "        self,\n",
    "        name: str,\n",
    "        approach_names: list[str] | None = None,\n",
    "        title: str | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Plot log-log scaling curves for a benchmark experiment.\n",
    "\n",
    "        Args:\n",
    "            name: Name of the benchmark to plot.\n",
    "            approach_names: Approaches to include. None plots all.\n",
    "            title: Plot title. None uses the experiment name.\n",
    "        \"\"\"\n",
    "        df = self.results[name]\n",
    "        if approach_names is None:\n",
    "            approach_names = [c.replace(' (ms)', '')\n",
    "                             for c in df.columns if c.endswith(' (ms)')]\n",
    "\n",
    "        colors = ['#1E88E5', '#E53935', '#43A047', '#FF9800', '#9C27B0', '#795548']\n",
    "        markers = ['o', 's', '^', 'D', 'v', 'p']\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        for idx, approach in enumerate(approach_names):\n",
    "            col = f'{approach} (ms)'\n",
    "            if col in df.columns:\n",
    "                ax.loglog(df['Size'], df[col],\n",
    "                         f'{markers[idx % len(markers)]}-',\n",
    "                         label=approach,\n",
    "                         color=colors[idx % len(colors)],\n",
    "                         linewidth=2, markersize=7)\n",
    "\n",
    "        ax.set_xlabel('Input Size (n)')\n",
    "        ax.set_ylabel('Time (ms, log scale)')\n",
    "        ax.set_title(title or f'Scaling: {name}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def summary_table(self, name: str) -> pd.DataFrame:\n",
    "        \"\"\"Return a summary of the benchmark with speedup ratios.\n",
    "\n",
    "        The first approach is treated as the baseline.\n",
    "\n",
    "        Args:\n",
    "            name: Name of the benchmark to summarize.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with timing and speedup columns.\n",
    "        \"\"\"\n",
    "        df = self.results[name].copy()\n",
    "        time_cols = [c for c in df.columns if c.endswith(' (ms)')]\n",
    "        baseline_col = time_cols[0]\n",
    "\n",
    "        for col in time_cols[1:]:\n",
    "            approach_name = col.replace(' (ms)', '')\n",
    "            df[f'Speedup vs {approach_name}'] = df[baseline_col] / df[col]\n",
    "\n",
    "        # Drop std columns for readability\n",
    "        drop_cols = [c for c in df.columns if c.endswith(' (std)')]\n",
    "        return df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the suite works by running the addition benchmark through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = BenchmarkSuite()\n",
    "\n",
    "# Define the data factory and approaches\n",
    "def addition_data_factory(size: int) -> dict:\n",
    "    \"\"\"Create test data for addition benchmarks.\n",
    "\n",
    "    Args:\n",
    "        size: Number of elements.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with list and array versions of data.\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    arr_a = np.random.randn(size)\n",
    "    arr_b = np.random.randn(size)\n",
    "    return {\n",
    "        'list_a': arr_a.tolist(),\n",
    "        'list_b': arr_b.tolist(),\n",
    "        'arr_a': arr_a,\n",
    "        'arr_b': arr_b,\n",
    "    }\n",
    "\n",
    "addition_approaches = {\n",
    "    'Python Loop': lambda d: lambda: python_add(d['list_a'], d['list_b']),\n",
    "    'List Comp': lambda d: lambda: python_add_comprehension(d['list_a'], d['list_b']),\n",
    "    'NumPy': lambda d: lambda: d['arr_a'] + d['arr_b'],\n",
    "}\n",
    "\n",
    "print('Running addition scaling benchmark...')\n",
    "suite.run_scaling_benchmark(\n",
    "    name='Element-wise Addition',\n",
    "    approaches=addition_approaches,\n",
    "    data_factory=addition_data_factory,\n",
    "    sizes=[100, 1_000, 10_000, 100_000, 1_000_000],\n",
    ")\n",
    "\n",
    "suite.plot_scaling('Element-wise Addition')\n",
    "print(suite.summary_table('Element-wise Addition').to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — PyTorch Tensors: The Bridge to GPU Computing\n",
    "\n",
    "So far we've compared Python loops vs NumPy. But in deep learning, we use **PyTorch\n",
    "tensors** — which are similar to NumPy arrays but can also run on GPUs.\n",
    "\n",
    "Let's bring PyTorch into the picture and measure how it compares to NumPy on CPU,\n",
    "and whether GPU acceleration (if available) provides further speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# Set PyTorch seed\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 NumPy Arrays vs PyTorch Tensors\n",
    "\n",
    "PyTorch tensors and NumPy arrays share many similarities — they're both typed,\n",
    "multi-dimensional arrays stored in contiguous memory. But there are key differences:\n",
    "\n",
    "| Feature | NumPy ndarray | PyTorch Tensor |\n",
    "|---------|--------------|----------------|\n",
    "| Backend | CPU only | CPU + GPU |\n",
    "| Autograd | No | Yes (gradient tracking) |\n",
    "| Default dtype | float64 | **float32** |\n",
    "| Memory format | Row-major | Row-major (+ channels-first for images) |\n",
    "| Ecosystem | Scientific computing | Deep learning |\n",
    "\n",
    "The default dtype difference (float64 vs float32) is important: deep learning almost\n",
    "always uses float32, which is 2× smaller and 2× faster than float64 on most hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create equivalent arrays and tensors\n",
    "np.random.seed(SEED)\n",
    "data_np = np.random.randn(1_000_000).astype(np.float32)\n",
    "data_torch = torch.from_numpy(data_np.copy())  # Copy to avoid shared memory\n",
    "\n",
    "print(f'NumPy:   dtype={data_np.dtype}, shape={data_np.shape}, '\n",
    "      f'nbytes={data_np.nbytes / 1024 / 1024:.1f} MB')\n",
    "print(f'PyTorch: dtype={data_torch.dtype}, shape={data_torch.shape}, '\n",
    "      f'nbytes={data_torch.element_size() * data_torch.nelement() / 1024 / 1024:.1f} MB')\n",
    "print()\n",
    "\n",
    "# Verify they contain the same data\n",
    "print(f'First 5 values match: {np.allclose(data_np[:5], data_torch[:5].numpy())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Zero-Copy Conversion Between NumPy and PyTorch\n",
    "\n",
    "NumPy and PyTorch can share memory through zero-copy conversions. This means\n",
    "converting between them is essentially free — no data is copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_zero_copy() -> None:\n",
    "    \"\"\"Show zero-copy conversion between NumPy and PyTorch.\"\"\"\n",
    "    # NumPy → PyTorch (shared memory)\n",
    "    np_arr = np.array([1.0, 2.0, 3.0])\n",
    "    torch_tensor = torch.from_numpy(np_arr)  # Zero copy!\n",
    "\n",
    "    print('Zero-copy NumPy → PyTorch:')\n",
    "    print(f'  np_arr:      {np_arr}')\n",
    "    print(f'  torch_tensor: {torch_tensor}')\n",
    "\n",
    "    # Modifying one affects the other (shared memory)\n",
    "    torch_tensor[0] = 999.0\n",
    "    print(f'  After torch_tensor[0] = 999:')\n",
    "    print(f'    np_arr:      {np_arr}  ← ALSO CHANGED (shared memory)')\n",
    "    np_arr[0] = 1.0  # Restore\n",
    "    print()\n",
    "\n",
    "    # PyTorch → NumPy (shared memory on CPU)\n",
    "    t = torch.tensor([10.0, 20.0, 30.0])\n",
    "    n = t.numpy()  # Zero copy on CPU\n",
    "    print('Zero-copy PyTorch → NumPy:')\n",
    "    print(f'  tensor: {t}')\n",
    "    print(f'  numpy:  {n}')\n",
    "    print(f'  Shares memory: {np.shares_memory(n, t.numpy())}')\n",
    "    print()\n",
    "\n",
    "    # Safe copy (independent memory)\n",
    "    safe_copy = torch.from_numpy(np_arr.copy())  # .copy() breaks sharing\n",
    "    safe_copy[0] = 999.0\n",
    "    print('Safe copy (no sharing):')\n",
    "    print(f'  np_arr unchanged: {np_arr}')\n",
    "\n",
    "\n",
    "demonstrate_zero_copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 CPU Speed: NumPy vs PyTorch\n",
    "\n",
    "On CPU, NumPy and PyTorch should have similar performance since both use optimized\n",
    "BLAS backends. Let's verify this across common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_numpy_vs_pytorch_cpu() -> pd.DataFrame:\n",
    "    \"\"\"Compare NumPy and PyTorch speed on CPU across common operations.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing results.\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    n = 1_000_000\n",
    "\n",
    "    # Create matching data\n",
    "    np_a = np.random.randn(n).astype(np.float32)\n",
    "    np_b = np.random.randn(n).astype(np.float32)\n",
    "    np_mat = np.random.randn(1000, 1000).astype(np.float32)\n",
    "\n",
    "    pt_a = torch.from_numpy(np_a.copy())\n",
    "    pt_b = torch.from_numpy(np_b.copy())\n",
    "    pt_mat = torch.from_numpy(np_mat.copy())\n",
    "\n",
    "    operations = [\n",
    "        ('Addition (1M)',\n",
    "         lambda: np_a + np_b,\n",
    "         lambda: pt_a + pt_b),\n",
    "        ('Multiplication (1M)',\n",
    "         lambda: np_a * np_b,\n",
    "         lambda: pt_a * pt_b),\n",
    "        ('Dot Product (1M)',\n",
    "         lambda: np.dot(np_a, np_b),\n",
    "         lambda: torch.dot(pt_a, pt_b)),\n",
    "        ('Sum (1M)',\n",
    "         lambda: np.sum(np_a),\n",
    "         lambda: torch.sum(pt_a)),\n",
    "        ('Exp (1M)',\n",
    "         lambda: np.exp(np_a),\n",
    "         lambda: torch.exp(pt_a)),\n",
    "        ('MatMul (1000×1000)',\n",
    "         lambda: np_mat @ np_mat,\n",
    "         lambda: pt_mat @ pt_mat),\n",
    "    ]\n",
    "\n",
    "    records: list[dict] = []\n",
    "    for name, np_func, pt_func in operations:\n",
    "        t_np, _ = measure_time(np_func)\n",
    "        t_pt, _ = measure_time(pt_func)\n",
    "        records.append({\n",
    "            'Operation': name,\n",
    "            'NumPy (ms)': t_np * 1000,\n",
    "            'PyTorch CPU (ms)': t_pt * 1000,\n",
    "            'Ratio (NP/PT)': t_np / t_pt,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "print('NumPy vs PyTorch CPU Benchmark (float32):')\n",
    "cpu_df = benchmark_numpy_vs_pytorch_cpu()\n",
    "print(cpu_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On CPU with the same dtype (float32), NumPy and PyTorch perform very similarly.\n",
    "The slight differences come from their different BLAS backends and internal\n",
    "optimizations, but for practical purposes they're interchangeable on CPU.\n",
    "\n",
    "The key advantage of PyTorch comes when you need:\n",
    "- **GPU acceleration** (see below if CUDA is available)\n",
    "- **Automatic differentiation** (Module 5: Backpropagation)\n",
    "- **Integration with the DL ecosystem** (data loaders, models, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 GPU Acceleration (If Available)\n",
    "\n",
    "If a CUDA GPU is available, let's measure the speedup from moving computation\n",
    "to the GPU. Note that GPU computation involves data transfer overhead, so small\n",
    "operations may actually be slower on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_gpu_if_available() -> pd.DataFrame | None:\n",
    "    \"\"\"Benchmark CPU vs GPU for various operations.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with results, or None if no GPU is available.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print('No CUDA GPU available. Skipping GPU benchmark.')\n",
    "        print('When running on a machine with a GPU (e.g., Colab with GPU runtime),')\n",
    "        print('this section will show significant speedups for large operations.')\n",
    "        return None\n",
    "\n",
    "    device_gpu = torch.device('cuda')\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    sizes = [1_000, 10_000, 100_000, 1_000_000, 10_000_000]\n",
    "    records: list[dict] = []\n",
    "\n",
    "    for n in sizes:\n",
    "        cpu_a = torch.randn(n)\n",
    "        cpu_b = torch.randn(n)\n",
    "        gpu_a = cpu_a.to(device_gpu)\n",
    "        gpu_b = cpu_b.to(device_gpu)\n",
    "\n",
    "        # CPU timing\n",
    "        t_cpu, _ = measure_time(lambda: cpu_a + cpu_b)\n",
    "\n",
    "        # GPU timing (include synchronization)\n",
    "        def gpu_add() -> None:\n",
    "            \"\"\"GPU addition with synchronization.\"\"\"\n",
    "            _ = gpu_a + gpu_b\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_gpu, _ = measure_time(gpu_add)\n",
    "\n",
    "        records.append({\n",
    "            'Size': n,\n",
    "            'CPU (ms)': t_cpu * 1000,\n",
    "            'GPU (ms)': t_gpu * 1000,\n",
    "            'Speedup': t_cpu / t_gpu,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Also benchmark matrix multiplication\n",
    "    mat_sizes = [256, 512, 1024, 2048, 4096]\n",
    "    mat_records: list[dict] = []\n",
    "    for n in mat_sizes:\n",
    "        cpu_mat = torch.randn(n, n)\n",
    "        gpu_mat = cpu_mat.to(device_gpu)\n",
    "\n",
    "        t_cpu, _ = measure_time(lambda: cpu_mat @ cpu_mat)\n",
    "\n",
    "        def gpu_matmul() -> None:\n",
    "            \"\"\"GPU matmul with synchronization.\"\"\"\n",
    "            _ = gpu_mat @ gpu_mat\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_gpu, _ = measure_time(gpu_matmul)\n",
    "        mat_records.append({\n",
    "            'Size': f'{n}×{n}',\n",
    "            'CPU (ms)': t_cpu * 1000,\n",
    "            'GPU (ms)': t_gpu * 1000,\n",
    "            'Speedup': t_cpu / t_gpu,\n",
    "        })\n",
    "\n",
    "    mat_df = pd.DataFrame(mat_records)\n",
    "\n",
    "    print('Vector Addition: CPU vs GPU')\n",
    "    print(df.to_string(index=False))\n",
    "    print()\n",
    "    print('Matrix Multiplication: CPU vs GPU')\n",
    "    print(mat_df.to_string(index=False))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "gpu_df = benchmark_gpu_if_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU performance notes:**\n",
    "- Small operations are **slower** on GPU due to kernel launch overhead (~5–10 µs per operation)\n",
    "- GPU wins for large arrays and especially matrix multiplication\n",
    "- The speedup increases with matrix size because GPUs have massive parallelism (thousands of cores)\n",
    "- Always remember to call `torch.cuda.synchronize()` when timing GPU operations — GPU\n",
    "  operations are asynchronous by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 — Evaluation & Analysis\n",
    "\n",
    "Let's run a comprehensive set of benchmarks to build a complete picture of performance\n",
    "across all the approaches and operations we've studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Comprehensive Operation Benchmark\n",
    "\n",
    "We'll benchmark a wide range of operations commonly used in ML, comparing Python\n",
    "loops, NumPy, and PyTorch on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_benchmark() -> pd.DataFrame:\n",
    "    \"\"\"Run a comprehensive benchmark of common ML operations.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing results for all approaches and operations.\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    n = 500_000\n",
    "\n",
    "    # Prepare data in all formats\n",
    "    list_a = list(np.random.randn(n))\n",
    "    list_b = list(np.random.randn(n))\n",
    "    np_a = np.array(list_a, dtype=np.float32)\n",
    "    np_b = np.array(list_b, dtype=np.float32)\n",
    "    pt_a = torch.tensor(np_a)\n",
    "    pt_b = torch.tensor(np_b)\n",
    "\n",
    "    # Matrix data\n",
    "    np_mat_a = np.random.randn(500, 500).astype(np.float32)\n",
    "    np_mat_b = np.random.randn(500, 500).astype(np.float32)\n",
    "    pt_mat_a = torch.from_numpy(np_mat_a.copy())\n",
    "    pt_mat_b = torch.from_numpy(np_mat_b.copy())\n",
    "\n",
    "    benchmarks = [\n",
    "        ('Vector Add (500K)',\n",
    "         lambda: python_add(list_a, list_b),\n",
    "         lambda: np_a + np_b,\n",
    "         lambda: pt_a + pt_b),\n",
    "        ('Vector Mul (500K)',\n",
    "         lambda: python_elementwise_mul(list_a, list_b),\n",
    "         lambda: np_a * np_b,\n",
    "         lambda: pt_a * pt_b),\n",
    "        ('Dot Product (500K)',\n",
    "         lambda: python_dot(list_a, list_b),\n",
    "         lambda: np.dot(np_a, np_b),\n",
    "         lambda: torch.dot(pt_a, pt_b)),\n",
    "        ('Sum (500K)',\n",
    "         lambda: python_sum(list_a),\n",
    "         lambda: np.sum(np_a),\n",
    "         lambda: torch.sum(pt_a)),\n",
    "        ('MatMul (500×500)',\n",
    "         lambda: python_matmul(np_mat_a[:50, :50].tolist(), np_mat_b[:50, :50].tolist()),\n",
    "         lambda: np_mat_a @ np_mat_b,\n",
    "         lambda: pt_mat_a @ pt_mat_b),\n",
    "    ]\n",
    "\n",
    "    records: list[dict] = []\n",
    "    for name, py_func, np_func, pt_func in benchmarks:\n",
    "        t_py, _ = measure_time(py_func, num_warmup=1, num_timed=3)\n",
    "        t_np, _ = measure_time(np_func)\n",
    "        t_pt, _ = measure_time(pt_func)\n",
    "        records.append({\n",
    "            'Operation': name,\n",
    "            'Python (ms)': t_py * 1000,\n",
    "            'NumPy (ms)': t_np * 1000,\n",
    "            'PyTorch CPU (ms)': t_pt * 1000,\n",
    "            'Speedup (Py→NP)': t_py / t_np,\n",
    "            'Speedup (Py→PT)': t_py / t_pt,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "print('Comprehensive ML Operations Benchmark:')\n",
    "comprehensive_df = comprehensive_benchmark()\n",
    "print(comprehensive_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualization: The Full Picture\n",
    "\n",
    "Let's create a comprehensive visualization showing speedups across all operations\n",
    "and approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Absolute times (log scale)\n",
    "x = range(len(comprehensive_df))\n",
    "width = 0.25\n",
    "axes[0].bar([i - width for i in x], comprehensive_df['Python (ms)'],\n",
    "            width, label='Python', color='#E53935')\n",
    "axes[0].bar(x, comprehensive_df['NumPy (ms)'],\n",
    "            width, label='NumPy', color='#1E88E5')\n",
    "axes[0].bar([i + width for i in x], comprehensive_df['PyTorch CPU (ms)'],\n",
    "            width, label='PyTorch CPU', color='#43A047')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comprehensive_df['Operation'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Time (ms, log scale)')\n",
    "axes[0].set_title('Execution Time by Operation')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Right: Speedup ratios\n",
    "axes[1].bar([i - width/2 for i in x], comprehensive_df['Speedup (Py→NP)'],\n",
    "            width, label='Python → NumPy', color='#1E88E5')\n",
    "axes[1].bar([i + width/2 for i in x], comprehensive_df['Speedup (Py→PT)'],\n",
    "            width, label='Python → PyTorch', color='#43A047')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comprehensive_df['Operation'], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Speedup Factor')\n",
    "axes[1].set_title('Speedup over Python Loops')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance Guidelines Summary\n",
    "\n",
    "Based on our benchmarks, here are the practical performance guidelines for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision guide\n",
    "guidelines = pd.DataFrame({\n",
    "    'Scenario': [\n",
    "        'Small data (n < 100)',\n",
    "        'Medium data (100 < n < 10K)',\n",
    "        'Large data (n > 10K)',\n",
    "        'Matrix operations',\n",
    "        'Need gradients (training)',\n",
    "        'GPU available + large data',\n",
    "        'Quick prototyping / EDA',\n",
    "        'Production ML pipeline',\n",
    "    ],\n",
    "    'Recommended': [\n",
    "        'Python lists (overhead of NumPy may not be worth it)',\n",
    "        'NumPy (clear speedup over Python)',\n",
    "        'NumPy or PyTorch (massive speedup)',\n",
    "        'NumPy/PyTorch (BLAS optimized)',\n",
    "        'PyTorch (autograd support)',\n",
    "        'PyTorch on GPU',\n",
    "        'NumPy + Pandas',\n",
    "        'PyTorch (ecosystem integration)',\n",
    "    ],\n",
    "    'Expected Speedup': [\n",
    "        '~1× (no benefit)',\n",
    "        '5–20×',\n",
    "        '50–200×',\n",
    "        '100–10,000×',\n",
    "        'N/A (feature, not speed)',\n",
    "        '10–100× over CPU',\n",
    "        'N/A (convenience)',\n",
    "        'N/A (ecosystem)',\n",
    "    ],\n",
    "})\n",
    "\n",
    "print('=== Performance Decision Guide ===')\n",
    "print(guidelines.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Common Anti-Patterns and Their Fixes\n",
    "\n",
    "Let's demonstrate the most common performance mistakes beginners make and how\n",
    "to fix them. These patterns will appear throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_antipatterns() -> None:\n",
    "    \"\"\"Show common performance anti-patterns and their vectorized fixes.\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    n = 100_000\n",
    "    data = np.random.randn(n)\n",
    "    matrix = np.random.randn(1000, 100)\n",
    "\n",
    "    print('=== Anti-Pattern 1: Growing a list in a loop ===')\n",
    "    # Bad: appending to a list\n",
    "    def bad_normalize() -> list[float]:\n",
    "        \"\"\"Normalize by appending to a list (slow).\"\"\"\n",
    "        mean = sum(data) / len(data)\n",
    "        std = (sum((x - mean) ** 2 for x in data) / len(data)) ** 0.5\n",
    "        result = []\n",
    "        for x in data:\n",
    "            result.append((x - mean) / std)\n",
    "        return result\n",
    "\n",
    "    # Good: vectorized\n",
    "    def good_normalize() -> np.ndarray:\n",
    "        \"\"\"Normalize using NumPy vectorization (fast).\"\"\"\n",
    "        return (data - data.mean()) / data.std()\n",
    "\n",
    "    t_bad, _ = measure_time(bad_normalize, num_warmup=1, num_timed=3)\n",
    "    t_good, _ = measure_time(good_normalize)\n",
    "    print(f'  Loop + append: {t_bad*1000:.2f} ms')\n",
    "    print(f'  Vectorized:    {t_good*1000:.2f} ms')\n",
    "    print(f'  Speedup:       {t_bad/t_good:.0f}×')\n",
    "    print()\n",
    "\n",
    "    print('=== Anti-Pattern 2: Iterating over rows ===')\n",
    "    # Bad: loop over rows\n",
    "    def bad_row_means() -> list[float]:\n",
    "        \"\"\"Compute row means with a loop (slow).\"\"\"\n",
    "        means = []\n",
    "        for i in range(matrix.shape[0]):\n",
    "            means.append(np.mean(matrix[i, :]))\n",
    "        return means\n",
    "\n",
    "    # Good: axis parameter\n",
    "    def good_row_means() -> np.ndarray:\n",
    "        \"\"\"Compute row means with axis parameter (fast).\"\"\"\n",
    "        return matrix.mean(axis=1)\n",
    "\n",
    "    t_bad, _ = measure_time(bad_row_means)\n",
    "    t_good, _ = measure_time(good_row_means)\n",
    "    print(f'  Loop over rows:  {t_bad*1000:.2f} ms')\n",
    "    print(f'  matrix.mean(1):  {t_good*1000:.2f} ms')\n",
    "    print(f'  Speedup:         {t_bad/t_good:.0f}×')\n",
    "    print()\n",
    "\n",
    "    print('=== Anti-Pattern 3: Conditional selection with loops ===')\n",
    "    # Bad: loop with if\n",
    "    def bad_filter() -> list[float]:\n",
    "        \"\"\"Filter positive values with a loop (slow).\"\"\"\n",
    "        result = []\n",
    "        for x in data:\n",
    "            if x > 0:\n",
    "                result.append(x)\n",
    "        return result\n",
    "\n",
    "    # Good: boolean indexing\n",
    "    def good_filter() -> np.ndarray:\n",
    "        \"\"\"Filter positive values with boolean indexing (fast).\"\"\"\n",
    "        return data[data > 0]\n",
    "\n",
    "    t_bad, _ = measure_time(bad_filter, num_warmup=1, num_timed=3)\n",
    "    t_good, _ = measure_time(good_filter)\n",
    "    print(f'  Loop + if:         {t_bad*1000:.2f} ms')\n",
    "    print(f'  Boolean indexing:  {t_good*1000:.2f} ms')\n",
    "    print(f'  Speedup:           {t_bad/t_good:.0f}×')\n",
    "    print()\n",
    "\n",
    "    print('=== Anti-Pattern 4: Pairwise distances with nested loops ===')\n",
    "    small_matrix = matrix[:100, :10]  # 100 points, 10 features\n",
    "\n",
    "    def bad_pairwise_dist() -> np.ndarray:\n",
    "        \"\"\"Compute pairwise distances with nested loops (slow).\"\"\"\n",
    "        n = small_matrix.shape[0]\n",
    "        dist = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                diff = small_matrix[i] - small_matrix[j]\n",
    "                dist[i, j] = np.sqrt(np.sum(diff ** 2))\n",
    "        return dist\n",
    "\n",
    "    def good_pairwise_dist() -> np.ndarray:\n",
    "        \"\"\"Compute pairwise distances with broadcasting (fast).\"\"\"\n",
    "        # (n, 1, d) - (1, n, d) → (n, n, d) → sum over d → sqrt\n",
    "        diff = small_matrix[:, np.newaxis, :] - small_matrix[np.newaxis, :, :]\n",
    "        return np.sqrt(np.sum(diff ** 2, axis=2))\n",
    "\n",
    "    t_bad, _ = measure_time(bad_pairwise_dist, num_warmup=1, num_timed=2)\n",
    "    t_good, _ = measure_time(good_pairwise_dist)\n",
    "\n",
    "    # Verify correctness\n",
    "    assert np.allclose(bad_pairwise_dist(), good_pairwise_dist(), atol=1e-10)\n",
    "\n",
    "    print(f'  Nested loops:   {t_bad*1000:.2f} ms')\n",
    "    print(f'  Broadcasting:   {t_good*1000:.2f} ms')\n",
    "    print(f'  Speedup:        {t_bad/t_good:.0f}×')\n",
    "\n",
    "\n",
    "demonstrate_antipatterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These anti-patterns are the most common source of slow Python code in ML.**\n",
    "Throughout this course, we'll always use the vectorized patterns on the right.\n",
    "When you see a `for` loop over array elements, ask yourself: *can this be vectorized?*\n",
    "The answer is almost always yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 dtype Impact on Performance\n",
    "\n",
    "The choice of data type (float32 vs float64) significantly affects both memory usage\n",
    "and computation speed. Deep learning almost always uses float32, while scientific\n",
    "computing traditionally uses float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_dtypes() -> pd.DataFrame:\n",
    "    \"\"\"Compare performance across different dtypes.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing and memory results.\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    n = 5_000_000\n",
    "    base = np.random.randn(n)\n",
    "\n",
    "    dtypes = [np.float16, np.float32, np.float64]\n",
    "    records: list[dict] = []\n",
    "\n",
    "    for dtype in dtypes:\n",
    "        arr = base.astype(dtype)\n",
    "        arr2 = np.random.randn(n).astype(dtype)\n",
    "\n",
    "        t_add, _ = measure_time(lambda: arr + arr2)\n",
    "        t_mul, _ = measure_time(lambda: arr * arr2)\n",
    "        t_sum, _ = measure_time(lambda: np.sum(arr))\n",
    "\n",
    "        records.append({\n",
    "            'dtype': str(dtype.__name__),\n",
    "            'Bytes/element': np.dtype(dtype).itemsize,\n",
    "            'Array MB': arr.nbytes / 1024 / 1024,\n",
    "            'Add (ms)': t_add * 1000,\n",
    "            'Multiply (ms)': t_mul * 1000,\n",
    "            'Sum (ms)': t_sum * 1000,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "dtype_df = benchmark_dtypes()\n",
    "print(f'dtype Performance Comparison (n=5,000,000):')\n",
    "print(dtype_df.to_string(index=False))\n",
    "print()\n",
    "# Memory savings summary\n",
    "f32_mb = dtype_df.loc[dtype_df['dtype'] == 'float32', 'Array MB'].values[0]\n",
    "f64_mb = dtype_df.loc[dtype_df['dtype'] == 'float64', 'Array MB'].values[0]\n",
    "print(f'Memory: float32 uses {f32_mb:.1f} MB vs float64 {f64_mb:.1f} MB ')\n",
    "print(f'  → {f64_mb / f32_mb:.0f}× memory savings with float32')\n",
    "print()\n",
    "print('float32 uses half the memory of float64 and is often faster due to')\n",
    "print('better cache utilization. This is why PyTorch defaults to float32.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5 — Summary & Lessons Learned\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Vectorization is non-negotiable.** NumPy and PyTorch are 50–500× faster than Python\n",
    "   loops for array operations. Always use vectorized operations instead of element-wise loops.\n",
    "\n",
    "2. **Broadcasting eliminates redundant copies.** Instead of manually expanding arrays with\n",
    "   `np.tile()`, let broadcasting handle shape mismatches — it's faster and uses less memory.\n",
    "\n",
    "3. **Memory layout matters.** C-order (row-major) arrays are faster for row operations.\n",
    "   NumPy defaults to C-order, which matches ML's samples-as-rows convention.\n",
    "\n",
    "4. **Views vs copies affect correctness and speed.** Slicing creates views (shared memory);\n",
    "   fancy indexing creates copies. Use `.copy()` when you need independent data.\n",
    "\n",
    "5. **float32 is the standard for ML.** It uses half the memory of float64 and is often\n",
    "   faster. PyTorch defaults to float32; NumPy defaults to float64 — be aware of this\n",
    "   when converting between them.\n",
    "\n",
    "### What's Next\n",
    "\n",
    "→ **01-02 (Advanced NumPy & PyTorch Operations)** builds on these fundamentals with\n",
    "  reshape, einsum, advanced indexing, and in-place operations — the power tools for\n",
    "  efficient tensor manipulation.\n",
    "\n",
    "### Going Further\n",
    "\n",
    "- [NumPy Internals](https://numpy.org/doc/stable/reference/internals.html) — How NumPy\n",
    "  stores and accesses array data\n",
    "- [Why Python is Slow](https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/) —\n",
    "  Deep dive into Python's object model overhead\n",
    "- [BLAS Libraries](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) —\n",
    "  The optimized math kernels behind NumPy and PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
