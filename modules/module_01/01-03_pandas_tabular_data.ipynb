{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations",
    "## 01-03: Pandas for Tabular Data",
    "",
    "**Objective:** Master the Pandas library for exploratory data analysis (EDA),",
    "data cleaning, feature engineering, and preprocessing — the essential pipeline",
    "before any ML model training.",
    "",
    "**Prerequisites:** 01-01 (Python, NumPy & Tensor Speed), 01-02 (Advanced NumPy & PyTorch Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 0 — Setup & Prerequisites",
    "",
    "In notebooks 01-01 and 01-02, we worked with NumPy arrays and PyTorch tensors —",
    "dense, homogeneous numerical data. Real-world ML data, however, is messy: it",
    "has missing values, mixed types (numbers, strings, dates), and requires careful",
    "preprocessing before it can be fed to any model.",
    "",
    "**Pandas** is the standard tool for this. It provides:",
    "- `DataFrame` — a 2D table with labeled columns of potentially different types",
    "- `Series` — a single column with an index",
    "- Rich I/O (CSV, JSON, SQL, Excel) and powerful group-by/merge/pivot operations",
    "",
    "We will cover:",
    "- DataFrame creation, indexing, and selection",
    "- Exploratory data analysis (EDA) workflow",
    "- Handling missing values (detection, imputation, deletion)",
    "- Categorical encoding (label, one-hot, ordinal, target encoding)",
    "- Feature engineering (transformations, binning, interactions)",
    "- Merging, grouping, and pivoting",
    "- Converting between Pandas, NumPy, and PyTorch",
    "",
    "We'll use the **California Housing** dataset from sklearn — a real regression",
    "dataset with 20,640 samples and 8 numerical features.",
    "",
    "**Prerequisites:** 01-01 (Python, NumPy & Tensor Speed), 01-02 (Advanced NumPy & PyTorch Operations)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "print(f'Pandas: {pd.__version__}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA: {torch.version.cuda}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "import random\n",
    "\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Display settings for Pandas\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.precision', 4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & EDA",
    "",
    "We load the California Housing dataset, which predicts median house value",
    "from 8 features: median income, house age, average rooms, etc. Each row",
    "represents a census block group (~600-3,000 people)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame  # Full DataFrame including target\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "print(f'Target: MedHouseVal (median house value in $100K)')\n",
    "print()\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Quick EDA: types, missing values, basic statistics\n",
    "print('=== Data Types ===')\n",
    "print(df.dtypes)\n",
    "print()\n",
    "\n",
    "print('=== Missing Values ===')\n",
    "print(df.isnull().sum())\n",
    "print()\n",
    "\n",
    "print('=== Descriptive Statistics ===')\n",
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 1 — Pandas Fundamentals from Scratch",
    "",
    "Before using Pandas' high-level functions, let's understand the fundamental",
    "data structures and operations by building them up step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 DataFrame Creation: Multiple Ways to Build Tables",
    "",
    "A DataFrame can be created from dictionaries, lists, NumPy arrays, or loaded",
    "from files. Understanding these constructors is essential because data arrives",
    "in many formats."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_dataframe_creation() -> None:\n",
    "    \"\"\"Show multiple ways to create DataFrames.\"\"\"\n",
    "    # Method 1: From a dictionary (most common)\n",
    "    dict_df = pd.DataFrame({\n",
    "        'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "        'age': [25, 30, 35, 28],\n",
    "        'score': [92.5, 88.0, 95.3, 91.7],\n",
    "        'passed': [True, True, True, True],\n",
    "    })\n",
    "    print('From dictionary:')\n",
    "    print(dict_df)\n",
    "    print(f'  dtypes: {dict(dict_df.dtypes)}')\n",
    "    print()\n",
    "\n",
    "    # Method 2: From a NumPy array\n",
    "    np_data = np.random.randn(5, 3)\n",
    "    np_df = pd.DataFrame(np_data, columns=['feat_a', 'feat_b', 'feat_c'])\n",
    "    print('From NumPy array:')\n",
    "    print(np_df)\n",
    "    print()\n",
    "\n",
    "    # Method 3: From a list of dictionaries (common with JSON/API data)\n",
    "    records = [\n",
    "        {'model': 'LinearReg', 'rmse': 0.85, 'r2': 0.72},\n",
    "        {'model': 'RandomForest', 'rmse': 0.62, 'r2': 0.87},\n",
    "        {'model': 'XGBoost', 'rmse': 0.58, 'r2': 0.89},\n",
    "    ]\n",
    "    records_df = pd.DataFrame(records)\n",
    "    print('From list of dicts:')\n",
    "    print(records_df)\n",
    "    print()\n",
    "\n",
    "    # Method 4: From a list of lists\n",
    "    list_df = pd.DataFrame(\n",
    "        [[1, 'A', 10.0], [2, 'B', 20.0], [3, 'C', 30.0]],\n",
    "        columns=['id', 'category', 'value'],\n",
    "    )\n",
    "    print('From list of lists:')\n",
    "    print(list_df)\n",
    "\n",
    "\n",
    "demonstrate_dataframe_creation()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Indexing & Selection: loc, iloc, and Boolean Indexing",
    "",
    "Pandas provides three main ways to select data:",
    "",
    "- **`df['col']`** or **`df.col`** — select a single column (returns Series)",
    "- **`df.loc[rows, cols]`** — label-based selection (by index/column names)",
    "- **`df.iloc[rows, cols]`** — position-based selection (by integer indices)",
    "- **`df[condition]`** — boolean indexing (filter rows)",
    "",
    "Understanding when to use each is critical for clean, bug-free data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_indexing(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Show all major indexing patterns on the housing data.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame to demonstrate indexing on.\n",
    "    \"\"\"\n",
    "    print('=== Column Selection ===')\n",
    "    # Single column → Series\n",
    "    income = data['MedInc']\n",
    "    print(f'data[\"MedInc\"]: type={type(income).__name__}, shape={income.shape}')\n",
    "    print()\n",
    "\n",
    "    # Multiple columns → DataFrame\n",
    "    subset = data[['MedInc', 'HouseAge', 'MedHouseVal']]\n",
    "    print(f'data[[\"MedInc\", \"HouseAge\", \"MedHouseVal\"]]: shape={subset.shape}')\n",
    "    print()\n",
    "\n",
    "    print('=== loc (label-based) ===')\n",
    "    # First 5 rows, specific columns\n",
    "    loc_result = data.loc[:4, ['MedInc', 'HouseAge']]\n",
    "    print(f'loc[:4, [\"MedInc\", \"HouseAge\"]]: shape={loc_result.shape}')\n",
    "    print(loc_result)\n",
    "    print()\n",
    "\n",
    "    print('=== iloc (position-based) ===')\n",
    "    # First 3 rows, first 2 columns\n",
    "    iloc_result = data.iloc[:3, :2]\n",
    "    print(f'iloc[:3, :2]: shape={iloc_result.shape}')\n",
    "    print(iloc_result)\n",
    "    print()\n",
    "\n",
    "    print('=== Boolean Indexing (filtering) ===')\n",
    "    # High income areas\n",
    "    high_income = data[data['MedInc'] > 8.0]\n",
    "    print(f'MedInc > 8.0: {len(high_income)} rows ({len(high_income)/len(data):.1%} of data)')\n",
    "\n",
    "    # Combined conditions\n",
    "    expensive_old = data[(data['MedHouseVal'] > 4.0) & (data['HouseAge'] > 40)]\n",
    "    print(f'MedHouseVal > 4.0 AND HouseAge > 40: {len(expensive_old)} rows')\n",
    "\n",
    "    # Using .query() for cleaner syntax\n",
    "    query_result = data.query('MedInc > 5 and HouseAge < 20')\n",
    "    print(f'query(\"MedInc > 5 and HouseAge < 20\"): {len(query_result)} rows')\n",
    "    print()\n",
    "\n",
    "    print('=== Common Selection Patterns ===')\n",
    "    patterns = pd.DataFrame({\n",
    "        'Pattern': [\n",
    "            'df[\"col\"]', 'df[[\"a\",\"b\"]]', 'df.loc[row, col]',\n",
    "            'df.iloc[i, j]', 'df[mask]', 'df.query(\"expr\")',\n",
    "        ],\n",
    "        'Returns': [\n",
    "            'Series', 'DataFrame', 'Scalar/Series/DF',\n",
    "            'Scalar/Series/DF', 'DataFrame', 'DataFrame',\n",
    "        ],\n",
    "        'Use When': [\n",
    "            'Need one column', 'Need multiple columns', 'Know label names',\n",
    "            'Know positions', 'Filtering rows', 'Complex conditions',\n",
    "        ],\n",
    "    })\n",
    "    print(patterns.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_indexing(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Types and Conversions",
    "",
    "Pandas columns have specific data types (dtypes). Getting them right is crucial:",
    "wrong types waste memory, cause silent errors, and prevent proper operations.",
    "",
    "Common dtype issues:",
    "- Numbers stored as strings (after CSV loading)",
    "- Categorical data stored as generic `object` type",
    "- Dates stored as strings instead of `datetime64`",
    "- Using float64 when float32 suffices (2× memory waste)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_dtypes() -> None:\n",
    "    \"\"\"Show dtype inspection, conversion, and memory impact.\"\"\"\n",
    "    # Create a messy DataFrame (simulating real-world data)\n",
    "    messy = pd.DataFrame({\n",
    "        'price': ['100.5', '200.0', '150.3', 'N/A', '300.0'],\n",
    "        'category': ['A', 'B', 'A', 'C', 'B'],\n",
    "        'date': ['2023-01-15', '2023-02-20', '2023-03-10', '2023-04-05', '2023-05-12'],\n",
    "        'quantity': [10, 20, 15, 5, 25],\n",
    "    })\n",
    "    print('=== Before Type Conversion ===')\n",
    "    print(messy.dtypes)\n",
    "    print(f'Memory: {messy.memory_usage(deep=True).sum() / 1024:.2f} KB')\n",
    "    print()\n",
    "\n",
    "    # Fix types\n",
    "    messy_fixed = messy.copy()\n",
    "    messy_fixed['price'] = pd.to_numeric(messy_fixed['price'], errors='coerce')\n",
    "    messy_fixed['category'] = messy_fixed['category'].astype('category')\n",
    "    messy_fixed['date'] = pd.to_datetime(messy_fixed['date'])\n",
    "    messy_fixed['quantity'] = messy_fixed['quantity'].astype(np.int16)\n",
    "\n",
    "    print('=== After Type Conversion ===')\n",
    "    print(messy_fixed.dtypes)\n",
    "    print(f'Memory: {messy_fixed.memory_usage(deep=True).sum() / 1024:.2f} KB')\n",
    "    print()\n",
    "    print(messy_fixed)\n",
    "    print()\n",
    "\n",
    "    # Memory optimization on the housing dataset\n",
    "    housing_f64 = df.copy()\n",
    "    housing_f32 = df.astype(np.float32)\n",
    "    mem_64 = housing_f64.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    mem_32 = housing_f32.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f'Housing dataset memory:')\n",
    "    print(f'  float64: {mem_64:.2f} MB')\n",
    "    print(f'  float32: {mem_32:.2f} MB')\n",
    "    print(f'  Savings: {(1 - mem_32/mem_64)*100:.0f}%')\n",
    "\n",
    "\n",
    "demonstrate_dtypes()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Missing Values: Detection, Analysis, and Treatment",
    "",
    "Real-world datasets almost always have missing values. The California Housing",
    "dataset is clean, so we'll artificially introduce missing values to demonstrate",
    "the full workflow.",
    "",
    "**Three strategies for handling missing data:**",
    "1. **Deletion** — drop rows or columns with too many missing values",
    "2. **Imputation** — fill with mean, median, mode, or more sophisticated methods",
    "3. **Indicator variables** — add a binary column marking where data was missing",
    "",
    "The right strategy depends on *why* the data is missing (random vs systematic)",
    "and how much is missing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_dataset_with_missing(data: pd.DataFrame, frac: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"Introduce random missing values into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data: Original clean DataFrame.\n",
    "        frac: Fraction of values to make missing.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with NaN values inserted randomly.\n",
    "    \"\"\"\n",
    "    df_dirty = data.copy()\n",
    "    np.random.seed(SEED)\n",
    "    mask = np.random.random(df_dirty.shape) < frac\n",
    "    df_dirty = df_dirty.mask(mask)\n",
    "    return df_dirty\n",
    "\n",
    "\n",
    "df_dirty = create_dataset_with_missing(df, frac=0.1)\n",
    "print(f'Original shape: {df.shape}')\n",
    "print(f'Missing values introduced: {df_dirty.isnull().sum().sum()} '\n",
    "      f'({df_dirty.isnull().sum().sum() / df_dirty.size:.1%} of all values)')\n",
    "print()\n",
    "print('Missing values per column:')\n",
    "print(df_dirty.isnull().sum().to_frame('missing').T)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_missing_values(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a comprehensive missing value report.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with missing value statistics per column.\n",
    "    \"\"\"\n",
    "    total = len(data)\n",
    "    missing = data.isnull().sum()\n",
    "    pct = missing / total * 100\n",
    "\n",
    "    report = pd.DataFrame({\n",
    "        'Column': data.columns,\n",
    "        'Missing': missing.values,\n",
    "        'Pct Missing': pct.values,\n",
    "        'dtype': data.dtypes.values,\n",
    "        'Non-Null Mean': [data[c].mean() if data[c].dtype != object else None\n",
    "                          for c in data.columns],\n",
    "    })\n",
    "    return report.sort_values('Pct Missing', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "missing_report = analyze_missing_values(df_dirty)\n",
    "print('=== Missing Value Report ===')\n",
    "print(missing_report.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the pattern of missing values. Understanding *where* data",
    "is missing helps decide whether deletion or imputation is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_missing(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Visualize missing value patterns.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame to visualize.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Left: Missing values bar chart\n",
    "    missing_pct = data.isnull().mean() * 100\n",
    "    colors = ['#E53935' if p > 15 else '#FF9800' if p > 5 else '#43A047'\n",
    "              for p in missing_pct]\n",
    "    axes[0].barh(missing_pct.index, missing_pct.values, color=colors)\n",
    "    axes[0].set_xlabel('% Missing')\n",
    "    axes[0].set_title('Missing Values by Column')\n",
    "    axes[0].axvline(x=5, color='gray', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "    axes[0].axvline(x=15, color='gray', linestyle=':', alpha=0.5, label='15% threshold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "    # Right: Missing value heatmap (sample)\n",
    "    sample = data.iloc[:100].isnull().astype(int)\n",
    "    axes[1].imshow(sample.T, aspect='auto', cmap='Reds', interpolation='none')\n",
    "    axes[1].set_yticks(range(len(data.columns)))\n",
    "    axes[1].set_yticklabels(data.columns)\n",
    "    axes[1].set_xlabel('Row Index (first 100)')\n",
    "    axes[1].set_title('Missing Pattern (red = missing)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_missing(df_dirty)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the missing value pattern understood, let's implement the three handling",
    "strategies and compare their effects on the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def handle_missing_values(data: pd.DataFrame) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Apply different missing value strategies and compare.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with missing values.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping strategy name to cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    results: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    # Strategy 1: Drop rows with any missing value\n",
    "    drop_any = data.dropna()\n",
    "    results['drop_any'] = drop_any\n",
    "    print(f'Strategy 1 — Drop rows (any NaN):')\n",
    "    print(f'  Rows: {len(data)} → {len(drop_any)} ({len(data)-len(drop_any)} dropped, '\n",
    "          f'{(len(data)-len(drop_any))/len(data):.1%} loss)')\n",
    "    print()\n",
    "\n",
    "    # Strategy 2: Drop rows with threshold (keep if ≥70% non-null)\n",
    "    thresh = int(data.shape[1] * 0.7)\n",
    "    drop_thresh = data.dropna(thresh=thresh)\n",
    "    results['drop_thresh'] = drop_thresh\n",
    "    print(f'Strategy 2 — Drop rows (≥30% missing):')\n",
    "    print(f'  Rows: {len(data)} → {len(drop_thresh)} ({len(data)-len(drop_thresh)} dropped)')\n",
    "    print()\n",
    "\n",
    "    # Strategy 3: Mean imputation\n",
    "    mean_imputed = data.copy()\n",
    "    for col in mean_imputed.select_dtypes(include=[np.number]).columns:\n",
    "        mean_imputed[col] = mean_imputed[col].fillna(mean_imputed[col].mean())\n",
    "    results['mean_impute'] = mean_imputed\n",
    "    print(f'Strategy 3 — Mean imputation:')\n",
    "    print(f'  Rows: {len(mean_imputed)} (all preserved)')\n",
    "    print(f'  Remaining NaN: {mean_imputed.isnull().sum().sum()}')\n",
    "    print()\n",
    "\n",
    "    # Strategy 4: Median imputation\n",
    "    median_imputed = data.copy()\n",
    "    for col in median_imputed.select_dtypes(include=[np.number]).columns:\n",
    "        median_imputed[col] = median_imputed[col].fillna(median_imputed[col].median())\n",
    "    results['median_impute'] = median_imputed\n",
    "    print(f'Strategy 4 — Median imputation:')\n",
    "    print(f'  Rows: {len(median_imputed)} (all preserved)')\n",
    "    print(f'  Remaining NaN: {median_imputed.isnull().sum().sum()}')\n",
    "    print()\n",
    "\n",
    "    # Strategy 5: Forward fill (useful for time series)\n",
    "    ffill = data.ffill().bfill()  # Forward fill then backward fill edges\n",
    "    results['ffill'] = ffill\n",
    "    print(f'Strategy 5 — Forward/backward fill:')\n",
    "    print(f'  Rows: {len(ffill)} (all preserved)')\n",
    "    print(f'  Remaining NaN: {ffill.isnull().sum().sum()}')\n",
    "    print()\n",
    "\n",
    "    # Strategy 6: Imputation with missing indicator\n",
    "    indicator = data.copy()\n",
    "    for col in indicator.select_dtypes(include=[np.number]).columns:\n",
    "        indicator[f'{col}_missing'] = indicator[col].isnull().astype(int)\n",
    "        indicator[col] = indicator[col].fillna(indicator[col].median())\n",
    "    results['indicator'] = indicator\n",
    "    print(f'Strategy 6 — Median + missing indicator columns:')\n",
    "    print(f'  Shape: {data.shape} → {indicator.shape} (added {indicator.shape[1] - data.shape[1]} indicator cols)')\n",
    "    print(f'  Remaining NaN: {indicator.isnull().sum().sum()}')\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "strategies = handle_missing_values(df_dirty)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare how each strategy affects the distribution of a feature. Mean",
    "imputation changes the variance (concentrates values at the mean), while",
    "deletion can introduce selection bias."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compare_imputation_effects(\n",
    "    original: pd.DataFrame,\n",
    "    strategies: dict[str, pd.DataFrame],\n",
    "    column: str,\n",
    ") -> None:\n",
    "    \"\"\"Visualize how different imputation strategies affect a column's distribution.\n",
    "\n",
    "    Args:\n",
    "        original: Clean original DataFrame.\n",
    "        strategies: Dictionary of strategy name to cleaned DataFrame.\n",
    "        column: Column name to compare.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Plot original\n",
    "    axes[0].hist(original[column], bins=50, color='#1E88E5', alpha=0.7, density=True)\n",
    "    axes[0].set_title(f'Original (n={len(original)})')\n",
    "    axes[0].set_xlabel(column)\n",
    "    axes[0].axvline(original[column].mean(), color='red', linestyle='--', label='mean')\n",
    "    axes[0].legend()\n",
    "\n",
    "    strategy_names = ['drop_any', 'drop_thresh', 'mean_impute',\n",
    "                      'median_impute', 'ffill']\n",
    "    colors = ['#E53935', '#FF9800', '#43A047', '#9C27B0', '#795548']\n",
    "\n",
    "    for idx, (name, color) in enumerate(zip(strategy_names, colors)):\n",
    "        ax = axes[idx + 1]\n",
    "        strat_df = strategies[name]\n",
    "        ax.hist(strat_df[column].dropna(), bins=50, color=color, alpha=0.7, density=True)\n",
    "        ax.set_title(f'{name} (n={len(strat_df)})')\n",
    "        ax.set_xlabel(column)\n",
    "        ax.axvline(strat_df[column].mean(), color='red', linestyle='--', label='mean')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.suptitle(f'Imputation Strategy Comparison: {column}', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Quantitative comparison\n",
    "    comp_records: list[dict] = []\n",
    "    for name in ['original'] + strategy_names:\n",
    "        data_col = original[column] if name == 'original' else strategies[name][column]\n",
    "        comp_records.append({\n",
    "            'Strategy': name,\n",
    "            'Mean': data_col.mean(),\n",
    "            'Std': data_col.std(),\n",
    "            'Median': data_col.median(),\n",
    "            'Count': len(data_col.dropna()),\n",
    "        })\n",
    "    comp_df = pd.DataFrame(comp_records)\n",
    "    print(f'=== Distribution Comparison: {column} ===')\n",
    "    print(comp_df.to_string(index=False))\n",
    "\n",
    "\n",
    "compare_imputation_effects(df, strategies, 'MedInc')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Categorical Encoding: Converting Categories to Numbers",
    "",
    "ML models need numerical input. When data has categorical features (like",
    "city names, product types, or education levels), we must encode them as numbers.",
    "",
    "**Encoding strategies:**",
    "- **Label encoding** — assign integer 0, 1, 2, ... (for ordinal data)",
    "- **One-hot encoding** — binary column per category (for nominal data)",
    "- **Ordinal encoding** — map to integers respecting order (low/medium/high)",
    "- **Target encoding** — replace category with mean target value (powerful but leaky)",
    "- **Frequency encoding** — replace category with its frequency",
    "",
    "The California Housing dataset is purely numerical, so we'll create a synthetic",
    "categorical version to demonstrate encoding."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_categorical_dataset(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add synthetic categorical features to demonstrate encoding.\n",
    "\n",
    "    Args:\n",
    "        data: Original numerical DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with additional categorical columns.\n",
    "    \"\"\"\n",
    "    df_cat = data.copy()\n",
    "\n",
    "    # Create a categorical 'income_bracket' from MedInc\n",
    "    df_cat['income_bracket'] = pd.cut(\n",
    "        df_cat['MedInc'],\n",
    "        bins=[0, 2, 4, 6, 8, np.inf],\n",
    "        labels=['very_low', 'low', 'medium', 'high', 'very_high'],\n",
    "    )\n",
    "\n",
    "    # Create 'house_age_category'\n",
    "    df_cat['age_category'] = pd.cut(\n",
    "        df_cat['HouseAge'],\n",
    "        bins=[0, 15, 30, 45, np.inf],\n",
    "        labels=['new', 'moderate', 'old', 'very_old'],\n",
    "    )\n",
    "\n",
    "    # Create a 'region' based on latitude/longitude\n",
    "    np.random.seed(SEED)\n",
    "    regions = ['Bay Area', 'LA Metro', 'San Diego', 'Sacramento', 'Central Valley']\n",
    "    df_cat['region'] = np.random.choice(regions, size=len(df_cat))\n",
    "\n",
    "    print(f'Added categorical columns:')\n",
    "    for col in ['income_bracket', 'age_category', 'region']:\n",
    "        print(f'  {col}: {df_cat[col].nunique()} categories — {df_cat[col].value_counts().head(3).to_dict()}')\n",
    "\n",
    "    return df_cat\n",
    "\n",
    "\n",
    "df_cat = create_categorical_dataset(df)\n",
    "print()\n",
    "df_cat[['MedInc', 'income_bracket', 'HouseAge', 'age_category', 'region', 'MedHouseVal']].head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_encoding_methods(data: pd.DataFrame) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Apply and compare different categorical encoding methods.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with categorical columns.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping encoding name to encoded DataFrame.\n",
    "    \"\"\"\n",
    "    results: dict[str, pd.DataFrame] = {}\n",
    "    cat_cols = ['income_bracket', 'age_category', 'region']\n",
    "\n",
    "    # 1. Label Encoding\n",
    "    label_encoded = data.copy()\n",
    "    label_maps: dict[str, dict] = {}\n",
    "    for col in cat_cols:\n",
    "        categories = label_encoded[col].astype(str).unique()\n",
    "        mapping = {cat: idx for idx, cat in enumerate(sorted(categories))}\n",
    "        label_maps[col] = mapping\n",
    "        label_encoded[f'{col}_label'] = label_encoded[col].astype(str).map(mapping)\n",
    "    results['label'] = label_encoded\n",
    "    print('1. Label Encoding:')\n",
    "    for col, mapping in label_maps.items():\n",
    "        print(f'   {col}: {mapping}')\n",
    "    print()\n",
    "\n",
    "    # 2. One-Hot Encoding\n",
    "    onehot_encoded = pd.get_dummies(data, columns=cat_cols, prefix=cat_cols, dtype=int)\n",
    "    results['onehot'] = onehot_encoded\n",
    "    new_cols = [c for c in onehot_encoded.columns if c not in data.columns]\n",
    "    print(f'2. One-Hot Encoding:')\n",
    "    print(f'   Original columns: {len(data.columns)}')\n",
    "    print(f'   After one-hot: {len(onehot_encoded.columns)} (+{len(new_cols)} dummy columns)')\n",
    "    print(f'   New columns: {new_cols[:6]}...')\n",
    "    print()\n",
    "\n",
    "    # 3. Ordinal Encoding (for ordered categories)\n",
    "    ordinal_encoded = data.copy()\n",
    "    ordinal_maps = {\n",
    "        'income_bracket': {'very_low': 0, 'low': 1, 'medium': 2, 'high': 3, 'very_high': 4},\n",
    "        'age_category': {'new': 0, 'moderate': 1, 'old': 2, 'very_old': 3},\n",
    "    }\n",
    "    for col, mapping in ordinal_maps.items():\n",
    "        ordinal_encoded[f'{col}_ordinal'] = ordinal_encoded[col].astype(str).map(mapping)\n",
    "    results['ordinal'] = ordinal_encoded\n",
    "    print(f'3. Ordinal Encoding (preserves order):')\n",
    "    for col, mapping in ordinal_maps.items():\n",
    "        print(f'   {col}: {mapping}')\n",
    "    print()\n",
    "\n",
    "    # 4. Target Encoding (mean of target per category)\n",
    "    target_encoded = data.copy()\n",
    "    target_col = 'MedHouseVal'\n",
    "    print(f'4. Target Encoding (mean {target_col} per category):')\n",
    "    for col in cat_cols:\n",
    "        means = data.groupby(col)[target_col].mean()\n",
    "        target_encoded[f'{col}_target'] = target_encoded[col].astype(str).map(means)\n",
    "        print(f'   {col}: {means.to_dict()}')\n",
    "    results['target'] = target_encoded\n",
    "    print()\n",
    "\n",
    "    # 5. Frequency Encoding\n",
    "    freq_encoded = data.copy()\n",
    "    print(f'5. Frequency Encoding:')\n",
    "    for col in cat_cols:\n",
    "        freq = data[col].value_counts(normalize=True)\n",
    "        freq_encoded[f'{col}_freq'] = freq_encoded[col].astype(str).map(freq)\n",
    "        print(f'   {col}: {freq.head(3).to_dict()}')\n",
    "    results['frequency'] = freq_encoded\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "encodings = demonstrate_encoding_methods(df_cat)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the shape and memory impact of each encoding method. One-hot",
    "encoding creates the most columns, which can be problematic with high-cardinality",
    "features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compare_encodings(encodings: dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Compare encoding methods by shape and memory.\n",
    "\n",
    "    Args:\n",
    "        encodings: Dictionary of encoding name to encoded DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Comparison DataFrame.\n",
    "    \"\"\"\n",
    "    records: list[dict] = []\n",
    "    for name, encoded_df in encodings.items():\n",
    "        records.append({\n",
    "            'Encoding': name,\n",
    "            'Columns': len(encoded_df.columns),\n",
    "            'Memory (KB)': encoded_df.memory_usage(deep=True).sum() / 1024,\n",
    "            'New Columns': len(encoded_df.columns) - len(df_cat.columns),\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "enc_comparison = compare_encodings(encodings)\n",
    "print('=== Encoding Method Comparison ===')\n",
    "print(enc_comparison.to_string(index=False))\n",
    "print()\n",
    "print('Trade-offs:')\n",
    "print('  Label: Compact but implies false ordering for nominal data')\n",
    "print('  One-hot: No false ordering but high dimensionality')\n",
    "print('  Ordinal: Perfect for truly ordered categories')\n",
    "print('  Target: Powerful but risk of target leakage (use with CV folds)')\n",
    "print('  Frequency: Captures rarity without leaking target info')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Feature Engineering: Creating Informative Features",
    "",
    "Feature engineering transforms raw columns into features that better capture",
    "the patterns in the data. Common techniques include:",
    "",
    "- **Mathematical transforms** — log, square root, reciprocal",
    "- **Interaction features** — products or ratios of existing features",
    "- **Binning** — converting continuous to categorical",
    "- **Aggregation** — group-level statistics (mean, count per category)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def engineer_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create engineered features from the housing dataset.\n",
    "\n",
    "    Args:\n",
    "        data: Original housing DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with additional engineered features.\n",
    "    \"\"\"\n",
    "    df_eng = data.copy()\n",
    "\n",
    "    # Log transforms (reduce right skew)\n",
    "    df_eng['log_MedInc'] = np.log1p(df_eng['MedInc'])\n",
    "    df_eng['log_Population'] = np.log1p(df_eng['Population'])\n",
    "\n",
    "    # Interaction features\n",
    "    df_eng['rooms_per_household'] = df_eng['AveRooms'] / df_eng['AveOccup'].clip(lower=0.1)\n",
    "    df_eng['bedrooms_ratio'] = df_eng['AveBedrms'] / df_eng['AveRooms'].clip(lower=0.1)\n",
    "    df_eng['income_per_room'] = df_eng['MedInc'] / df_eng['AveRooms'].clip(lower=0.1)\n",
    "\n",
    "    # Binning\n",
    "    df_eng['income_bin'] = pd.qcut(df_eng['MedInc'], q=5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "    # Polynomial features\n",
    "    df_eng['MedInc_squared'] = df_eng['MedInc'] ** 2\n",
    "\n",
    "    # Distance from major city (rough approximation using lat/lon)\n",
    "    la_lat, la_lon = 34.05, -118.24\n",
    "    sf_lat, sf_lon = 37.77, -122.42\n",
    "    df_eng['dist_to_LA'] = np.sqrt(\n",
    "        (df_eng['Latitude'] - la_lat)**2 + (df_eng['Longitude'] - la_lon)**2)\n",
    "    df_eng['dist_to_SF'] = np.sqrt(\n",
    "        (df_eng['Latitude'] - sf_lat)**2 + (df_eng['Longitude'] - sf_lon)**2)\n",
    "    df_eng['min_city_dist'] = df_eng[['dist_to_LA', 'dist_to_SF']].min(axis=1)\n",
    "\n",
    "    print(f'Original features: {len(data.columns)}')\n",
    "    new_features = [c for c in df_eng.columns if c not in data.columns]\n",
    "    print(f'Engineered features: {len(new_features)}')\n",
    "    print(f'Total features: {len(df_eng.columns)}')\n",
    "    print()\n",
    "    for feat in new_features:\n",
    "        vals = df_eng[feat].dropna()\n",
    "        print(f'  {feat:25s}: mean={vals.mean():8.3f}, std={vals.std():8.3f}')\n",
    "\n",
    "    return df_eng\n",
    "\n",
    "\n",
    "df_engineered = engineer_features(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how feature engineering improves the relationship with the",
    "target variable. The log transform, for instance, often linearizes skewed",
    "relationships."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_feature_engineering(original: pd.DataFrame, engineered: pd.DataFrame) -> None:\n",
    "    \"\"\"Compare original vs engineered features' relationship with target.\n",
    "\n",
    "    Args:\n",
    "        original: Original DataFrame.\n",
    "        engineered: DataFrame with engineered features.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "\n",
    "    target = 'MedHouseVal'\n",
    "    sample_idx = np.random.choice(len(original), 2000, replace=False)\n",
    "\n",
    "    # Row 1: Original features\n",
    "    pairs_orig = [('MedInc', target), ('Population', target), ('AveRooms', target)]\n",
    "    for idx, (feat, tgt) in enumerate(pairs_orig):\n",
    "        axes[0, idx].scatter(original[feat].iloc[sample_idx],\n",
    "                            original[tgt].iloc[sample_idx],\n",
    "                            s=5, alpha=0.3, color='#1E88E5')\n",
    "        axes[0, idx].set_xlabel(feat)\n",
    "        axes[0, idx].set_ylabel(tgt)\n",
    "        axes[0, idx].set_title(f'Original: {feat}')\n",
    "\n",
    "    # Row 2: Engineered features\n",
    "    pairs_eng = [('log_MedInc', target), ('log_Population', target),\n",
    "                 ('rooms_per_household', target)]\n",
    "    for idx, (feat, tgt) in enumerate(pairs_eng):\n",
    "        vals = engineered[feat].iloc[sample_idx]\n",
    "        # Clip extreme values for better visualization\n",
    "        lower, upper = vals.quantile(0.01), vals.quantile(0.99)\n",
    "        mask = (vals >= lower) & (vals <= upper)\n",
    "        axes[1, idx].scatter(vals[mask],\n",
    "                            engineered[tgt].iloc[sample_idx][mask],\n",
    "                            s=5, alpha=0.3, color='#43A047')\n",
    "        axes[1, idx].set_xlabel(feat)\n",
    "        axes[1, idx].set_ylabel(tgt)\n",
    "        axes[1, idx].set_title(f'Engineered: {feat}')\n",
    "\n",
    "    plt.suptitle('Feature Engineering: Original vs Transformed', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_feature_engineering(df, df_engineered)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 2 — Putting It All Together: DataPipeline Class",
    "",
    "We've built individual preprocessing components. Now let's assemble them into",
    "a reusable `DataPipeline` class that handles the full EDA → clean → encode →",
    "engineer workflow in a structured, reproducible way."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"End-to-end data preprocessing pipeline for tabular ML.\n",
    "\n",
    "    Handles missing values, categorical encoding, feature engineering,\n",
    "    and train/test splitting in a reproducible manner.\n",
    "\n",
    "    Attributes:\n",
    "        impute_strategy: Strategy for handling missing values.\n",
    "        encoding_method: Strategy for encoding categorical features.\n",
    "        numeric_columns: List of numeric column names after fitting.\n",
    "        categorical_columns: List of categorical column names after fitting.\n",
    "        impute_values: Dictionary of column → imputation value (after fit).\n",
    "        encoding_maps: Dictionary of column → encoding mapping (after fit).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        impute_strategy: str = 'median',\n",
    "        encoding_method: str = 'onehot',\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize pipeline with strategy choices.\n",
    "\n",
    "        Args:\n",
    "            impute_strategy: 'mean', 'median', or 'drop'.\n",
    "            encoding_method: 'onehot', 'label', or 'ordinal'.\n",
    "        \"\"\"\n",
    "        self.impute_strategy = impute_strategy\n",
    "        self.encoding_method = encoding_method\n",
    "        self.numeric_columns: list[str] = []\n",
    "        self.categorical_columns: list[str] = []\n",
    "        self.impute_values: dict[str, float] = {}\n",
    "        self.encoding_maps: dict[str, dict] = {}\n",
    "        self._fitted = False\n",
    "\n",
    "    def fit(self, data: pd.DataFrame, target_col: str | None = None) -> 'DataPipeline':\n",
    "        \"\"\"Learn preprocessing parameters from training data.\n",
    "\n",
    "        Args:\n",
    "            data: Training DataFrame.\n",
    "            target_col: Name of target column (excluded from features).\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining.\n",
    "        \"\"\"\n",
    "        feature_cols = [c for c in data.columns if c != target_col]\n",
    "\n",
    "        # Identify column types\n",
    "        self.numeric_columns = [\n",
    "            c for c in feature_cols\n",
    "            if data[c].dtype in [np.float64, np.float32, np.int64, np.int32]\n",
    "        ]\n",
    "        self.categorical_columns = [\n",
    "            c for c in feature_cols\n",
    "            if data[c].dtype in ['object', 'category']\n",
    "        ]\n",
    "\n",
    "        # Learn imputation values\n",
    "        for col in self.numeric_columns:\n",
    "            if self.impute_strategy == 'mean':\n",
    "                self.impute_values[col] = data[col].mean()\n",
    "            elif self.impute_strategy == 'median':\n",
    "                self.impute_values[col] = data[col].median()\n",
    "\n",
    "        # Learn encoding maps\n",
    "        for col in self.categorical_columns:\n",
    "            categories = sorted(data[col].astype(str).unique())\n",
    "            self.encoding_maps[col] = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply learned preprocessing to data.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame to transform.\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed DataFrame with only numeric columns.\n",
    "        \"\"\"\n",
    "        assert self._fitted, 'Pipeline must be fit before transform'\n",
    "        result = data.copy()\n",
    "\n",
    "        # Impute missing values\n",
    "        if self.impute_strategy == 'drop':\n",
    "            result = result.dropna(subset=self.numeric_columns)\n",
    "        else:\n",
    "            for col in self.numeric_columns:\n",
    "                if col in result.columns:\n",
    "                    result[col] = result[col].fillna(self.impute_values.get(col, 0))\n",
    "\n",
    "        # Encode categoricals\n",
    "        if self.encoding_method == 'onehot':\n",
    "            result = pd.get_dummies(\n",
    "                result, columns=self.categorical_columns,\n",
    "                prefix=self.categorical_columns, dtype=int,\n",
    "            )\n",
    "        elif self.encoding_method == 'label':\n",
    "            for col in self.categorical_columns:\n",
    "                if col in result.columns:\n",
    "                    mapping = self.encoding_maps[col]\n",
    "                    result[col] = result[col].astype(str).map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def fit_transform(self, data: pd.DataFrame, target_col: str | None = None) -> pd.DataFrame:\n",
    "        \"\"\"Fit and transform in one step.\n",
    "\n",
    "        Args:\n",
    "            data: Training DataFrame.\n",
    "            target_col: Name of target column.\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed DataFrame.\n",
    "        \"\"\"\n",
    "        return self.fit(data, target_col).transform(data)\n",
    "\n",
    "    def summary(self) -> None:\n",
    "        \"\"\"Print pipeline configuration summary.\"\"\"\n",
    "        assert self._fitted, 'Pipeline not fitted yet'\n",
    "        print(f'=== DataPipeline Summary ===')\n",
    "        print(f'  Imputation: {self.impute_strategy}')\n",
    "        print(f'  Encoding: {self.encoding_method}')\n",
    "        print(f'  Numeric columns: {len(self.numeric_columns)}')\n",
    "        print(f'  Categorical columns: {len(self.categorical_columns)}')\n",
    "        if self.impute_values:\n",
    "            print(f'  Impute values: {dict(list(self.impute_values.items())[:3])}...')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the pipeline on our data with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def test_pipeline() -> None:\n",
    "    \"\"\"Test DataPipeline with various configurations.\"\"\"\n",
    "    # Create test data with categoricals and missing values\n",
    "    test_data = create_dataset_with_missing(df_cat, frac=0.05)\n",
    "\n",
    "    # Pipeline 1: Median imputation + one-hot encoding\n",
    "    pipe1 = DataPipeline(impute_strategy='median', encoding_method='onehot')\n",
    "    result1 = pipe1.fit_transform(test_data, target_col='MedHouseVal')\n",
    "    pipe1.summary()\n",
    "    print(f'  Output shape: {result1.shape}')\n",
    "    print(f'  Missing values: {result1.isnull().sum().sum()}')\n",
    "    print()\n",
    "\n",
    "    # Pipeline 2: Mean imputation + label encoding\n",
    "    pipe2 = DataPipeline(impute_strategy='mean', encoding_method='label')\n",
    "    result2 = pipe2.fit_transform(test_data, target_col='MedHouseVal')\n",
    "    pipe2.summary()\n",
    "    print(f'  Output shape: {result2.shape}')\n",
    "    print(f'  Missing values: {result2.isnull().sum().sum()}')\n",
    "    print()\n",
    "\n",
    "    # Pipeline 3: Drop missing\n",
    "    pipe3 = DataPipeline(impute_strategy='drop', encoding_method='onehot')\n",
    "    result3 = pipe3.fit_transform(test_data, target_col='MedHouseVal')\n",
    "    pipe3.summary()\n",
    "    print(f'  Output shape: {result3.shape}')\n",
    "    print(f'  Rows dropped: {len(test_data) - len(result3)}')\n",
    "\n",
    "\n",
    "test_pipeline()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 3 — Application: Complete EDA & Preprocessing Pipeline",
    "",
    "Now we apply everything to the California Housing dataset in a realistic",
    "workflow: full EDA, correlation analysis, preprocessing, and conversion",
    "to NumPy/PyTorch for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Comprehensive EDA",
    "",
    "A thorough EDA explores distributions, correlations, and outliers before",
    "any modeling. This prevents surprises later."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def comprehensive_eda(data: pd.DataFrame, target: str) -> None:\n",
    "    \"\"\"Perform comprehensive exploratory data analysis.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame to analyze.\n",
    "        target: Name of the target column.\n",
    "    \"\"\"\n",
    "    print('=== Dataset Overview ===')\n",
    "    print(f'Shape: {data.shape}')\n",
    "    print(f'Memory: {data.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB')\n",
    "    print(f'Duplicates: {data.duplicated().sum()}')\n",
    "    print()\n",
    "\n",
    "    # Distribution summary\n",
    "    print('=== Feature Distributions ===')\n",
    "    desc = data.describe().T\n",
    "    desc['skew'] = data.skew()\n",
    "    desc['kurtosis'] = data.kurtosis()\n",
    "    desc['iqr'] = desc['75%'] - desc['25%']\n",
    "    print(desc[['mean', 'std', 'min', 'max', 'skew', 'kurtosis']].to_string())\n",
    "    print()\n",
    "\n",
    "    # Outlier detection using IQR\n",
    "    print('=== Outliers (IQR Method) ===')\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((data[col] < Q1 - 1.5 * IQR) | (data[col] > Q3 + 1.5 * IQR)).sum()\n",
    "        if outliers > 0:\n",
    "            print(f'  {col}: {outliers} outliers ({outliers/len(data):.1%})')\n",
    "\n",
    "\n",
    "comprehensive_eda(df, 'MedHouseVal')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_distributions(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot distribution of all numeric features.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame to visualize.\n",
    "    \"\"\"\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        axes[idx].hist(data[col], bins=50, color='#1E88E5', alpha=0.7, edgecolor='white')\n",
    "        axes[idx].set_title(f'{col} (skew={data[col].skew():.2f})')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].axvline(data[col].mean(), color='red', linestyle='--',\n",
    "                          label=f'mean={data[col].mean():.2f}')\n",
    "        axes[idx].axvline(data[col].median(), color='green', linestyle='--',\n",
    "                          label=f'median={data[col].median():.2f}')\n",
    "        axes[idx].legend(fontsize=8)\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.suptitle('Feature Distributions', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_distributions(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Correlation Analysis",
    "",
    "Understanding correlations between features and with the target helps us:",
    "- Identify the most predictive features",
    "- Detect multicollinearity (redundant features)",
    "- Guide feature selection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def correlation_analysis(data: pd.DataFrame, target: str) -> None:\n",
    "    \"\"\"Analyze and visualize feature correlations.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame to analyze.\n",
    "        target: Target column name.\n",
    "    \"\"\"\n",
    "    corr_matrix = data.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "    # Correlation with target\n",
    "    target_corr = corr_matrix[target].drop(target).sort_values(ascending=False)\n",
    "    print(f'=== Correlation with {target} ===')\n",
    "    for feat, corr_val in target_corr.items():\n",
    "        bar = '█' * int(abs(corr_val) * 20)\n",
    "        sign = '+' if corr_val > 0 else '-'\n",
    "        print(f'  {feat:20s}: {corr_val:+.4f} {sign}{bar}')\n",
    "    print()\n",
    "\n",
    "    # Correlation heatmap\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Full heatmap\n",
    "    im = axes[0].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "    axes[0].set_xticks(range(len(corr_matrix)))\n",
    "    axes[0].set_xticklabels(corr_matrix.columns, rotation=45, ha='right', fontsize=8)\n",
    "    axes[0].set_yticks(range(len(corr_matrix)))\n",
    "    axes[0].set_yticklabels(corr_matrix.columns, fontsize=8)\n",
    "    axes[0].set_title('Correlation Heatmap')\n",
    "    plt.colorbar(im, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    # Annotate significant correlations\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(len(corr_matrix)):\n",
    "            val = corr_matrix.iloc[i, j]\n",
    "            if abs(val) > 0.3 and i != j:\n",
    "                axes[0].text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=7)\n",
    "\n",
    "    # Bar chart of target correlations\n",
    "    colors = ['#43A047' if v > 0 else '#E53935' for v in target_corr.values]\n",
    "    axes[1].barh(target_corr.index, target_corr.values, color=colors)\n",
    "    axes[1].set_xlabel(f'Correlation with {target}')\n",
    "    axes[1].set_title(f'Feature Importance (by correlation)')\n",
    "    axes[1].axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    axes[1].grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "correlation_analysis(df, 'MedHouseVal')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 GroupBy and Aggregation",
    "",
    "Pandas' `groupby` is one of its most powerful features. It allows split-apply-combine",
    "operations that are essential for understanding data subgroups and creating",
    "aggregate features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_groupby(data: pd.DataFrame) -> None:\n",
    "    \"\"\"Show groupby patterns commonly used in ML feature engineering.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with categorical and numeric columns.\n",
    "    \"\"\"\n",
    "    # Add income bracket for grouping\n",
    "    df_grouped = data.copy()\n",
    "    df_grouped['income_bracket'] = pd.cut(\n",
    "        df_grouped['MedInc'], bins=[0, 2, 4, 6, 8, np.inf],\n",
    "        labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "\n",
    "    # Basic groupby: mean target by income bracket\n",
    "    print('=== Mean House Value by Income Bracket ===')\n",
    "    group_means = df_grouped.groupby('income_bracket', observed=True)['MedHouseVal'].agg(\n",
    "        ['mean', 'median', 'std', 'count']\n",
    "    )\n",
    "    print(group_means.to_string())\n",
    "    print()\n",
    "\n",
    "    # Multiple aggregations\n",
    "    print('=== Multi-Column Aggregation ===')\n",
    "    agg_result = df_grouped.groupby('income_bracket', observed=True).agg({\n",
    "        'MedHouseVal': ['mean', 'std'],\n",
    "        'HouseAge': 'mean',\n",
    "        'AveRooms': 'mean',\n",
    "        'Population': 'sum',\n",
    "    })\n",
    "    # Flatten multi-level columns\n",
    "    agg_result.columns = ['_'.join(col).strip() for col in agg_result.columns]\n",
    "    print(agg_result.to_string())\n",
    "    print()\n",
    "\n",
    "    # Transform: add group statistics back to original rows\n",
    "    df_grouped['bracket_mean_val'] = df_grouped.groupby(\n",
    "        'income_bracket', observed=True\n",
    "    )['MedHouseVal'].transform('mean')\n",
    "    df_grouped['bracket_std_val'] = df_grouped.groupby(\n",
    "        'income_bracket', observed=True\n",
    "    )['MedHouseVal'].transform('std')\n",
    "    df_grouped['val_zscore_in_bracket'] = (\n",
    "        (df_grouped['MedHouseVal'] - df_grouped['bracket_mean_val'])\n",
    "        / df_grouped['bracket_std_val']\n",
    "    )\n",
    "    print('=== Group-Level Features (transform) ===')\n",
    "    print(df_grouped[['income_bracket', 'MedHouseVal', 'bracket_mean_val',\n",
    "                       'val_zscore_in_bracket']].head(10).to_string())\n",
    "\n",
    "\n",
    "demonstrate_groupby(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Pandas ↔ NumPy ↔ PyTorch Conversion",
    "",
    "The final step in any data pipeline is converting the processed DataFrame",
    "into the format your model expects. For classical ML, that's NumPy arrays.",
    "For deep learning, that's PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_conversions(data: pd.DataFrame, target_col: str) -> None:\n",
    "    \"\"\"Show conversion between Pandas, NumPy, and PyTorch.\n",
    "\n",
    "    Args:\n",
    "        data: Preprocessed DataFrame.\n",
    "        target_col: Name of the target column.\n",
    "    \"\"\"\n",
    "    features = data.drop(columns=[target_col]).select_dtypes(include=[np.number])\n",
    "    target = data[target_col]\n",
    "\n",
    "    # ── Pandas → NumPy ──────────────────────────────────────────────────────\n",
    "    X_np = features.values  # or features.to_numpy()\n",
    "    y_np = target.values\n",
    "    print('=== Pandas → NumPy ===')\n",
    "    print(f'  X: {X_np.shape}, dtype={X_np.dtype}')\n",
    "    print(f'  y: {y_np.shape}, dtype={y_np.dtype}')\n",
    "    print()\n",
    "\n",
    "    # ── NumPy → PyTorch ─────────────────────────────────────────────────────\n",
    "    X_tensor = torch.from_numpy(X_np.astype(np.float32))\n",
    "    y_tensor = torch.from_numpy(y_np.astype(np.float32))\n",
    "    print('=== NumPy → PyTorch ===')\n",
    "    print(f'  X: {X_tensor.shape}, dtype={X_tensor.dtype}')\n",
    "    print(f'  y: {y_tensor.shape}, dtype={y_tensor.dtype}')\n",
    "    print()\n",
    "\n",
    "    # ── Train/Test Split (sklearn convention for Module 1-4) ────────────────\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_np, y_np, test_size=0.2, random_state=SEED)\n",
    "    print('=== Train/Test Split (80/20) ===')\n",
    "    print(f'  X_train: {X_train.shape}')\n",
    "    print(f'  X_test:  {X_test.shape}')\n",
    "    print(f'  y_train: {y_train.shape}')\n",
    "    print(f'  y_test:  {y_test.shape}')\n",
    "    print()\n",
    "\n",
    "    # ── PyTorch → Pandas (for analysis after model predictions) ─────────────\n",
    "    predictions = torch.randn(len(X_test))  # Simulated predictions\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': predictions.numpy(),\n",
    "        'Error': y_test - predictions.numpy(),\n",
    "    })\n",
    "    print('=== PyTorch → Pandas (analysis) ===')\n",
    "    print(results_df.describe().to_string())\n",
    "    print()\n",
    "\n",
    "    # Memory comparison\n",
    "    mem_pandas = features.memory_usage(deep=True).sum()\n",
    "    mem_numpy = X_np.nbytes\n",
    "    mem_torch = X_tensor.element_size() * X_tensor.nelement()\n",
    "    print('=== Memory Comparison ===')\n",
    "    mem_df = pd.DataFrame({\n",
    "        'Format': ['Pandas (float64)', 'NumPy (float64)', 'PyTorch (float32)'],\n",
    "        'Memory (MB)': [mem_pandas / 1e6, mem_numpy / 1e6, mem_torch / 1e6],\n",
    "    })\n",
    "    print(mem_df.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_conversions(df, 'MedHouseVal')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 4 — Evaluation & Analysis",
    "",
    "Let's evaluate our preprocessing pipeline by measuring its impact on a simple",
    "model, analyzing common errors, and benchmarking Pandas operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pipeline Impact on Model Performance",
    "",
    "To see if our preprocessing actually helps, we'll train a simple linear",
    "regression on raw vs preprocessed data and compare results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_pipeline_impact() -> pd.DataFrame:\n",
    "    \"\"\"Compare model performance with different preprocessing strategies.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with performance comparison.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    target_col = 'MedHouseVal'\n",
    "    records: list[dict] = []\n",
    "\n",
    "    # Raw data (no preprocessing)\n",
    "    X_raw = df.drop(columns=[target_col]).values\n",
    "    y = df[target_col].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_raw, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    model_raw = LinearRegression().fit(X_train, y_train)\n",
    "    pred_raw = model_raw.predict(X_test)\n",
    "    records.append({\n",
    "        'Pipeline': 'Raw (no preprocessing)',\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, pred_raw)),\n",
    "        'R²': r2_score(y_test, pred_raw),\n",
    "        'Features': X_raw.shape[1],\n",
    "    })\n",
    "\n",
    "    # Standardized data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_train = scaler.fit_transform(X_train)\n",
    "    X_scaled_test = scaler.transform(X_test)\n",
    "\n",
    "    model_scaled = LinearRegression().fit(X_scaled_train, y_train)\n",
    "    pred_scaled = model_scaled.predict(X_scaled_test)\n",
    "    records.append({\n",
    "        'Pipeline': 'StandardScaler',\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, pred_scaled)),\n",
    "        'R²': r2_score(y_test, pred_scaled),\n",
    "        'Features': X_scaled_train.shape[1],\n",
    "    })\n",
    "\n",
    "    # With engineered features\n",
    "    df_eng = engineer_features(df)\n",
    "    numeric_eng = df_eng.select_dtypes(include=[np.number]).drop(columns=[target_col])\n",
    "    # Remove any inf values\n",
    "    numeric_eng = numeric_eng.replace([np.inf, -np.inf], np.nan).fillna(numeric_eng.median())\n",
    "    X_eng = numeric_eng.values\n",
    "    X_eng_train, X_eng_test, y_eng_train, y_eng_test = train_test_split(\n",
    "        X_eng, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    scaler_eng = StandardScaler()\n",
    "    X_eng_train_s = scaler_eng.fit_transform(X_eng_train)\n",
    "    X_eng_test_s = scaler_eng.transform(X_eng_test)\n",
    "\n",
    "    model_eng = LinearRegression().fit(X_eng_train_s, y_eng_train)\n",
    "    pred_eng = model_eng.predict(X_eng_test_s)\n",
    "    records.append({\n",
    "        'Pipeline': 'Scaled + Engineered Features',\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_eng_test, pred_eng)),\n",
    "        'R²': r2_score(y_eng_test, pred_eng),\n",
    "        'Features': X_eng_train_s.shape[1],\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "impact_df = evaluate_pipeline_impact()\n",
    "print('=== Preprocessing Impact on Linear Regression ===')\n",
    "print(impact_df.to_string(index=False))\n",
    "print()\n",
    "print('Feature engineering improves R² because the engineered features')\n",
    "print('(log transforms, interaction terms, distance) capture non-linear')\n",
    "print('relationships that raw linear regression cannot learn.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Error Analysis: Where Does Preprocessing Fail?",
    "",
    "Let's examine which samples have the largest prediction errors and whether",
    "they share common characteristics — this reveals preprocessing gaps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def error_analysis() -> None:\n",
    "    \"\"\"Analyze prediction errors to identify preprocessing weaknesses.\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    target_col = 'MedHouseVal'\n",
    "    X = df.drop(columns=[target_col]).values\n",
    "    y = df[target_col].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    model = LinearRegression().fit(X_train_s, y_train)\n",
    "    preds = model.predict(X_test_s)\n",
    "    errors = y_test - preds\n",
    "    abs_errors = np.abs(errors)\n",
    "\n",
    "    # Create analysis DataFrame\n",
    "    feature_names = df.drop(columns=[target_col]).columns\n",
    "    analysis = pd.DataFrame(X_test, columns=feature_names)\n",
    "    analysis['actual'] = y_test\n",
    "    analysis['predicted'] = preds\n",
    "    analysis['error'] = errors\n",
    "    analysis['abs_error'] = abs_errors\n",
    "\n",
    "    # Error distribution\n",
    "    print('=== Error Distribution ===')\n",
    "    print(f'  Mean Error: {errors.mean():.4f}')\n",
    "    print(f'  Std Error: {errors.std():.4f}')\n",
    "    print(f'  Mean Abs Error: {abs_errors.mean():.4f}')\n",
    "    print(f'  Median Abs Error: {np.median(abs_errors):.4f}')\n",
    "    print(f'  Max Abs Error: {abs_errors.max():.4f}')\n",
    "    print()\n",
    "\n",
    "    # Worst predictions\n",
    "    worst = analysis.nlargest(10, 'abs_error')\n",
    "    print('=== Top 10 Worst Predictions ===')\n",
    "    print(worst[['MedInc', 'HouseAge', 'AveRooms', 'actual', 'predicted', 'error']].to_string())\n",
    "    print()\n",
    "\n",
    "    # Error by income bracket\n",
    "    analysis['income_bracket'] = pd.cut(\n",
    "        analysis['MedInc'], bins=[0, 2, 4, 6, 8, np.inf],\n",
    "        labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    print('=== Error by Income Bracket ===')\n",
    "    error_by_bracket = analysis.groupby('income_bracket', observed=True)['abs_error'].agg(\n",
    "        ['mean', 'median', 'count']).round(4)\n",
    "    print(error_by_bracket.to_string())\n",
    "    print()\n",
    "    print('Higher income brackets tend to have larger errors because the')\n",
    "    print('relationship between income and house value is non-linear.')\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    axes[0].hist(errors, bins=50, color='#1E88E5', alpha=0.7, edgecolor='white')\n",
    "    axes[0].set_xlabel('Prediction Error')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Error Distribution')\n",
    "    axes[0].axvline(0, color='red', linestyle='--')\n",
    "\n",
    "    axes[1].scatter(y_test, preds, s=3, alpha=0.3, color='#1E88E5')\n",
    "    axes[1].plot([0, 5], [0, 5], 'r--', linewidth=2, label='Perfect')\n",
    "    axes[1].set_xlabel('Actual')\n",
    "    axes[1].set_ylabel('Predicted')\n",
    "    axes[1].set_title('Predicted vs Actual')\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].scatter(analysis['MedInc'], abs_errors, s=3, alpha=0.3, color='#E53935')\n",
    "    axes[2].set_xlabel('Median Income')\n",
    "    axes[2].set_ylabel('Absolute Error')\n",
    "    axes[2].set_title('Error vs Income')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "error_analysis()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance Benchmark: Pandas Operations",
    "",
    "Let's benchmark common Pandas operations to understand their computational cost.",
    "This helps decide when to convert to NumPy for performance-critical code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def benchmark_pandas_ops() -> pd.DataFrame:\n",
    "    \"\"\"Benchmark common Pandas operations.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing results.\n",
    "    \"\"\"\n",
    "    # Use a larger DataFrame for meaningful benchmarks\n",
    "    np.random.seed(SEED)\n",
    "    n = 100_000\n",
    "    big_df = pd.DataFrame({\n",
    "        'A': np.random.randn(n),\n",
    "        'B': np.random.randn(n),\n",
    "        'C': np.random.randn(n),\n",
    "        'cat': np.random.choice(['x', 'y', 'z'], n),\n",
    "    })\n",
    "    big_np = big_df[['A', 'B', 'C']].values\n",
    "\n",
    "    records: list[dict] = []\n",
    "\n",
    "    def measure(name: str, func: callable) -> None:\n",
    "        \"\"\"Measure and record timing.\"\"\"\n",
    "        for _ in range(3):\n",
    "            func()\n",
    "        times = []\n",
    "        for _ in range(5):\n",
    "            start = time.perf_counter()\n",
    "            func()\n",
    "            times.append(time.perf_counter() - start)\n",
    "        records.append({'Operation': name, 'Time (ms)': np.mean(times) * 1000})\n",
    "\n",
    "    # Pandas operations\n",
    "    measure('df.sum()', lambda: big_df[['A', 'B', 'C']].sum())\n",
    "    measure('df.mean()', lambda: big_df[['A', 'B', 'C']].mean())\n",
    "    measure('df.describe()', lambda: big_df.describe())\n",
    "    measure('df.groupby().mean()', lambda: big_df.groupby('cat')[['A', 'B']].mean())\n",
    "    measure('df.sort_values()', lambda: big_df.sort_values('A'))\n",
    "    measure('df[mask] filter', lambda: big_df[big_df['A'] > 0])\n",
    "    measure('df.apply(lambda)', lambda: big_df['A'].apply(lambda x: x ** 2))\n",
    "    measure('df.fillna()', lambda: big_df.fillna(0))\n",
    "\n",
    "    # NumPy equivalents\n",
    "    measure('np.sum()', lambda: np.sum(big_np, axis=0))\n",
    "    measure('np.mean()', lambda: np.mean(big_np, axis=0))\n",
    "    measure('np.sort()', lambda: np.sort(big_np[:, 0]))\n",
    "    measure('np[mask] filter', lambda: big_np[big_np[:, 0] > 0])\n",
    "    measure('np ** 2', lambda: big_np[:, 0] ** 2)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "ops_bench = benchmark_pandas_ops()\n",
    "print('=== Pandas vs NumPy Operation Speed (n=100K) ===')\n",
    "print(ops_bench.to_string(index=False))\n",
    "print()\n",
    "print('Pandas adds overhead for index management and type checking.')\n",
    "print('For numerical-only operations, NumPy is faster.')\n",
    "print('Use Pandas for data wrangling; convert to NumPy for computation.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Merge and Join Operations",
    "",
    "In practice, ML features often come from multiple tables that must be joined.",
    "Pandas provides SQL-style merge operations for combining DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_merge_operations() -> None:\n",
    "    \"\"\"Show merge/join patterns common in ML feature pipelines.\"\"\"\n",
    "    # Create sample tables\n",
    "    customers = pd.DataFrame({\n",
    "        'customer_id': [1, 2, 3, 4, 5],\n",
    "        'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "        'segment': ['premium', 'basic', 'premium', 'basic', 'premium'],\n",
    "    })\n",
    "\n",
    "    orders = pd.DataFrame({\n",
    "        'order_id': [101, 102, 103, 104, 105, 106],\n",
    "        'customer_id': [1, 2, 1, 3, 2, 6],  # customer 6 doesn't exist\n",
    "        'amount': [100.0, 50.0, 200.0, 150.0, 75.0, 30.0],\n",
    "    })\n",
    "\n",
    "    print('=== customers ===')\n",
    "    print(customers.to_string(index=False))\n",
    "    print()\n",
    "    print('=== orders ===')\n",
    "    print(orders.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Inner join: only matching rows\n",
    "    inner = pd.merge(customers, orders, on='customer_id', how='inner')\n",
    "    print(f'Inner join: {len(inner)} rows (only matching customer_ids)')\n",
    "    print(inner.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Left join: keep all customers\n",
    "    left = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "    print(f'Left join: {len(left)} rows (all customers, NaN for no orders)')\n",
    "    print(left.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Aggregation after join: total spend per customer\n",
    "    customer_spend = orders.groupby('customer_id')['amount'].agg(\n",
    "        total_spend='sum', num_orders='count', avg_order='mean'\n",
    "    ).reset_index()\n",
    "    enriched = pd.merge(customers, customer_spend, on='customer_id', how='left')\n",
    "    enriched = enriched.fillna({'total_spend': 0, 'num_orders': 0, 'avg_order': 0})\n",
    "    print('=== Enriched Customer Table ===')\n",
    "    print(enriched.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Merge types summary\n",
    "    merge_ref = pd.DataFrame({\n",
    "        'Join Type': ['inner', 'left', 'right', 'outer'],\n",
    "        'Keeps': ['Matching only', 'All left + matching right',\n",
    "                  'Matching left + all right', 'All from both'],\n",
    "        'NaN Where': ['Never', 'Right cols for non-matches',\n",
    "                      'Left cols for non-matches', 'Both sides for non-matches'],\n",
    "    })\n",
    "    print('=== Merge Type Reference ===')\n",
    "    print(merge_ref.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_merge_operations()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 5 — Summary & Lessons Learned",
    "",
    "### Key Takeaways",
    "",
    "1. **Always start with EDA.** Before any modeling, check data types, missing",
    "   values, distributions, and correlations. `df.describe()`, `df.info()`, and",
    "   `df.isnull().sum()` are your first three commands on any new dataset.",
    "",
    "2. **Handle missing values deliberately.** Choose between deletion (if few and",
    "   random), imputation (mean/median for numerical, mode for categorical), or",
    "   indicator variables (if missingness itself is informative). Never silently",
    "   ignore NaN values.",
    "",
    "3. **Choose encoding based on the feature type.** Use ordinal encoding for",
    "   ordered categories (low/medium/high), one-hot for unordered categories with",
    "   few values, and target/frequency encoding for high-cardinality features.",
    "",
    "4. **Feature engineering can matter more than model choice.** Adding log",
    "   transforms, interaction features, and domain-specific variables improved",
    "   our linear regression more than switching to a fancier algorithm would.",
    "",
    "5. **Use Pandas for wrangling, NumPy/PyTorch for computation.** Pandas adds",
    "   overhead from index management. Convert to NumPy arrays or PyTorch tensors",
    "   before training loops.",
    "",
    "### What's Next",
    "",
    "→ **01-04 (Visualization with Matplotlib)** teaches the plotting skills needed",
    "  to create the EDA visualizations, decision boundaries, and training curves",
    "  used throughout this course.",
    "",
    "### Going Further",
    "",
    "- [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html) —",
    "  Official comprehensive documentation",
    "- [Feature Engineering for ML (Alice Zheng)](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/) —",
    "  Practical guide to feature engineering techniques",
    "- [Handling Missing Data (scikit-learn)](https://scikit-learn.org/stable/modules/impute.html) —",
    "  sklearn's imputation strategies including KNN and iterative imputers"
   ]
  }
 ]
}