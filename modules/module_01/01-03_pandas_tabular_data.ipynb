{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations\n",
    "## 1-03: Pandas for Tabular Data\n",
    "\n",
    "**Objective:** Master Pandas for exploratory data analysis — the essential first step before any ML modeling.\n",
    "\n",
    "**Prerequisites:** 1-01 (Python, NumPy & Tensor Speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 0 — Setup & Prerequisites\n",
    "\n",
    "This notebook covers the complete Pandas EDA workflow: summary statistics, missing-value handling,\n",
    "categorical encoding, and merge/join operations. We build every imputation and encoding strategy\n",
    "from scratch before comparing against library implementations.\n",
    "\n",
    "**Prerequisites:** 1-01 (Python, NumPy & Tensor Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Reproducibility ─────────────────────────────────────────────────────────\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & EDA\n",
    "\n",
    "We load two classic sklearn datasets:\n",
    "- **Iris** — small classification dataset (150 samples, 4 features, 3 classes).\n",
    "- **California Housing** — larger regression dataset (~20k samples, 8 features).\n",
    "\n",
    "Both are used throughout this notebook to demonstrate Pandas operations on real tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load Iris ────────────────────────────────────────────────────────────────\n",
    "iris_bunch = load_iris()\n",
    "iris_df = pd.DataFrame(iris_bunch.data, columns=iris_bunch.feature_names)\n",
    "iris_df[\"species\"] = pd.Categorical.from_codes(iris_bunch.target, iris_bunch.target_names)\n",
    "\n",
    "print(\"=== Iris Dataset ===\")\n",
    "print(f\"Shape: {iris_df.shape}\")\n",
    "print(f\"\\nDtypes:\\n{iris_df.dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load California Housing ─────────────────────────────────────────────────\n",
    "housing_bunch = fetch_california_housing()\n",
    "housing_df = pd.DataFrame(housing_bunch.data, columns=housing_bunch.feature_names)\n",
    "housing_df[\"MedHouseVal\"] = housing_bunch.target\n",
    "\n",
    "print(\"=== California Housing Dataset ===\")\n",
    "print(f\"Shape: {housing_df.shape}\")\n",
    "print(f\"\\nDtypes:\\n{housing_df.dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Basic Statistics ─────────────────────────────────────────────────────────\n",
    "print(\"=== Iris — Summary Statistics ===\")\n",
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== California Housing — Summary Statistics ===\")\n",
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 — Pandas Operations from Scratch\n",
    "\n",
    "We explore the core Pandas operations that form the backbone of any EDA workflow:\n",
    "DataFrames/Series basics, summary statistics, missing-value handling, categorical encoding,\n",
    "and merge/join operations.\n",
    "\n",
    "### 1.1 DataFrames and Series Basics\n",
    "\n",
    "A **Series** is a one-dimensional labeled array. A **DataFrame** is a two-dimensional\n",
    "labeled table — essentially a dictionary of Series sharing the same index. DataFrames can be\n",
    "created from dictionaries, NumPy arrays, or lists of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Creating DataFrames from different sources ────────────────────────────────\n",
    "\n",
    "# From a dictionary\n",
    "dict_df = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n",
    "    \"age\": [25, 30, 35, 28],\n",
    "    \"score\": [88.5, 92.3, 79.1, 95.0],\n",
    "})\n",
    "print(\"DataFrame from dict:\")\n",
    "print(dict_df)\n",
    "\n",
    "# From a NumPy array\n",
    "array_data = np.random.randn(4, 3)\n",
    "array_df = pd.DataFrame(array_data, columns=[\"feature_a\", \"feature_b\", \"feature_c\"])\n",
    "print(\"\\nDataFrame from NumPy array:\")\n",
    "print(array_df)\n",
    "\n",
    "# From a list of dicts (CSV-like row-oriented data)\n",
    "rows = [\n",
    "    {\"city\": \"NYC\", \"population\": 8336817, \"area_sq_mi\": 302.6},\n",
    "    {\"city\": \"LA\", \"population\": 3979576, \"area_sq_mi\": 468.7},\n",
    "    {\"city\": \"Chicago\", \"population\": 2693976, \"area_sq_mi\": 227.3},\n",
    "]\n",
    "cities_df = pd.DataFrame(rows)\n",
    "print(\"\\nDataFrame from list of dicts:\")\n",
    "print(cities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Operations\n",
    "\n",
    "A Series supports element-wise arithmetic, boolean masking, and integer/label-based indexing.\n",
    "All operations are vectorized — no explicit loops needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Series: indexing, slicing, operations ─────────────────────────────────────\n",
    "ages = dict_df[\"age\"]\n",
    "print(f\"Type: {type(ages)}\")\n",
    "print(f\"First element (ages[0]):  {ages[0]}\")\n",
    "print(f\"Slice (ages[1:3]):\\n{ages[1:3]}\")\n",
    "print(f\"\\nVectorized ops — ages + 10:\\n{ages + 10}\")\n",
    "print(f\"\\nBoolean mask — ages > 28:\\n{ages[ages > 28]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Selection: `.loc` vs `.iloc`\n",
    "\n",
    "Pandas provides two main indexers:\n",
    "- **`.loc`** — label-based: selects by row/column *names*. Inclusive on both ends.\n",
    "- **`.iloc`** — integer-position-based: selects by *position*. Exclusive on the end (like Python slicing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── DataFrame selection: columns, rows, .loc vs .iloc ────────────────────────\n",
    "\n",
    "# Select a single column (returns Series)\n",
    "print(\"Single column (iris_df['species']):\\n\", iris_df[\"species\"].head(), \"\\n\")\n",
    "\n",
    "# Select multiple columns (returns DataFrame)\n",
    "print(\"Multiple columns:\")\n",
    "print(iris_df[[\"sepal length (cm)\", \"species\"]].head(), \"\\n\")\n",
    "\n",
    "# Filter rows with boolean condition\n",
    "setosa = iris_df[iris_df[\"species\"] == \"setosa\"]\n",
    "print(f\"Filtered rows (setosa only): {len(setosa)} rows\\n\")\n",
    "\n",
    "# .loc — label-based indexing\n",
    "print(\".loc[0:2, 'sepal length (cm)':'petal length (cm)']:\")\n",
    "print(iris_df.loc[0:2, \"sepal length (cm)\":\"petal length (cm)\"], \"\\n\")\n",
    "\n",
    "# .iloc — integer-position-based indexing\n",
    "print(\".iloc[0:3, 0:2]:\")\n",
    "print(iris_df.iloc[0:3, 0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 EDA Workflow\n",
    "\n",
    "A solid EDA workflow answers three questions:\n",
    "1. **What does each feature look like?** (`describe`, `value_counts`)\n",
    "2. **How do features relate to each other?** (`corr`, `groupby`)\n",
    "3. **Are there issues to fix?** (missing values, outliers, class imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── .describe() — summary statistics ─────────────────────────────────────────\n",
    "print(\"=== Iris — Numeric Summary ===\")\n",
    "print(iris_df.describe().round(2))\n",
    "\n",
    "print(\"\\n=== Iris — Categorical Summary ===\")\n",
    "print(iris_df.describe(include=[\"category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── .value_counts() — categorical distributions ──────────────────────────────\n",
    "print(\"Species distribution:\")\n",
    "print(iris_df[\"species\"].value_counts())\n",
    "print(f\"\\nBalanced: {iris_df['species'].value_counts().nunique() == 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy Aggregations\n",
    "\n",
    "`groupby()` splits a DataFrame by unique values of a column, applies an aggregation\n",
    "function to each group, and combines the results. This is the Pandas equivalent of\n",
    "SQL's `GROUP BY` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── .groupby() — per-group aggregations on Iris ──────────────────────────────\n",
    "iris_grouped = iris_df.groupby(\"species\").agg([\"mean\", \"std\", \"count\"])\n",
    "print(\"=== Per-Species Statistics ===\")\n",
    "iris_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "\n",
    "The Pearson correlation coefficient measures linear relationship between two variables.\n",
    "Values close to $+1$ or $-1$ indicate strong linear correlation; values near $0$ suggest\n",
    "no linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── .corr() — correlation matrix for Iris ────────────────────────────────────\n",
    "iris_numeric = iris_df.select_dtypes(include=[np.number])\n",
    "corr_matrix = iris_numeric.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "cax = ax.matshow(corr_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(range(len(corr_matrix.columns)))\n",
    "ax.set_yticks(range(len(corr_matrix.columns)))\n",
    "ax.set_xticklabels(corr_matrix.columns, rotation=45, ha=\"left\")\n",
    "ax.set_yticklabels(corr_matrix.columns)\n",
    "ax.set_title(\"Iris Feature Correlation Matrix\", pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### California Housing — Extended EDA\n",
    "\n",
    "California Housing is a larger dataset with more features, making it better suited\n",
    "for demonstrating skewness, kurtosis, and multi-feature correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── California Housing — feature distributions ───────────────────────────────\n",
    "housing_numeric = housing_df.select_dtypes(include=[np.number])\n",
    "housing_stats = housing_numeric.describe().T\n",
    "housing_stats[\"skewness\"] = housing_numeric.skew()\n",
    "housing_stats[\"kurtosis\"] = housing_numeric.kurtosis()\n",
    "print(\"=== California Housing — Extended Statistics ===\")\n",
    "housing_stats.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── California Housing — correlation heatmap ─────────────────────────────────\n",
    "housing_corr = housing_numeric.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cax = ax.matshow(housing_corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(range(len(housing_corr.columns)))\n",
    "ax.set_yticks(range(len(housing_corr.columns)))\n",
    "ax.set_xticklabels(housing_corr.columns, rotation=45, ha=\"left\", fontsize=8)\n",
    "ax.set_yticklabels(housing_corr.columns, fontsize=8)\n",
    "ax.set_title(\"California Housing Correlation Matrix\", pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Handling Missing Values\n",
    "\n",
    "Real-world datasets almost always have missing entries. The strategy for imputation\n",
    "depends on the data distribution and the downstream model:\n",
    "\n",
    "- **Mean imputation:** Best for roughly symmetric continuous features.\n",
    "- **Median imputation:** Robust to outliers and skewed distributions.\n",
    "- **Mode imputation:** Standard choice for categorical features.\n",
    "- **Forward-fill:** Appropriate for time-ordered data where the last known value carries forward.\n",
    "\n",
    "We first inject synthetic missing values into copies of our datasets, then implement\n",
    "each strategy from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inject synthetic missing values ──────────────────────────────────────────\n",
    "MISSING_FRACTION = 0.10  # 10% missing values\n",
    "\n",
    "\n",
    "def inject_missing_values(\n",
    "    df: pd.DataFrame,\n",
    "    fraction: float,\n",
    "    columns: list[str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Inject NaN values randomly into specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame (not modified in place).\n",
    "        fraction: Fraction of values to set to NaN per column (0.0 to 1.0).\n",
    "        columns: List of column names to inject NaNs into. If None, uses\n",
    "            all numeric columns.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with NaN values injected.\n",
    "    \"\"\"\n",
    "    df_missing = df.copy()\n",
    "    if columns is None:\n",
    "        columns = df_missing.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_rows = len(df_missing)\n",
    "    num_to_remove = int(num_rows * fraction)\n",
    "    for col in columns:\n",
    "        missing_indices = np.random.choice(num_rows, size=num_to_remove, replace=False)\n",
    "        df_missing.loc[missing_indices, col] = np.nan\n",
    "    return df_missing\n",
    "\n",
    "\n",
    "iris_missing = inject_missing_values(iris_df, MISSING_FRACTION,\n",
    "                                     columns=[\"sepal length (cm)\", \"petal width (cm)\"])\n",
    "housing_missing = inject_missing_values(housing_df, MISSING_FRACTION,\n",
    "                                        columns=[\"MedInc\", \"AveRooms\", \"AveOccup\"])\n",
    "\n",
    "print(\"=== Iris — Missing Values ===\")\n",
    "print(iris_missing.isnull().sum())\n",
    "print(f\"\\n=== Housing — Missing Values ===\")\n",
    "print(housing_missing.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 1: Mean Imputation\n",
    "\n",
    "Replace each missing value with the arithmetic mean of the non-missing values in that\n",
    "column. This preserves the overall mean but reduces variance (all imputed values\n",
    "cluster at the center)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Strategy 1: Mean Imputation (from scratch) ───────────────────────────────\n",
    "\n",
    "\n",
    "def impute_mean(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Impute missing values with the column mean.\n",
    "\n",
    "    Args:\n",
    "        series: A pandas Series with potential NaN values.\n",
    "\n",
    "    Returns:\n",
    "        A new Series with NaN values replaced by the column mean.\n",
    "    \"\"\"\n",
    "    col_mean = series.dropna().mean()\n",
    "    return series.fillna(col_mean)\n",
    "\n",
    "\n",
    "# Apply to iris\n",
    "iris_mean_imputed = iris_missing.copy()\n",
    "iris_mean_imputed[\"sepal length (cm)\"] = impute_mean(iris_missing[\"sepal length (cm)\"])\n",
    "iris_mean_imputed[\"petal width (cm)\"] = impute_mean(iris_missing[\"petal width (cm)\"])\n",
    "\n",
    "print(\"Mean-imputed Iris — remaining NaNs:\")\n",
    "print(iris_mean_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 2: Median Imputation\n",
    "\n",
    "Replace missing values with the median. Unlike the mean, the median is robust to\n",
    "outliers — a handful of extreme values will not distort the imputed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Strategy 2: Median Imputation (from scratch) ─────────────────────────────\n",
    "\n",
    "\n",
    "def impute_median(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Impute missing values with the column median.\n",
    "\n",
    "    Args:\n",
    "        series: A pandas Series with potential NaN values.\n",
    "\n",
    "    Returns:\n",
    "        A new Series with NaN values replaced by the column median.\n",
    "    \"\"\"\n",
    "    col_median = series.dropna().median()\n",
    "    return series.fillna(col_median)\n",
    "\n",
    "\n",
    "iris_median_imputed = iris_missing.copy()\n",
    "iris_median_imputed[\"sepal length (cm)\"] = impute_median(iris_missing[\"sepal length (cm)\"])\n",
    "iris_median_imputed[\"petal width (cm)\"] = impute_median(iris_missing[\"petal width (cm)\"])\n",
    "\n",
    "print(\"Median-imputed Iris — remaining NaNs:\")\n",
    "print(iris_median_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 3: Mode Imputation\n",
    "\n",
    "Replace missing values with the most frequent value. This is the standard choice for\n",
    "categorical features where mean and median are not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Strategy 3: Mode Imputation (from scratch) ───────────────────────────────\n",
    "\n",
    "\n",
    "def impute_mode(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Impute missing values with the column mode (most frequent value).\n",
    "\n",
    "    Suitable for categorical or discrete features.\n",
    "\n",
    "    Args:\n",
    "        series: A pandas Series with potential NaN values.\n",
    "\n",
    "    Returns:\n",
    "        A new Series with NaN values replaced by the most frequent value.\n",
    "    \"\"\"\n",
    "    value_counts = series.dropna().value_counts()\n",
    "    mode_value = value_counts.index[0]\n",
    "    return series.fillna(mode_value)\n",
    "\n",
    "\n",
    "# Demonstrate mode imputation on a categorical-like column\n",
    "# Inject NaN into species (simulate missing labels)\n",
    "iris_cat_missing = iris_df.copy()\n",
    "cat_missing_idx = np.random.choice(len(iris_cat_missing), size=15, replace=False)\n",
    "iris_cat_missing.loc[cat_missing_idx, \"species\"] = np.nan\n",
    "\n",
    "print(f\"Missing species before mode imputation: {iris_cat_missing['species'].isnull().sum()}\")\n",
    "iris_cat_missing[\"species\"] = impute_mode(iris_cat_missing[\"species\"])\n",
    "print(f\"Missing species after mode imputation:  {iris_cat_missing['species'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 4: Forward-Fill\n",
    "\n",
    "Carry the last observed value forward. This is most useful for time-series or\n",
    "sequentially-ordered data where the previous observation is a reasonable proxy for the\n",
    "missing one. If leading values are NaN, we backfill them from the first non-NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Strategy 4: Forward-Fill (from scratch) ──────────────────────────────────\n",
    "\n",
    "\n",
    "def impute_forward_fill(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Impute missing values using forward-fill (last observation carried forward).\n",
    "\n",
    "    If the first value(s) are NaN, they remain NaN since there is no prior\n",
    "    observation to carry forward. A subsequent backfill handles leading NaNs.\n",
    "\n",
    "    Args:\n",
    "        series: A pandas Series with potential NaN values.\n",
    "\n",
    "    Returns:\n",
    "        A new Series with NaN values replaced by the most recent non-NaN value.\n",
    "    \"\"\"\n",
    "    result = series.copy()\n",
    "    values = result.values.copy()\n",
    "    for idx in range(1, len(values)):\n",
    "        if pd.isna(values[idx]):\n",
    "            values[idx] = values[idx - 1]\n",
    "    # Handle leading NaNs with backfill\n",
    "    for idx in range(len(values) - 2, -1, -1):\n",
    "        if pd.isna(values[idx]):\n",
    "            values[idx] = values[idx + 1]\n",
    "    return pd.Series(values, index=series.index, name=series.name)\n",
    "\n",
    "\n",
    "iris_ffill_imputed = iris_missing.copy()\n",
    "iris_ffill_imputed[\"sepal length (cm)\"] = impute_forward_fill(iris_missing[\"sepal length (cm)\"])\n",
    "iris_ffill_imputed[\"petal width (cm)\"] = impute_forward_fill(iris_missing[\"petal width (cm)\"])\n",
    "\n",
    "print(\"Forward-fill imputed Iris — remaining NaNs:\")\n",
    "print(iris_ffill_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Comparison of Imputation Strategies\n",
    "\n",
    "Overlaying the imputed distributions against the original reveals how each strategy\n",
    "distorts the data. Mean imputation creates a spike at the mean; median imputation\n",
    "creates a spike at the median; forward-fill distributes imputed values more broadly\n",
    "but depends on data ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compare imputed distributions with histograms ────────────────────────────\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "feature = \"sepal length (cm)\"\n",
    "bins = 15\n",
    "\n",
    "axes[0, 0].hist(iris_df[feature], bins=bins, alpha=0.6, label=\"Original\", color=\"steelblue\")\n",
    "axes[0, 0].hist(iris_mean_imputed[feature], bins=bins, alpha=0.4, label=\"Mean\", color=\"orange\")\n",
    "axes[0, 0].set_title(\"Mean Imputation\")\n",
    "axes[0, 0].set_xlabel(feature)\n",
    "axes[0, 0].set_ylabel(\"Count\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].hist(iris_df[feature], bins=bins, alpha=0.6, label=\"Original\", color=\"steelblue\")\n",
    "axes[0, 1].hist(iris_median_imputed[feature], bins=bins, alpha=0.4, label=\"Median\", color=\"green\")\n",
    "axes[0, 1].set_title(\"Median Imputation\")\n",
    "axes[0, 1].set_xlabel(feature)\n",
    "axes[0, 1].set_ylabel(\"Count\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[1, 0].hist(iris_df[feature], bins=bins, alpha=0.6, label=\"Original\", color=\"steelblue\")\n",
    "axes[1, 0].hist(iris_ffill_imputed[feature], bins=bins, alpha=0.4, label=\"Forward-Fill\", color=\"red\")\n",
    "axes[1, 0].set_title(\"Forward-Fill Imputation\")\n",
    "axes[1, 0].set_xlabel(feature)\n",
    "axes[1, 0].set_ylabel(\"Count\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Compare all strategies on a single plot\n",
    "axes[1, 1].hist(iris_df[feature], bins=bins, alpha=0.5, label=\"Original\", color=\"steelblue\")\n",
    "axes[1, 1].hist(iris_mean_imputed[feature], bins=bins, alpha=0.3, label=\"Mean\", color=\"orange\")\n",
    "axes[1, 1].hist(iris_median_imputed[feature], bins=bins, alpha=0.3, label=\"Median\", color=\"green\")\n",
    "axes[1, 1].hist(iris_ffill_imputed[feature], bins=bins, alpha=0.3, label=\"Forward-Fill\", color=\"red\")\n",
    "axes[1, 1].set_title(\"All Strategies Compared\")\n",
    "axes[1, 1].set_xlabel(feature)\n",
    "axes[1, 1].set_ylabel(\"Count\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "fig.suptitle(\"Imputation Strategy Comparison — sepal length (cm)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Categorical Encoding\n",
    "\n",
    "Machine learning models work with numbers, not strings. We need to convert categorical\n",
    "features into numeric representations. Three common strategies:\n",
    "\n",
    "| Encoding | Description | When to use |\n",
    "|----------|-------------|-------------|\n",
    "| **One-hot** | Binary column per category | Nominal categories (no ordering), few unique values |\n",
    "| **Ordinal** | Integer mapping preserving order | Ordered categories (e.g., low/medium/high) |\n",
    "| **Target** | Replace category with mean of target | High cardinality, tree-based models |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── One-Hot Encoding (from scratch with NumPy) ───────────────────────────────\n",
    "\n",
    "\n",
    "def one_hot_encode_scratch(\n",
    "    series: pd.Series,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"One-hot encode a categorical Series using NumPy.\n",
    "\n",
    "    Creates a binary column for each unique category. Values are 0 or 1.\n",
    "\n",
    "    Args:\n",
    "        series: A pandas Series containing categorical values.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with one binary column per unique category.\n",
    "    \"\"\"\n",
    "    categories = sorted(series.dropna().unique())\n",
    "    num_samples = len(series)\n",
    "    num_categories = len(categories)\n",
    "    encoded = np.zeros((num_samples, num_categories), dtype=int)\n",
    "    cat_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    for row_idx, value in enumerate(series):\n",
    "        if pd.notna(value) and value in cat_to_idx:\n",
    "            encoded[row_idx, cat_to_idx[value]] = 1\n",
    "    column_names = [f\"{series.name}_{cat}\" for cat in categories]\n",
    "    return pd.DataFrame(encoded, columns=column_names, index=series.index)\n",
    "\n",
    "\n",
    "# Apply to Iris species\n",
    "iris_onehot_scratch = one_hot_encode_scratch(iris_df[\"species\"])\n",
    "print(\"=== One-Hot Encoding (from scratch) ===\")\n",
    "print(iris_onehot_scratch.head(10))\n",
    "\n",
    "# Compare with pd.get_dummies\n",
    "iris_onehot_pandas = pd.get_dummies(iris_df[\"species\"], prefix=\"species\", dtype=int)\n",
    "print(\"\\n=== One-Hot Encoding (pd.get_dummies) ===\")\n",
    "print(iris_onehot_pandas.head(10))\n",
    "\n",
    "# Verify equivalence\n",
    "assert iris_onehot_scratch.values.sum() == iris_onehot_pandas.values.sum(), \"Mismatch!\"\n",
    "print(f\"\\nTotal 1s match: {iris_onehot_scratch.values.sum()} == {iris_onehot_pandas.values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding\n",
    "\n",
    "Maps each category to an integer based on a specified ordering. Unlike one-hot encoding,\n",
    "ordinal encoding produces a single column — but it introduces an artificial numerical\n",
    "relationship between categories that may not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Ordinal Encoding (from scratch) ──────────────────────────────────────────\n",
    "\n",
    "\n",
    "def ordinal_encode_scratch(\n",
    "    series: pd.Series,\n",
    "    order: list[str],\n",
    ") -> pd.Series:\n",
    "    \"\"\"Ordinal encode a categorical Series given a specific ordering.\n",
    "\n",
    "    Maps each category to an integer based on the specified order.\n",
    "\n",
    "    Args:\n",
    "        series: A pandas Series containing categorical values.\n",
    "        order: Ordered list of categories from lowest to highest.\n",
    "\n",
    "    Returns:\n",
    "        A new Series with integer-encoded values.\n",
    "    \"\"\"\n",
    "    mapping = {cat: idx for idx, cat in enumerate(order)}\n",
    "    return series.map(mapping).rename(f\"{series.name}_ordinal\")\n",
    "\n",
    "\n",
    "# Apply to Iris species (arbitrary order for demonstration)\n",
    "species_order = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "iris_ordinal_scratch = ordinal_encode_scratch(iris_df[\"species\"], species_order)\n",
    "print(\"=== Ordinal Encoding (from scratch) ===\")\n",
    "print(iris_ordinal_scratch.value_counts().sort_index())\n",
    "\n",
    "# Compare with sklearn OrdinalEncoder\n",
    "sklearn_ordinal = OrdinalEncoder(categories=[species_order])\n",
    "iris_ordinal_sklearn = sklearn_ordinal.fit_transform(\n",
    "    iris_df[[\"species\"]].astype(str)\n",
    ").ravel()\n",
    "print(f\"\\nScratch vs sklearn match: {np.allclose(iris_ordinal_scratch.values, iris_ordinal_sklearn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Encoding\n",
    "\n",
    "Replace each category with the mean of the target variable for that category.\n",
    "This produces a single numeric column that captures the category's relationship\n",
    "with the target. **Warning:** target encoding must be computed on training data only —\n",
    "using the full dataset leaks information from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Target Encoding (from scratch) ───────────────────────────────────────────\n",
    "\n",
    "\n",
    "def target_encode_scratch(\n",
    "    series: pd.Series,\n",
    "    target: pd.Series,\n",
    ") -> tuple[pd.Series, dict[str, float]]:\n",
    "    \"\"\"Target encode a categorical Series using the mean of the target per category.\n",
    "\n",
    "    Each category value is replaced by the mean of the target variable for\n",
    "    samples in that category. This must be fitted on training data only\n",
    "    to avoid data leakage.\n",
    "\n",
    "    Args:\n",
    "        series: A pandas Series containing categorical values.\n",
    "        target: A pandas Series containing the numeric target variable.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (encoded Series, mapping dict from category to mean target).\n",
    "    \"\"\"\n",
    "    combined = pd.DataFrame({\"category\": series, \"target\": target})\n",
    "    encoding_map = combined.groupby(\"category\")[\"target\"].mean().to_dict()\n",
    "    encoded = series.map(encoding_map).rename(f\"{series.name}_target_encoded\")\n",
    "    return encoded, encoding_map\n",
    "\n",
    "\n",
    "# Demonstrate on Iris: encode species using sepal length as a proxy target\n",
    "iris_target_encoded, target_map = target_encode_scratch(\n",
    "    iris_df[\"species\"], iris_df[\"sepal length (cm)\"]\n",
    ")\n",
    "print(\"=== Target Encoding Map ===\")\n",
    "for category, mean_val in target_map.items():\n",
    "    print(f\"  {category}: {mean_val:.4f}\")\n",
    "print(f\"\\nEncoded values (first 5): {iris_target_encoded.head().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualize encoding strategies side by side ────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# One-hot: show as heatmap for first 15 samples\n",
    "axes[0].imshow(iris_onehot_scratch.iloc[:15].values, cmap=\"viridis\", aspect=\"auto\")\n",
    "axes[0].set_xticks(range(3))\n",
    "axes[0].set_xticklabels([\"setosa\", \"versicolor\", \"virginica\"], rotation=45, ha=\"right\")\n",
    "axes[0].set_ylabel(\"Sample Index\")\n",
    "axes[0].set_title(\"One-Hot Encoding\")\n",
    "\n",
    "# Ordinal: bar chart of encoded values\n",
    "sample_indices = range(0, 150, 10)\n",
    "axes[1].bar(range(len(sample_indices)), iris_ordinal_scratch.iloc[list(sample_indices)].values,\n",
    "            color=\"steelblue\")\n",
    "axes[1].set_xlabel(\"Sample (every 10th)\")\n",
    "axes[1].set_ylabel(\"Ordinal Value\")\n",
    "axes[1].set_title(\"Ordinal Encoding\")\n",
    "\n",
    "# Target: scatter of encoded values\n",
    "axes[2].scatter(range(len(iris_target_encoded)), iris_target_encoded.values,\n",
    "                c=iris_ordinal_scratch.values, cmap=\"viridis\", alpha=0.6, s=10)\n",
    "axes[2].set_xlabel(\"Sample Index\")\n",
    "axes[2].set_ylabel(\"Target-Encoded Value\")\n",
    "axes[2].set_title(\"Target Encoding\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Merge and Join\n",
    "\n",
    "In practice, data lives in multiple tables. Pandas provides `merge()` for SQL-style\n",
    "joins and `concat()` for stacking DataFrames.\n",
    "\n",
    "| Join Type | Keeps |\n",
    "|-----------|-------|\n",
    "| **Inner** | Only rows with matching keys in both tables |\n",
    "| **Left** | All rows from the left table, matching rows from the right |\n",
    "| **Right** | All rows from the right table, matching rows from the left |\n",
    "| **Outer** | All rows from both tables |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Create related DataFrames for merge demonstration ────────────────────────\n",
    "# Feature table: per-species measurements\n",
    "species_features = pd.DataFrame({\n",
    "    \"species\": [\"setosa\", \"versicolor\", \"virginica\"],\n",
    "    \"avg_sepal_length\": [5.006, 5.936, 6.588],\n",
    "    \"avg_petal_length\": [1.462, 4.260, 5.552],\n",
    "})\n",
    "\n",
    "# Metadata table: additional info (note: \"unknown\" species not in features)\n",
    "species_metadata = pd.DataFrame({\n",
    "    \"species\": [\"setosa\", \"versicolor\", \"virginica\", \"unknown\"],\n",
    "    \"petal_color\": [\"white\", \"purple\", \"violet\", \"N/A\"],\n",
    "    \"native_region\": [\"North America\", \"Eastern North America\", \"Eastern North America\", \"Unknown\"],\n",
    "})\n",
    "\n",
    "print(\"=== Features Table ===\")\n",
    "print(species_features)\n",
    "print(\"\\n=== Metadata Table ===\")\n",
    "print(species_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inner Join: only matching keys ───────────────────────────────────────────\n",
    "inner_merged = pd.merge(species_features, species_metadata, on=\"species\", how=\"inner\")\n",
    "print(\"=== Inner Join ===\")\n",
    "print(inner_merged)\n",
    "print(f\"Rows: {len(inner_merged)} (dropped 'unknown' — no match in features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Left Join: all rows from left table ──────────────────────────────────────\n",
    "left_merged = pd.merge(species_features, species_metadata, on=\"species\", how=\"left\")\n",
    "print(\"=== Left Join ===\")\n",
    "print(left_merged)\n",
    "print(f\"Rows: {len(left_merged)} (keeps all from features, drops unmatched metadata)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Outer Join: all rows from both tables ────────────────────────────────────\n",
    "outer_merged = pd.merge(species_features, species_metadata, on=\"species\", how=\"outer\")\n",
    "print(\"=== Outer Join ===\")\n",
    "print(outer_merged)\n",
    "print(f\"Rows: {len(outer_merged)} (keeps everything, NaN where no match)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation with `pd.concat()`\n",
    "\n",
    "`pd.concat()` stacks DataFrames either vertically (row-wise, `axis=0`) or\n",
    "horizontally (column-wise, `axis=1`). Unlike `merge()`, it does not match on keys —\n",
    "it simply glues DataFrames together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── pd.concat for stacking DataFrames ────────────────────────────────────────\n",
    "iris_first_half = iris_df.iloc[:75]\n",
    "iris_second_half = iris_df.iloc[75:]\n",
    "\n",
    "# Vertical concatenation (stacking rows)\n",
    "iris_recombined = pd.concat([iris_first_half, iris_second_half], axis=0, ignore_index=True)\n",
    "print(f\"Original shape: {iris_df.shape}\")\n",
    "print(f\"Recombined shape: {iris_recombined.shape}\")\n",
    "assert iris_recombined.shape == iris_df.shape, \"Shape mismatch after concat!\"\n",
    "\n",
    "# Horizontal concatenation (adding columns)\n",
    "extra_features = pd.DataFrame({\n",
    "    \"sepal_area\": iris_df[\"sepal length (cm)\"] * iris_df[\"sepal width (cm)\"],\n",
    "    \"petal_area\": iris_df[\"petal length (cm)\"] * iris_df[\"petal width (cm)\"],\n",
    "})\n",
    "iris_extended = pd.concat([iris_df, extra_features], axis=1)\n",
    "print(f\"\\nExtended shape: {iris_extended.shape} (added sepal_area, petal_area)\")\n",
    "iris_extended.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — Putting It All Together\n",
    "\n",
    "We combine the individual operations from Part 1 into two reusable classes:\n",
    "1. **`EDAReport`** — generates a structured exploratory data analysis report.\n",
    "2. **`DataPreprocessor`** — handles missing values and categorical encoding with a\n",
    "   scikit-learn-style `fit()`/`transform()` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDAReport Class\n",
    "\n",
    "The `EDAReport` class takes a DataFrame and provides four methods:\n",
    "- `summary()` — extended descriptive statistics including skewness and kurtosis.\n",
    "- `missing_report()` — counts and percentages of missing values per column.\n",
    "- `correlation_plot()` — renders a correlation heatmap.\n",
    "- `distribution_plots()` — renders histograms for all numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDAReport:\n",
    "    \"\"\"Generate a structured exploratory data analysis report for a DataFrame.\n",
    "\n",
    "    Attributes:\n",
    "        df: The input DataFrame to analyze.\n",
    "        numeric_cols: List of numeric column names.\n",
    "        categorical_cols: List of categorical column names.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Initialize EDAReport with a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: The DataFrame to analyze.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = df.select_dtypes(\n",
    "            include=[\"object\", \"category\"]\n",
    "        ).columns.tolist()\n",
    "\n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate summary statistics for all numeric columns.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with mean, std, min, max, skewness, and kurtosis per column.\n",
    "        \"\"\"\n",
    "        stats = self.df[self.numeric_cols].describe().T\n",
    "        stats[\"skewness\"] = self.df[self.numeric_cols].skew()\n",
    "        stats[\"kurtosis\"] = self.df[self.numeric_cols].kurtosis()\n",
    "        return stats.round(4)\n",
    "\n",
    "    def missing_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate a report of missing values per column.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with count and percentage of missing values per column.\n",
    "        \"\"\"\n",
    "        missing_count = self.df.isnull().sum()\n",
    "        missing_pct = (missing_count / len(self.df)) * 100\n",
    "        report = pd.DataFrame({\n",
    "            \"missing_count\": missing_count,\n",
    "            \"missing_pct\": missing_pct.round(2),\n",
    "        })\n",
    "        return report[report[\"missing_count\"] > 0].sort_values(\n",
    "            \"missing_count\", ascending=False\n",
    "        )\n",
    "\n",
    "    def correlation_plot(self) -> None:\n",
    "        \"\"\"Display a correlation heatmap for all numeric columns.\"\"\"\n",
    "        corr = self.df[self.numeric_cols].corr()\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        cax = ax.matshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticks(range(len(corr.columns)))\n",
    "        ax.set_yticks(range(len(corr.columns)))\n",
    "        ax.set_xticklabels(corr.columns, rotation=45, ha=\"left\", fontsize=8)\n",
    "        ax.set_yticklabels(corr.columns, fontsize=8)\n",
    "        ax.set_title(\"Correlation Heatmap\", pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def distribution_plots(self, max_cols: int = 8) -> None:\n",
    "        \"\"\"Plot histograms for numeric columns.\n",
    "\n",
    "        Args:\n",
    "            max_cols: Maximum number of columns to plot.\n",
    "        \"\"\"\n",
    "        cols_to_plot = self.numeric_cols[:max_cols]\n",
    "        num_plots = len(cols_to_plot)\n",
    "        ncols = min(4, num_plots)\n",
    "        nrows = (num_plots + ncols - 1) // ncols\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 3 * nrows))\n",
    "        if num_plots == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes_flat = axes.flatten()\n",
    "        for idx, col in enumerate(cols_to_plot):\n",
    "            axes_flat[idx].hist(self.df[col].dropna(), bins=20, color=\"steelblue\",\n",
    "                                edgecolor=\"white\", alpha=0.8)\n",
    "            axes_flat[idx].set_title(col, fontsize=9)\n",
    "            axes_flat[idx].set_xlabel(\"Value\")\n",
    "            axes_flat[idx].set_ylabel(\"Count\")\n",
    "        # Hide unused subplots\n",
    "        for idx in range(num_plots, len(axes_flat)):\n",
    "            axes_flat[idx].set_visible(False)\n",
    "        fig.suptitle(\"Feature Distributions\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPreprocessor Class\n",
    "\n",
    "The `DataPreprocessor` follows the scikit-learn convention:\n",
    "- **`fit(df)`** — learns imputation values (means or medians) and encoding maps from training data.\n",
    "- **`transform(df)`** — applies the learned parameters to any DataFrame (train or test).\n",
    "\n",
    "This separation ensures we never leak test-set information into the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Preprocess tabular data with missing value imputation and categorical encoding.\n",
    "\n",
    "    Follows the scikit-learn fit/transform pattern: fit() learns parameters\n",
    "    from training data, transform() applies them to any data split.\n",
    "\n",
    "    Attributes:\n",
    "        impute_strategy: Strategy for numeric imputation ('mean' or 'median').\n",
    "        encode_strategy: Strategy for categorical encoding ('onehot' or 'ordinal').\n",
    "        numeric_fill_values: Dict mapping column name to imputation value (learned in fit).\n",
    "        categorical_maps: Dict mapping column name to encoding map (learned in fit).\n",
    "        fitted: Whether fit() has been called.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        impute_strategy: str = \"mean\",\n",
    "        encode_strategy: str = \"onehot\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the DataPreprocessor.\n",
    "\n",
    "        Args:\n",
    "            impute_strategy: Strategy for imputing numeric missing values.\n",
    "                One of 'mean' or 'median'.\n",
    "            encode_strategy: Strategy for encoding categorical columns.\n",
    "                One of 'onehot' or 'ordinal'.\n",
    "        \"\"\"\n",
    "        self.impute_strategy = impute_strategy\n",
    "        self.encode_strategy = encode_strategy\n",
    "        self.numeric_fill_values: dict[str, float] = {}\n",
    "        self.categorical_maps: dict[str, dict] = {}\n",
    "        self.fitted: bool = False\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> \"DataPreprocessor\":\n",
    "        \"\"\"Learn imputation values and encoding maps from the training data.\n",
    "\n",
    "        Args:\n",
    "            df: Training DataFrame to learn parameters from.\n",
    "\n",
    "        Returns:\n",
    "            Self, for method chaining.\n",
    "        \"\"\"\n",
    "        # Learn numeric imputation values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if df[col].isnull().any():\n",
    "                if self.impute_strategy == \"mean\":\n",
    "                    self.numeric_fill_values[col] = df[col].mean()\n",
    "                elif self.impute_strategy == \"median\":\n",
    "                    self.numeric_fill_values[col] = df[col].median()\n",
    "\n",
    "        # Learn categorical encoding maps\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        for col in categorical_cols:\n",
    "            unique_vals = sorted(df[col].dropna().unique())\n",
    "            if self.encode_strategy == \"onehot\":\n",
    "                self.categorical_maps[col] = {\n",
    "                    val: idx for idx, val in enumerate(unique_vals)\n",
    "                }\n",
    "            elif self.encode_strategy == \"ordinal\":\n",
    "                self.categorical_maps[col] = {\n",
    "                    val: idx for idx, val in enumerate(unique_vals)\n",
    "                }\n",
    "\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply learned imputation and encoding to a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to transform. Must have the same columns as the\n",
    "                DataFrame used in fit().\n",
    "\n",
    "        Returns:\n",
    "            Transformed DataFrame with imputed values and encoded categoricals.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If transform() is called before fit().\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() before transform().\")\n",
    "\n",
    "        result = df.copy()\n",
    "\n",
    "        # Apply numeric imputation\n",
    "        for col, fill_value in self.numeric_fill_values.items():\n",
    "            if col in result.columns:\n",
    "                result[col] = result[col].fillna(fill_value)\n",
    "\n",
    "        # Apply categorical encoding\n",
    "        encoded_frames = []\n",
    "        cols_to_drop = []\n",
    "        for col, mapping in self.categorical_maps.items():\n",
    "            if col not in result.columns:\n",
    "                continue\n",
    "            if self.encode_strategy == \"onehot\":\n",
    "                num_categories = len(mapping)\n",
    "                encoded_array = np.zeros((len(result), num_categories), dtype=int)\n",
    "                for row_idx, value in enumerate(result[col]):\n",
    "                    if pd.notna(value) and value in mapping:\n",
    "                        encoded_array[row_idx, mapping[value]] = 1\n",
    "                col_names = [f\"{col}_{cat}\" for cat in mapping.keys()]\n",
    "                encoded_frames.append(\n",
    "                    pd.DataFrame(encoded_array, columns=col_names, index=result.index)\n",
    "                )\n",
    "                cols_to_drop.append(col)\n",
    "            elif self.encode_strategy == \"ordinal\":\n",
    "                result[col] = result[col].map(mapping)\n",
    "\n",
    "        # Drop original categorical columns and add one-hot columns\n",
    "        if cols_to_drop:\n",
    "            result = result.drop(columns=cols_to_drop)\n",
    "        if encoded_frames:\n",
    "            result = pd.concat([result] + encoded_frames, axis=1)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "\n",
    "Before applying our classes to real data, we verify them on a small toy DataFrame\n",
    "with known missing values and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sanity Check on a toy DataFrame ──────────────────────────────────────────\n",
    "toy_df = pd.DataFrame({\n",
    "    \"height\": [170.0, np.nan, 165.0, 180.0, np.nan],\n",
    "    \"weight\": [65.0, 70.0, np.nan, 80.0, 75.0],\n",
    "    \"color\": [\"red\", \"blue\", \"red\", \"green\", \"blue\"],\n",
    "})\n",
    "print(\"=== Toy DataFrame (before) ===\")\n",
    "print(toy_df)\n",
    "\n",
    "preprocessor = DataPreprocessor(impute_strategy=\"mean\", encode_strategy=\"onehot\")\n",
    "preprocessor.fit(toy_df)\n",
    "toy_transformed = preprocessor.transform(toy_df)\n",
    "\n",
    "print(\"\\n=== Toy DataFrame (after transform) ===\")\n",
    "print(toy_transformed)\n",
    "\n",
    "# Verify: no missing values remain\n",
    "assert toy_transformed.isnull().sum().sum() == 0, \"Missing values remain!\"\n",
    "print(f\"\\nNo missing values: {toy_transformed.isnull().sum().sum() == 0}\")\n",
    "print(f\"Learned fill values: {preprocessor.numeric_fill_values}\")\n",
    "print(f\"Learned encoding maps: {preprocessor.categorical_maps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — Application on Real Data\n",
    "\n",
    "We apply our `EDAReport` and `DataPreprocessor` classes to the California Housing\n",
    "dataset, then compare our from-scratch preprocessing against an equivalent sklearn\n",
    "pipeline. We also demonstrate the critical principle of fitting on training data\n",
    "only, then transforming test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── EDAReport on California Housing ──────────────────────────────────────────\n",
    "housing_report = EDAReport(housing_missing)\n",
    "\n",
    "print(\"=== Summary Statistics ===\")\n",
    "housing_report.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Missing Values Report ===\")\n",
    "housing_report.missing_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_report.correlation_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_report.distribution_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split and Preprocessing\n",
    "\n",
    "A critical rule: **always fit preprocessors on training data only, then transform both\n",
    "train and test**. If we compute the mean from the full dataset (including test), the\n",
    "imputed values in the training set would be contaminated by test-set information —\n",
    "this is a form of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Train/Test Split (fit on train, transform on test) ───────────────────────\n",
    "# Separate features and target\n",
    "housing_features = housing_missing.drop(columns=[\"MedHouseVal\"])\n",
    "housing_target = housing_missing[\"MedHouseVal\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    housing_features, housing_target, test_size=0.2, random_state=SEED\n",
    ")\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape:  {X_test.shape}\")\n",
    "print(f\"\\nTrain NaNs:\\n{X_train.isnull().sum()[X_train.isnull().sum() > 0]}\")\n",
    "print(f\"\\nTest NaNs:\\n{X_test.isnull().sum()[X_test.isnull().sum() > 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── From-Scratch Preprocessing ───────────────────────────────────────────────\n",
    "preprocessor_scratch = DataPreprocessor(impute_strategy=\"mean\", encode_strategy=\"onehot\")\n",
    "preprocessor_scratch.fit(X_train)  # Fit on training data ONLY\n",
    "\n",
    "X_train_scratch = preprocessor_scratch.transform(X_train)\n",
    "X_test_scratch = preprocessor_scratch.transform(X_test)\n",
    "\n",
    "print(\"=== From-Scratch Preprocessing ===\")\n",
    "print(f\"Train NaNs after transform: {X_train_scratch.isnull().sum().sum()}\")\n",
    "print(f\"Test NaNs after transform:  {X_test_scratch.isnull().sum().sum()}\")\n",
    "print(f\"Train shape: {X_train_scratch.shape}\")\n",
    "print(f\"Test shape:  {X_test_scratch.shape}\")\n",
    "print(f\"\\nLearned fill values:\")\n",
    "for col, val in preprocessor_scratch.numeric_fill_values.items():\n",
    "    print(f\"  {col}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Comparison\n",
    "\n",
    "We build an equivalent sklearn pipeline using `SimpleImputer` with the `mean` strategy\n",
    "and verify that our from-scratch implementation produces numerically identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── sklearn Pipeline Comparison ──────────────────────────────────────────────\n",
    "# Build an equivalent sklearn pipeline with SimpleImputer (mean strategy)\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "sklearn_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "])\n",
    "\n",
    "X_train_sklearn = sklearn_pipeline.fit_transform(X_train[numeric_cols])\n",
    "X_test_sklearn = sklearn_pipeline.transform(X_test[numeric_cols])\n",
    "\n",
    "# Convert to DataFrame for comparison\n",
    "X_train_sklearn_df = pd.DataFrame(X_train_sklearn, columns=numeric_cols, index=X_train.index)\n",
    "X_test_sklearn_df = pd.DataFrame(X_test_sklearn, columns=numeric_cols, index=X_test.index)\n",
    "\n",
    "print(\"=== sklearn Pipeline Preprocessing ===\")\n",
    "print(f\"Train NaNs after transform: {np.isnan(X_train_sklearn).sum()}\")\n",
    "print(f\"Test NaNs after transform:  {np.isnan(X_test_sklearn).sum()}\")\n",
    "print(f\"Train shape: {X_train_sklearn.shape}\")\n",
    "print(f\"Test shape:  {X_test_sklearn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Verify Numerical Equivalence ─────────────────────────────────────────────\n",
    "\n",
    "\n",
    "def compare_preprocessing_results(\n",
    "    scratch_df: pd.DataFrame,\n",
    "    sklearn_df: pd.DataFrame,\n",
    "    label: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare from-scratch and sklearn preprocessing results column by column.\n",
    "\n",
    "    Args:\n",
    "        scratch_df: DataFrame from the from-scratch preprocessor.\n",
    "        sklearn_df: DataFrame from the sklearn pipeline.\n",
    "        label: Label for the comparison (e.g., 'Train' or 'Test').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame showing max absolute difference per column.\n",
    "    \"\"\"\n",
    "    common_cols = [col for col in scratch_df.columns if col in sklearn_df.columns]\n",
    "    diffs = []\n",
    "    for col in common_cols:\n",
    "        max_diff = np.abs(\n",
    "            scratch_df[col].values - sklearn_df[col].values\n",
    "        ).max()\n",
    "        diffs.append({\"column\": col, \"max_abs_diff\": max_diff})\n",
    "    result = pd.DataFrame(diffs)\n",
    "    result[\"match\"] = result[\"max_abs_diff\"] < 1e-10\n",
    "    return result\n",
    "\n",
    "\n",
    "train_comparison = compare_preprocessing_results(\n",
    "    X_train_scratch[numeric_cols], X_train_sklearn_df, \"Train\"\n",
    ")\n",
    "test_comparison = compare_preprocessing_results(\n",
    "    X_test_scratch[numeric_cols], X_test_sklearn_df, \"Test\"\n",
    ")\n",
    "\n",
    "print(\"=== Train Set — From-Scratch vs sklearn ===\")\n",
    "print(train_comparison.to_string(index=False))\n",
    "print(f\"\\nAll columns match: {train_comparison['match'].all()}\")\n",
    "\n",
    "print(\"\\n=== Test Set — From-Scratch vs sklearn ===\")\n",
    "print(test_comparison.to_string(index=False))\n",
    "print(f\"\\nAll columns match: {test_comparison['match'].all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 — Evaluation & Analysis\n",
    "\n",
    "We compare imputation strategies to see which preserves the original data\n",
    "distribution best, analyze encoding strategies for their impact on\n",
    "feature dimensionality and sparsity, and produce a summary comparison table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Strategy Analysis\n",
    "\n",
    "A good imputation strategy preserves the original distribution as closely as possible.\n",
    "We measure the shift in mean, standard deviation, and skewness introduced by each strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compare Imputation Strategies: Distribution Preservation ──────────────────\n",
    "\n",
    "\n",
    "def compute_distribution_shift(\n",
    "    original: pd.Series,\n",
    "    imputed: pd.Series,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute statistics measuring how imputation shifted the distribution.\n",
    "\n",
    "    Args:\n",
    "        original: The original Series (no missing values).\n",
    "        imputed: The imputed Series (missing values filled).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with mean_diff, std_diff, and skew_diff.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mean_diff\": abs(original.mean() - imputed.mean()),\n",
    "        \"std_diff\": abs(original.std() - imputed.std()),\n",
    "        \"skew_diff\": abs(original.skew() - imputed.skew()),\n",
    "    }\n",
    "\n",
    "\n",
    "feature_name = \"sepal length (cm)\"\n",
    "original_series = iris_df[feature_name]\n",
    "\n",
    "strategies = {\n",
    "    \"Mean\": iris_mean_imputed[feature_name],\n",
    "    \"Median\": iris_median_imputed[feature_name],\n",
    "    \"Forward-Fill\": iris_ffill_imputed[feature_name],\n",
    "}\n",
    "\n",
    "shift_results = []\n",
    "for strategy_name, imputed_series in strategies.items():\n",
    "    shift = compute_distribution_shift(original_series, imputed_series)\n",
    "    shift[\"strategy\"] = strategy_name\n",
    "    shift_results.append(shift)\n",
    "\n",
    "shift_df = pd.DataFrame(shift_results)[[\"strategy\", \"mean_diff\", \"std_diff\", \"skew_diff\"]]\n",
    "print(\"=== Distribution Shift by Imputation Strategy ===\")\n",
    "print(shift_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Histogram overlay: all strategies vs original ─────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for idx, (strategy_name, imputed_series) in enumerate(strategies.items()):\n",
    "    axes[idx].hist(original_series, bins=15, alpha=0.6, label=\"Original\",\n",
    "                   color=\"steelblue\", edgecolor=\"white\")\n",
    "    axes[idx].hist(imputed_series, bins=15, alpha=0.4, label=strategy_name,\n",
    "                   color=\"orange\", edgecolor=\"white\")\n",
    "    axes[idx].set_title(f\"{strategy_name} vs Original\")\n",
    "    axes[idx].set_xlabel(feature_name)\n",
    "    axes[idx].set_ylabel(\"Count\")\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Strategy Analysis\n",
    "\n",
    "Different encoding strategies produce different output shapes and sparsity levels.\n",
    "One-hot encoding increases dimensionality (one new column per category) but avoids\n",
    "imposing artificial ordering. Ordinal and target encoding maintain a single column\n",
    "but introduce different assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compare Encoding Strategies: Dimensionality & Sparsity ────────────────────\n",
    "\n",
    "\n",
    "def analyze_encoding(\n",
    "    name: str,\n",
    "    encoded_data: np.ndarray,\n",
    "    num_original_features: int,\n",
    ") -> dict[str, float | int | str]:\n",
    "    \"\"\"Analyze the dimensionality and sparsity of an encoding.\n",
    "\n",
    "    Args:\n",
    "        name: Name of the encoding strategy.\n",
    "        encoded_data: The encoded data as a NumPy array.\n",
    "        num_original_features: Number of features before encoding.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with encoding name, dimensions, and sparsity metrics.\n",
    "    \"\"\"\n",
    "    total_elements = encoded_data.size\n",
    "    zero_elements = np.sum(encoded_data == 0)\n",
    "    sparsity = zero_elements / total_elements\n",
    "    return {\n",
    "        \"encoding\": name,\n",
    "        \"original_features\": num_original_features,\n",
    "        \"encoded_features\": encoded_data.shape[1],\n",
    "        \"dimensionality_increase\": encoded_data.shape[1] - num_original_features,\n",
    "        \"sparsity_pct\": round(sparsity * 100, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare encoded versions of Iris (numeric features + encoded species)\n",
    "iris_numeric_features = iris_df[iris_df.select_dtypes(include=[np.number]).columns]\n",
    "num_original = iris_numeric_features.shape[1]  # 4 numeric + 1 categorical = 5 total features\n",
    "\n",
    "# One-hot encoded\n",
    "onehot_full = pd.concat([iris_numeric_features, iris_onehot_scratch], axis=1)\n",
    "# Ordinal encoded\n",
    "ordinal_full = iris_numeric_features.copy()\n",
    "ordinal_full[\"species_ordinal\"] = iris_ordinal_scratch.values\n",
    "# Target encoded\n",
    "target_full = iris_numeric_features.copy()\n",
    "target_full[\"species_target\"] = iris_target_encoded.values\n",
    "\n",
    "encoding_analysis = pd.DataFrame([\n",
    "    analyze_encoding(\"One-Hot\", onehot_full.values, num_original + 1),\n",
    "    analyze_encoding(\"Ordinal\", ordinal_full.values, num_original + 1),\n",
    "    analyze_encoding(\"Target\", target_full.values, num_original + 1),\n",
    "])\n",
    "\n",
    "print(\"=== Encoding Strategy Comparison ===\")\n",
    "encoding_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Correlation heatmap of processed features ────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for idx, (name, df_encoded) in enumerate([\n",
    "    (\"One-Hot\", onehot_full),\n",
    "    (\"Ordinal\", ordinal_full),\n",
    "    (\"Target\", target_full),\n",
    "]):\n",
    "    corr = df_encoded.corr()\n",
    "    cax = axes[idx].matshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    axes[idx].set_title(f\"{name} Encoding\", pad=15)\n",
    "    axes[idx].set_xticks(range(len(corr.columns)))\n",
    "    axes[idx].set_yticks(range(len(corr.columns)))\n",
    "    axes[idx].set_xticklabels(corr.columns, rotation=90, fontsize=6)\n",
    "    axes[idx].set_yticklabels(corr.columns, fontsize=6)\n",
    "\n",
    "fig.colorbar(cax, ax=axes, shrink=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Comparison Table\n",
    "\n",
    "The table below consolidates all preprocessing approaches covered in this notebook,\n",
    "including their best use cases, distribution preservation characteristics, and\n",
    "computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Summary Table: All Preprocessing Approaches ──────────────────────────────\n",
    "summary_data = [\n",
    "    {\n",
    "        \"approach\": \"Mean Imputation\",\n",
    "        \"type\": \"Imputation\",\n",
    "        \"best_for\": \"Symmetric continuous features\",\n",
    "        \"preserves_distribution\": \"Moderately (inflates center)\",\n",
    "        \"complexity\": \"O(n)\",\n",
    "    },\n",
    "    {\n",
    "        \"approach\": \"Median Imputation\",\n",
    "        \"type\": \"Imputation\",\n",
    "        \"best_for\": \"Skewed continuous features\",\n",
    "        \"preserves_distribution\": \"Better (robust to outliers)\",\n",
    "        \"complexity\": \"O(n log n)\",\n",
    "    },\n",
    "    {\n",
    "        \"approach\": \"Mode Imputation\",\n",
    "        \"type\": \"Imputation\",\n",
    "        \"best_for\": \"Categorical features\",\n",
    "        \"preserves_distribution\": \"Inflates most common class\",\n",
    "        \"complexity\": \"O(n)\",\n",
    "    },\n",
    "    {\n",
    "        \"approach\": \"Forward-Fill\",\n",
    "        \"type\": \"Imputation\",\n",
    "        \"best_for\": \"Time-ordered data\",\n",
    "        \"preserves_distribution\": \"Depends on ordering\",\n",
    "        \"complexity\": \"O(n)\",\n",
    "    },\n",
    "    {\n",
    "        \"approach\": \"One-Hot Encoding\",\n",
    "        \"type\": \"Encoding\",\n",
    "        \"best_for\": \"Nominal categories, few unique values\",\n",
    "        \"preserves_distribution\": \"N/A\",\n",
    "        \"complexity\": \"O(n * k)\",\n",
    "    },\n",
    "    {\n",
    "        \"approach\": \"Ordinal Encoding\",\n",
    "        \"type\": \"Encoding\",\n",
    "        \"best_for\": \"Ordered categories\",\n",
    "        \"preserves_distribution\": \"N/A\",\n",
    "        \"complexity\": \"O(n)\",\n",
    "    },\n",
    "    {\n",
    "        \"approach\": \"Target Encoding\",\n",
    "        \"type\": \"Encoding\",\n",
    "        \"best_for\": \"High cardinality, tree models\",\n",
    "        \"preserves_distribution\": \"N/A (risk of data leakage)\",\n",
    "        \"complexity\": \"O(n)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"=== Preprocessing Approaches — Summary ===\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5 — Summary & Lessons Learned\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always explore data before modeling** — `.describe()`, `.value_counts()`, and `.corr()` reveal dataset characteristics such as class balance, feature ranges, and correlations that inform all downstream decisions.\n",
    "\n",
    "2. **Missing value strategy depends on the data:** mean imputation works for roughly symmetric continuous features, median is robust for skewed distributions, and mode is standard for categorical columns. Forward-fill is appropriate only for time-ordered data.\n",
    "\n",
    "3. **One-hot encoding is safe but increases dimensionality;** ordinal encoding is compact but implies ordering; target encoding is compact but risks data leakage if not computed strictly on training data.\n",
    "\n",
    "4. **Always fit preprocessors on training data only, then transform test data** — this prevents data leakage and ensures the model sees genuinely unseen data during evaluation.\n",
    "\n",
    "5. **Pandas operations are vectorized and fast** — avoid iterating rows with `for` loops. Use `.apply()`, `.groupby()`, and vectorized arithmetic instead.\n",
    "\n",
    "### What's Next\n",
    "\n",
    "\\u2192 **1-04 (Visualization with Matplotlib)** teaches the plotting skills to create publication-quality EDA visualizations. These Pandas skills are used extensively in **Module 2 (Supervised Learning)** and **Module 4 (ML Theory & Evaluation)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}