{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations",
    "## 01-08: Information Theory for ML",
    "",
    "**Objective:** Build the information-theoretic foundations that underpin",
    "loss functions, model comparison, and representation learning — entropy,",
    "cross-entropy, KL divergence, and mutual information — from scratch with",
    "numerical experiments and visual intuition.",
    "",
    "**Prerequisites:** 01-01 (Python, NumPy & Tensor Speed), 01-07 (Probability & Statistics for ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 0 — Setup & Prerequisites",
    "",
    "Information theory, pioneered by Claude Shannon in 1948, provides the",
    "mathematical language for quantifying uncertainty, surprise, and the",
    "amount of information in data. In machine learning, information-theoretic",
    "concepts appear everywhere:",
    "",
    "- **Cross-entropy** is the standard classification loss function",
    "- **KL divergence** measures how one distribution differs from another (VAEs, knowledge distillation)",
    "- **Mutual information** quantifies feature relevance and representation quality",
    "- **Entropy** sets limits on compression and optimal coding",
    "",
    "This notebook covers:",
    "- **Shannon entropy** — measuring uncertainty in a random variable",
    "- **Cross-entropy** — the loss function connection",
    "- **KL divergence** — distance between distributions",
    "- **Mutual information** — shared information between variables",
    "- **Conditional entropy and chain rule** — decomposing information",
    "- **Jensen–Shannon divergence** — a symmetric, bounded alternative to KL",
    "",
    "**Prerequisites:** 01-01, 01-07 (Probability & Statistics for ML)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy import stats as sp_stats\n",
    "\n",
    "from sklearn.datasets import load_digits, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA: {torch.version.cuda}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "import random\n",
    "\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "FIGSIZE = (10, 6)\n",
    "COLORS = {\n",
    "    'blue': '#1E88E5',\n",
    "    'red': '#E53935',\n",
    "    'green': '#43A047',\n",
    "    'orange': '#FF9800',\n",
    "    'purple': '#9C27B0',\n",
    "    'teal': '#00897B',\n",
    "    'gray': '#757575',\n",
    "}\n",
    "COLOR_LIST = list(COLORS.values())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading",
    "",
    "We use the Digits dataset (10-class classification) and synthetic",
    "distributions to demonstrate information-theoretic concepts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Digits dataset for classification-related demonstrations\n",
    "digits = load_digits()\n",
    "X_digits = digits.data.astype(np.float64)\n",
    "y_digits = digits.target\n",
    "n_classes = len(np.unique(y_digits))\n",
    "print(f'Digits dataset: {X_digits.shape}')\n",
    "print(f'Classes: {n_classes}, samples per class:')\n",
    "class_counts = np.bincount(y_digits)\n",
    "for c in range(n_classes):\n",
    "    print(f'  Digit {c}: {class_counts[c]} samples')\n",
    "\n",
    "# Show sample digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(digits.images[i * 18], cmap='gray_r')\n",
    "    ax.set_title(f'Label: {y_digits[i * 18]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Digits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 1 — Information Theory from Scratch",
    "",
    "We build up from the notion of surprise (information content) to entropy,",
    "cross-entropy, KL divergence, and mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Information Content and Shannon Entropy",
    "",
    "The **information content** (surprise) of an event with probability $p$ is:",
    "",
    "$$I(x) = -\\log_2 p(x)$$",
    "",
    "Rare events carry more information (more surprise). The **Shannon entropy**",
    "is the expected information content:",
    "",
    "$$H(X) = -\\sum_{x} p(x) \\log_2 p(x) = \\mathbb{E}[-\\log_2 p(X)]$$",
    "",
    "Key properties:",
    "- $H(X) \\geq 0$ — entropy is non-negative",
    "- $H(X) = 0$ iff $X$ is deterministic",
    "- $H(X) \\leq \\log_2 K$ for $K$ outcomes — maximum when uniform",
    "- Uses $0 \\cdot \\log_2(0) \\triangleq 0$ by continuity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def information_content(p: np.ndarray, base: float = 2.0) -> np.ndarray:\n",
    "    \"\"\"Compute information content (surprise) of events.\n",
    "\n",
    "    Args:\n",
    "        p: Probability of each event.\n",
    "        base: Logarithm base (2 for bits, e for nats).\n",
    "\n",
    "    Returns:\n",
    "        Information content for each event.\n",
    "    \"\"\"\n",
    "    return -np.log(p) / np.log(base)\n",
    "\n",
    "\n",
    "def shannon_entropy(p: np.ndarray, base: float = 2.0) -> float:\n",
    "    \"\"\"Compute Shannon entropy of a discrete distribution.\n",
    "\n",
    "    Args:\n",
    "        p: Probability vector (must sum to 1).\n",
    "        base: Logarithm base (2 for bits, e for nats).\n",
    "\n",
    "    Returns:\n",
    "        Entropy value.\n",
    "    \"\"\"\n",
    "    # Filter out zero probabilities\n",
    "    p_nonzero = p[p > 0]\n",
    "    return -np.sum(p_nonzero * np.log(p_nonzero) / np.log(base))\n",
    "\n",
    "\n",
    "# Demonstrate: information content vs probability\n",
    "probs = np.linspace(0.001, 1.0, 500)\n",
    "info = information_content(probs, base=2.0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Information content curve\n",
    "axes[0].plot(probs, info, color=COLORS['blue'], linewidth=2)\n",
    "axes[0].set_xlabel('Probability p(x)', fontsize=12)\n",
    "axes[0].set_ylabel('Information Content (bits)', fontsize=12)\n",
    "axes[0].set_title('$I(x) = -\\log_2 p(x)$: Rare Events = More Surprise')\n",
    "# Mark specific probabilities\n",
    "for p_val, label in [(0.5, 'Fair coin'), (0.1, '10% event'), (0.01, '1% event')]:\n",
    "    i_val = information_content(np.array([p_val]))[0]\n",
    "    axes[0].plot(p_val, i_val, 'o', markersize=8)\n",
    "    axes[0].annotate(f'{label}\\n{i_val:.2f} bits',\n",
    "                      xy=(p_val, i_val), xytext=(p_val + 0.08, i_val + 0.5),\n",
    "                      fontsize=9, arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "# Entropy of a binary variable\n",
    "p_binary = np.linspace(0.001, 0.999, 500)\n",
    "entropy_binary = np.array(\n",
    "    [shannon_entropy(np.array([p, 1 - p]), base=2.0) for p in p_binary]\n",
    ")\n",
    "axes[1].plot(p_binary, entropy_binary, color=COLORS['red'], linewidth=2)\n",
    "axes[1].set_xlabel('p (probability of heads)', fontsize=12)\n",
    "axes[1].set_ylabel('H(X) (bits)', fontsize=12)\n",
    "axes[1].set_title('Binary Entropy Function')\n",
    "axes[1].axvline(0.5, color=COLORS['gray'], linestyle='--', alpha=0.5,\n",
    "                 label='Max entropy at p=0.5')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify: maximum entropy for fair coin\n",
    "print(f'Binary entropy at p=0.5: {shannon_entropy(np.array([0.5, 0.5])):.4f} bits')\n",
    "print(f'Binary entropy at p=0.9: {shannon_entropy(np.array([0.9, 0.1])):.4f} bits')\n",
    "print(f'Binary entropy at p=1.0: {shannon_entropy(np.array([1.0, 0.0])):.4f} bits')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Entropy of Multi-Class Distributions",
    "",
    "For a $K$-class distribution, entropy measures how \"spread out\" the",
    "probability mass is. A uniform distribution has maximum entropy",
    "$\\log_2 K$, while a peaked distribution has low entropy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def entropy_vs_concentration() -> None:\n",
    "    \"\"\"Visualize how entropy changes with distribution shape.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    n_classes_demo = 8\n",
    "\n",
    "    # Create distributions from uniform to peaked\n",
    "    temperatures = [2.0, 1.0, 0.5, 0.1, 0.01]\n",
    "    logits = np.random.RandomState(SEED).randn(n_classes_demo)\n",
    "\n",
    "    for temp, color in zip(temperatures, COLOR_LIST[:5]):\n",
    "        probs = np.exp(logits / temp) / np.sum(np.exp(logits / temp))\n",
    "        h = shannon_entropy(probs)\n",
    "        axes[0].bar(np.arange(n_classes_demo) + (temp - 1.0) * 0.08,\n",
    "                     probs, width=0.15, alpha=0.7, color=color,\n",
    "                     label=f'T={temp}, H={h:.2f}')\n",
    "\n",
    "    axes[0].set_xlabel('Class')\n",
    "    axes[0].set_ylabel('Probability')\n",
    "    axes[0].set_title('Softmax at Different Temperatures')\n",
    "    axes[0].legend(fontsize=8)\n",
    "\n",
    "    # Entropy vs temperature (smooth curve)\n",
    "    temp_range = np.linspace(0.01, 5.0, 200)\n",
    "    entropies_temp = []\n",
    "    for t in temp_range:\n",
    "        p = np.exp(logits / t) / np.sum(np.exp(logits / t))\n",
    "        entropies_temp.append(shannon_entropy(p))\n",
    "    axes[1].plot(temp_range, entropies_temp, color=COLORS['blue'], linewidth=2)\n",
    "    axes[1].axhline(np.log2(n_classes_demo), color=COLORS['red'], linestyle='--',\n",
    "                     label=f'Max entropy = log₂({n_classes_demo}) = {np.log2(n_classes_demo):.2f}')\n",
    "    axes[1].set_xlabel('Temperature')\n",
    "    axes[1].set_ylabel('Entropy (bits)')\n",
    "    axes[1].set_title('Entropy vs Temperature')\n",
    "    axes[1].legend(fontsize=9)\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    # Entropy of real digit class distribution\n",
    "    digit_probs = class_counts / class_counts.sum()\n",
    "    digit_entropy = shannon_entropy(digit_probs)\n",
    "    uniform_entropy = np.log2(n_classes)\n",
    "\n",
    "    axes[2].bar(range(n_classes), digit_probs, color=COLORS['green'], alpha=0.7,\n",
    "                 edgecolor='white')\n",
    "    axes[2].set_xlabel('Digit Class')\n",
    "    axes[2].set_ylabel('Proportion')\n",
    "    axes[2].set_title(f'Digits Distribution\\nH={digit_entropy:.4f} bits '\n",
    "                       f'(max={uniform_entropy:.4f})')\n",
    "    axes[2].grid(True, alpha=0.2, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Digits class entropy: {digit_entropy:.4f} bits')\n",
    "    print(f'Maximum possible:     {uniform_entropy:.4f} bits')\n",
    "    print(f'Efficiency:           {digit_entropy / uniform_entropy:.2%}')\n",
    "\n",
    "\n",
    "entropy_vs_concentration()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross-Entropy",
    "",
    "The **cross-entropy** between a true distribution $p$ and a model",
    "distribution $q$ is:",
    "",
    "$$H(p, q) = -\\sum_{x} p(x) \\log q(x)$$",
    "",
    "Key insight: **Cross-entropy = Entropy + KL divergence**",
    "",
    "$$H(p, q) = H(p) + D_{\\text{KL}}(p \\| q)$$",
    "",
    "Since $H(p)$ is constant w.r.t. model parameters, minimizing",
    "cross-entropy $\\equiv$ minimizing KL divergence $\\equiv$ making $q$",
    "match $p$.",
    "",
    "**ML connection:** When we train a classifier with cross-entropy loss,",
    "we're finding the $q$ (model predictions) closest to $p$ (true labels)",
    "in the KL sense."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def cross_entropy(\n",
    "    p: np.ndarray,\n",
    "    q: np.ndarray,\n",
    "    base: float = 2.0,\n",
    ") -> float:\n",
    "    \"\"\"Compute cross-entropy H(p, q).\n",
    "\n",
    "    Args:\n",
    "        p: True distribution.\n",
    "        q: Model distribution.\n",
    "        base: Logarithm base.\n",
    "\n",
    "    Returns:\n",
    "        Cross-entropy value.\n",
    "    \"\"\"\n",
    "    eps = 1e-15\n",
    "    q = np.clip(q, eps, 1.0)\n",
    "    mask = p > 0\n",
    "    return -np.sum(p[mask] * np.log(q[mask]) / np.log(base))\n",
    "\n",
    "\n",
    "def categorical_cross_entropy_loss(\n",
    "    labels: np.ndarray,\n",
    "    logits: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"Compute categorical cross-entropy loss (nats) from logits.\n",
    "\n",
    "    This is equivalent to PyTorch's nn.CrossEntropyLoss.\n",
    "    Applies softmax internally for numerical stability.\n",
    "\n",
    "    Args:\n",
    "        labels: Integer class labels of shape (n,).\n",
    "        logits: Raw model outputs of shape (n, K).\n",
    "\n",
    "    Returns:\n",
    "        Average cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Log-softmax (numerically stable)\n",
    "    max_logits = logits.max(axis=1, keepdims=True)\n",
    "    shifted = logits - max_logits\n",
    "    log_sum_exp = np.log(np.sum(np.exp(shifted), axis=1, keepdims=True))\n",
    "    log_probs = shifted - log_sum_exp\n",
    "\n",
    "    # Pick log-probability of correct class\n",
    "    n = len(labels)\n",
    "    nll = -log_probs[np.arange(n), labels]\n",
    "    return nll.mean()\n",
    "\n",
    "\n",
    "# Demonstrate cross-entropy behavior\n",
    "true_dist = np.array([0.7, 0.2, 0.1])\n",
    "model_dists = {\n",
    "    'Perfect match': np.array([0.7, 0.2, 0.1]),\n",
    "    'Good model':    np.array([0.6, 0.25, 0.15]),\n",
    "    'Bad model':     np.array([0.33, 0.34, 0.33]),\n",
    "    'Wrong model':   np.array([0.1, 0.2, 0.7]),\n",
    "}\n",
    "\n",
    "print('Cross-entropy H(p, q) for different model distributions q:')\n",
    "print(f'  True distribution p = {true_dist}')\n",
    "print(f'  Entropy H(p) = {shannon_entropy(true_dist):.4f} bits (lower bound)\\n')\n",
    "\n",
    "results = []\n",
    "for name, q in model_dists.items():\n",
    "    ce = cross_entropy(true_dist, q)\n",
    "    h = shannon_entropy(true_dist)\n",
    "    kl = ce - h  # KL divergence = CE - H\n",
    "    results.append({'Model': name, 'H(p,q)': f'{ce:.4f}', 'KL(p||q)': f'{kl:.4f}'})\n",
    "    print(f'  {name:16s}: H(p,q) = {ce:.4f}  KL(p||q) = {kl:.4f}')\n",
    "\n",
    "print(f'\\nVerify: H(p,q) = H(p) + KL(p||q) always holds')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our cross-entropy implementation against PyTorch's built-in."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def verify_cross_entropy_against_pytorch() -> None:\n",
    "    \"\"\"Compare our cross-entropy with PyTorch's nn.CrossEntropyLoss.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    n_samples = 100\n",
    "    n_cls = 5\n",
    "\n",
    "    # Random logits and labels\n",
    "    logits_np = rng.randn(n_samples, n_cls).astype(np.float32)\n",
    "    labels_np = rng.randint(0, n_cls, size=n_samples)\n",
    "\n",
    "    # Our implementation\n",
    "    our_loss = categorical_cross_entropy_loss(labels_np, logits_np)\n",
    "\n",
    "    # PyTorch implementation\n",
    "    logits_pt = torch.tensor(logits_np)\n",
    "    labels_pt = torch.tensor(labels_np, dtype=torch.long)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    pt_loss = criterion(logits_pt, labels_pt).item()\n",
    "\n",
    "    print(f'Our cross-entropy:     {our_loss:.6f}')\n",
    "    print(f'PyTorch CrossEntropy:  {pt_loss:.6f}')\n",
    "    print(f'Absolute difference:   {abs(our_loss - pt_loss):.2e}')\n",
    "    assert abs(our_loss - pt_loss) < 1e-5, 'Mismatch!'\n",
    "    print('Verification PASSED')\n",
    "\n",
    "\n",
    "verify_cross_entropy_against_pytorch()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Kullback–Leibler (KL) Divergence",
    "",
    "KL divergence measures how much one distribution $q$ diverges from",
    "a reference distribution $p$:",
    "",
    "$$D_{\\text{KL}}(p \\| q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} = H(p, q) - H(p)$$",
    "",
    "Key properties:",
    "- $D_{\\text{KL}}(p \\| q) \\geq 0$ (Gibbs' inequality)",
    "- $D_{\\text{KL}}(p \\| q) = 0$ iff $p = q$",
    "- **Not symmetric:** $D_{\\text{KL}}(p \\| q) \\neq D_{\\text{KL}}(q \\| p)$ in general",
    "- **Not a metric:** does not satisfy the triangle inequality",
    "",
    "**ML applications:**",
    "- **VAE loss:** $D_{\\text{KL}}(q(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))$ regularizes the latent space",
    "- **Knowledge distillation:** $D_{\\text{KL}}(p_{\\text{teacher}} \\| p_{\\text{student}})$ transfers knowledge",
    "- **Policy optimization:** $D_{\\text{KL}}(\\pi_{\\text{old}} \\| \\pi_{\\text{new}})$ constrains policy updates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def kl_divergence(\n",
    "    p: np.ndarray,\n",
    "    q: np.ndarray,\n",
    "    base: float = 2.0,\n",
    ") -> float:\n",
    "    \"\"\"Compute KL divergence D_KL(p || q).\n",
    "\n",
    "    Args:\n",
    "        p: True/reference distribution.\n",
    "        q: Approximate distribution.\n",
    "        base: Logarithm base.\n",
    "\n",
    "    Returns:\n",
    "        KL divergence value (non-negative).\n",
    "    \"\"\"\n",
    "    eps = 1e-15\n",
    "    q = np.clip(q, eps, 1.0)\n",
    "    mask = p > 0\n",
    "    return np.sum(p[mask] * np.log(p[mask] / q[mask]) / np.log(base))\n",
    "\n",
    "\n",
    "# Demonstrate asymmetry\n",
    "p = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "q = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "kl_pq = kl_divergence(p, q)\n",
    "kl_qp = kl_divergence(q, p)\n",
    "\n",
    "print(f'p = {p}')\n",
    "print(f'q = {q} (uniform)')\n",
    "print(f'KL(p || q) = {kl_pq:.6f} bits')\n",
    "print(f'KL(q || p) = {kl_qp:.6f} bits')\n",
    "print(f'Asymmetry: KL(p||q) - KL(q||p) = {kl_pq - kl_qp:.6f} bits')\n",
    "print(f'\\nNote: KL is NOT symmetric — it is a directed divergence, not a distance.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward vs Reverse KL",
    "",
    "The choice of direction matters in practice:",
    "- **Forward KL** $D_{\\text{KL}}(p \\| q)$: penalizes $q$ for having",
    "  low probability where $p$ has high probability → **mode-covering** (q is spread out)",
    "- **Reverse KL** $D_{\\text{KL}}(q \\| p)$: penalizes $q$ for having",
    "  high probability where $p$ has low probability → **mode-seeking** (q is peaked)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_kl_asymmetry() -> None:\n",
    "    \"\"\"Visualize forward vs reverse KL with a bimodal target.\"\"\"\n",
    "    x = np.linspace(-6, 6, 1000)\n",
    "\n",
    "    # True distribution: bimodal\n",
    "    p = 0.5 * sp_stats.norm.pdf(x, -2, 0.8) + 0.5 * sp_stats.norm.pdf(x, 2, 0.8)\n",
    "    p = p / p.sum()  # Normalize to a proper PMF for computation\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Forward KL: mode-covering\n",
    "    axes[0].fill_between(x, p / p.max(), alpha=0.3, color=COLORS['blue'], label='p (true)')\n",
    "    axes[0].plot(x, p / p.max(), color=COLORS['blue'], linewidth=2)\n",
    "    q_forward = sp_stats.norm.pdf(x, 0, 2.5)\n",
    "    q_forward = q_forward / q_forward.max()\n",
    "    axes[0].plot(x, q_forward, '--', color=COLORS['red'], linewidth=2,\n",
    "                  label='q (forward KL fit)')\n",
    "    axes[0].set_title('Forward KL: Mode-Covering\\n$q$ spreads to cover all of $p$')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].set_xlabel('x')\n",
    "    axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "    # Reverse KL: mode-seeking (left mode)\n",
    "    axes[1].fill_between(x, p / p.max(), alpha=0.3, color=COLORS['blue'], label='p (true)')\n",
    "    axes[1].plot(x, p / p.max(), color=COLORS['blue'], linewidth=2)\n",
    "    q_reverse = sp_stats.norm.pdf(x, -2, 0.8)\n",
    "    q_reverse = q_reverse / q_reverse.max()\n",
    "    axes[1].plot(x, q_reverse, '--', color=COLORS['green'], linewidth=2,\n",
    "                  label='q (reverse KL fit)')\n",
    "    axes[1].set_title('Reverse KL: Mode-Seeking\\n$q$ locks onto one mode')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    # Both on same plot\n",
    "    axes[2].fill_between(x, p / p.max(), alpha=0.3, color=COLORS['blue'], label='p (true)')\n",
    "    axes[2].plot(x, p / p.max(), color=COLORS['blue'], linewidth=2)\n",
    "    axes[2].plot(x, q_forward, '--', color=COLORS['red'], linewidth=2, label='Forward KL')\n",
    "    axes[2].plot(x, q_reverse, '--', color=COLORS['green'], linewidth=2, label='Reverse KL')\n",
    "    axes[2].set_title('Forward vs Reverse KL')\n",
    "    axes[2].legend(fontsize=10)\n",
    "    axes[2].set_xlabel('x')\n",
    "    axes[2].grid(True, alpha=0.2)\n",
    "\n",
    "    plt.suptitle('KL Divergence Asymmetry', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_kl_asymmetry()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Jensen–Shannon (JS) Divergence",
    "",
    "Since KL divergence is asymmetric, we often use the **Jensen–Shannon",
    "divergence** which is symmetric and bounded:",
    "",
    "$$D_{\\text{JS}}(p \\| q) = \\frac{1}{2} D_{\\text{KL}}\\left(p \\| \\frac{p+q}{2}\\right) + \\frac{1}{2} D_{\\text{KL}}\\left(q \\| \\frac{p+q}{2}\\right)$$",
    "",
    "Properties:",
    "- $0 \\leq D_{\\text{JS}}(p \\| q) \\leq 1$ (when using $\\log_2$)",
    "- Symmetric: $D_{\\text{JS}}(p \\| q) = D_{\\text{JS}}(q \\| p)$",
    "- $\\sqrt{D_{\\text{JS}}}$ is a proper metric",
    "",
    "**ML application:** The original GAN loss is related to JS divergence between",
    "the real and generated distributions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def js_divergence(\n",
    "    p: np.ndarray,\n",
    "    q: np.ndarray,\n",
    "    base: float = 2.0,\n",
    ") -> float:\n",
    "    \"\"\"Compute Jensen-Shannon divergence.\n",
    "\n",
    "    Args:\n",
    "        p: First distribution.\n",
    "        q: Second distribution.\n",
    "        base: Logarithm base.\n",
    "\n",
    "    Returns:\n",
    "        JS divergence (symmetric, bounded in [0, 1] for base=2).\n",
    "    \"\"\"\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, m, base) + 0.5 * kl_divergence(q, m, base)\n",
    "\n",
    "\n",
    "# Compare KL and JS divergences as distributions diverge\n",
    "base_dist = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "alpha_range = np.linspace(0, 1, 50)\n",
    "uniform_dist = np.ones(4) / 4\n",
    "\n",
    "kl_fwd_vals = []\n",
    "kl_rev_vals = []\n",
    "js_vals = []\n",
    "\n",
    "for alpha in alpha_range:\n",
    "    q_interp = (1 - alpha) * base_dist + alpha * uniform_dist\n",
    "    q_interp = q_interp / q_interp.sum()\n",
    "    kl_fwd_vals.append(kl_divergence(base_dist, q_interp))\n",
    "    kl_rev_vals.append(kl_divergence(q_interp, base_dist))\n",
    "    js_vals.append(js_divergence(base_dist, q_interp))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(alpha_range, kl_fwd_vals, color=COLORS['blue'], linewidth=2, label='KL(p || q)')\n",
    "ax.plot(alpha_range, kl_rev_vals, color=COLORS['green'], linewidth=2, label='KL(q || p)')\n",
    "ax.plot(alpha_range, js_vals, color=COLORS['red'], linewidth=2, label='JS(p, q)')\n",
    "ax.set_xlabel('α (interpolation towards uniform)', fontsize=12)\n",
    "ax.set_ylabel('Divergence (bits)', fontsize=12)\n",
    "ax.set_title('KL vs JS Divergence: Asymmetry and Boundedness')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify symmetry\n",
    "p_test = np.array([0.3, 0.5, 0.2])\n",
    "q_test = np.array([0.1, 0.6, 0.3])\n",
    "print(f'JS(p, q) = {js_divergence(p_test, q_test):.6f}')\n",
    "print(f'JS(q, p) = {js_divergence(q_test, p_test):.6f}')\n",
    "print(f'Symmetric: {np.isclose(js_divergence(p_test, q_test), js_divergence(q_test, p_test))}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Conditional Entropy and the Chain Rule",
    "",
    "The **conditional entropy** measures the remaining uncertainty in $Y$",
    "after observing $X$:",
    "",
    "$$H(Y | X) = -\\sum_{x, y} p(x, y) \\log_2 p(y | x)$$",
    "",
    "The **chain rule of entropy**:",
    "",
    "$$H(X, Y) = H(X) + H(Y | X) = H(Y) + H(X | Y)$$",
    "",
    "This decomposes the joint uncertainty into the uncertainty of one",
    "variable plus the remaining uncertainty of the other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def joint_entropy(\n",
    "    joint_prob: np.ndarray,\n",
    "    base: float = 2.0,\n",
    ") -> float:\n",
    "    \"\"\"Compute joint entropy H(X, Y) from a joint probability table.\n",
    "\n",
    "    Args:\n",
    "        joint_prob: Joint probability matrix of shape (|X|, |Y|).\n",
    "        base: Logarithm base.\n",
    "\n",
    "    Returns:\n",
    "        Joint entropy value.\n",
    "    \"\"\"\n",
    "    p_flat = joint_prob.ravel()\n",
    "    p_nonzero = p_flat[p_flat > 0]\n",
    "    return -np.sum(p_nonzero * np.log(p_nonzero) / np.log(base))\n",
    "\n",
    "\n",
    "def conditional_entropy(\n",
    "    joint_prob: np.ndarray,\n",
    "    base: float = 2.0,\n",
    ") -> float:\n",
    "    \"\"\"Compute conditional entropy H(Y|X) from joint probability table.\n",
    "\n",
    "    H(Y|X) = H(X,Y) - H(X)\n",
    "\n",
    "    Args:\n",
    "        joint_prob: Joint probability matrix of shape (|X|, |Y|).\n",
    "        base: Logarithm base.\n",
    "\n",
    "    Returns:\n",
    "        Conditional entropy H(Y|X).\n",
    "    \"\"\"\n",
    "    h_xy = joint_entropy(joint_prob, base)\n",
    "    p_x = joint_prob.sum(axis=1)\n",
    "    h_x = shannon_entropy(p_x, base)\n",
    "    return h_xy - h_x\n",
    "\n",
    "\n",
    "# Example: Weather and Umbrella\n",
    "# Joint distribution P(Weather, Umbrella)\n",
    "#              Umbrella=Yes  Umbrella=No\n",
    "# Sunny          0.10          0.40\n",
    "# Rainy          0.35          0.05\n",
    "# Cloudy         0.05          0.05\n",
    "joint_wu = np.array([\n",
    "    [0.10, 0.40],\n",
    "    [0.35, 0.05],\n",
    "    [0.05, 0.05],\n",
    "])\n",
    "\n",
    "# Marginals\n",
    "p_weather = joint_wu.sum(axis=1)\n",
    "p_umbrella = joint_wu.sum(axis=0)\n",
    "\n",
    "h_w = shannon_entropy(p_weather)\n",
    "h_u = shannon_entropy(p_umbrella)\n",
    "h_wu = joint_entropy(joint_wu)\n",
    "h_u_given_w = conditional_entropy(joint_wu)\n",
    "h_w_given_u = h_wu - h_u  # Chain rule: H(W|U) = H(W,U) - H(U)\n",
    "\n",
    "print('=== Entropy Decomposition ===')\n",
    "print(f'H(Weather)       = {h_w:.4f} bits')\n",
    "print(f'H(Umbrella)      = {h_u:.4f} bits')\n",
    "print(f'H(Weather, Umb)  = {h_wu:.4f} bits')\n",
    "print(f'H(Umb | Weather) = {h_u_given_w:.4f} bits')\n",
    "print(f'H(Weather | Umb) = {h_w_given_u:.4f} bits')\n",
    "print()\n",
    "print('Chain rule verification:')\n",
    "print(f'  H(W) + H(U|W) = {h_w:.4f} + {h_u_given_w:.4f} = {h_w + h_u_given_w:.4f}')\n",
    "print(f'  H(W, U)        = {h_wu:.4f}')\n",
    "assert np.isclose(h_w + h_u_given_w, h_wu), 'Chain rule violated!'\n",
    "print('  Chain rule VERIFIED')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Mutual Information",
    "",
    "**Mutual information** $I(X; Y)$ measures how much knowing $X$ reduces",
    "the uncertainty about $Y$ (and vice versa):",
    "",
    "$$I(X; Y) = H(Y) - H(Y | X) = H(X) - H(X | Y)$$",
    "",
    "Equivalently:",
    "",
    "$$I(X; Y) = \\sum_{x, y} p(x, y) \\log_2 \\frac{p(x, y)}{p(x) p(y)} = D_{\\text{KL}}(p(x, y) \\| p(x) p(y))$$",
    "",
    "Properties:",
    "- $I(X; Y) \\geq 0$ (non-negative)",
    "- $I(X; Y) = 0$ iff $X$ and $Y$ are independent",
    "- $I(X; Y) = I(Y; X)$ (symmetric)",
    "- $I(X; X) = H(X)$ (self-information = entropy)",
    "",
    "**ML applications:**",
    "- Feature selection (select features with highest MI with target)",
    "- Information bottleneck (compress representations while preserving MI with labels)",
    "- Infomax principle (maximize MI between input and learned representation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def mutual_information(\n",
    "    joint_prob: np.ndarray,\n",
    "    base: float = 2.0,\n",
    ") -> float:\n",
    "    \"\"\"Compute mutual information I(X; Y) from joint probability table.\n",
    "\n",
    "    Args:\n",
    "        joint_prob: Joint probability matrix of shape (|X|, |Y|).\n",
    "        base: Logarithm base.\n",
    "\n",
    "    Returns:\n",
    "        Mutual information value.\n",
    "    \"\"\"\n",
    "    p_x = joint_prob.sum(axis=1)  # Marginal P(X)\n",
    "    p_y = joint_prob.sum(axis=0)  # Marginal P(Y)\n",
    "    h_x = shannon_entropy(p_x, base)\n",
    "    h_y_given_x = conditional_entropy(joint_prob, base)\n",
    "    return h_x - h_y_given_x  # I(X;Y) = H(X) - H(X|Y) ... wait, using H(Y|X)\n",
    "\n",
    "\n",
    "def mutual_information_direct(\n",
    "    joint_prob: np.ndarray,\n",
    "    base: float = 2.0,\n",
    ") -> float:\n",
    "    \"\"\"Compute mutual information directly via KL(p(x,y) || p(x)p(y)).\n",
    "\n",
    "    Args:\n",
    "        joint_prob: Joint probability matrix of shape (|X|, |Y|).\n",
    "        base: Logarithm base.\n",
    "\n",
    "    Returns:\n",
    "        Mutual information value.\n",
    "    \"\"\"\n",
    "    p_x = joint_prob.sum(axis=1, keepdims=True)\n",
    "    p_y = joint_prob.sum(axis=0, keepdims=True)\n",
    "    independent = p_x * p_y  # P(X)P(Y) product distribution\n",
    "\n",
    "    eps = 1e-15\n",
    "    mask = joint_prob > 0\n",
    "    mi = np.sum(joint_prob[mask] * np.log(\n",
    "        joint_prob[mask] / (independent[mask] + eps)\n",
    "    ) / np.log(base))\n",
    "    return mi\n",
    "\n",
    "\n",
    "# Compute MI for the weather-umbrella example\n",
    "mi_formula = h_u - h_u_given_w  # I(W; U) = H(U) - H(U|W)\n",
    "mi_direct = mutual_information_direct(joint_wu)\n",
    "\n",
    "print(f'MI via formula:   I(W; U) = H(U) - H(U|W) = {h_u:.4f} - {h_u_given_w:.4f} = {mi_formula:.4f}')\n",
    "print(f'MI via KL(joint || product): {mi_direct:.4f}')\n",
    "assert np.isclose(mi_formula, mi_direct, atol=1e-10), 'MI computation mismatch!'\n",
    "print('Both methods agree!')\n",
    "print(f'\\nInterpretation: Knowing the weather reduces umbrella uncertainty by {mi_formula:.4f} bits')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 The Information Venn Diagram",
    "",
    "The relationships between entropy, conditional entropy, joint entropy,",
    "and mutual information form a Venn diagram:",
    "",
    "- $H(X, Y) = H(X) + H(Y) - I(X; Y)$",
    "- $H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$",
    "- $I(X; Y) = H(X) + H(Y) - H(X, Y)$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_information_venn(\n",
    "    h_x: float,\n",
    "    h_y: float,\n",
    "    h_xy: float,\n",
    "    mi_xy: float,\n",
    "    label_x: str = 'X',\n",
    "    label_y: str = 'Y',\n",
    ") -> None:\n",
    "    \"\"\"Plot Venn diagram of information-theoretic quantities.\n",
    "\n",
    "    Args:\n",
    "        h_x: Entropy of X.\n",
    "        h_y: Entropy of Y.\n",
    "        h_xy: Joint entropy.\n",
    "        mi_xy: Mutual information.\n",
    "        label_x: Label for variable X.\n",
    "        label_y: Label for variable Y.\n",
    "    \"\"\"\n",
    "    from matplotlib.patches import Circle\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Draw circles\n",
    "    c1 = Circle((-0.5, 0), 1.5, fill=False, edgecolor=COLORS['blue'],\n",
    "                 linewidth=3, label=f'H({label_x})')\n",
    "    c2 = Circle((0.5, 0), 1.5, fill=False, edgecolor=COLORS['red'],\n",
    "                 linewidth=3, label=f'H({label_y})')\n",
    "    ax.add_patch(c1)\n",
    "    ax.add_patch(c2)\n",
    "\n",
    "    # Fill regions\n",
    "    from matplotlib.patches import FancyBboxPatch\n",
    "    h_x_only = h_x - mi_xy\n",
    "    h_y_only = h_y - mi_xy\n",
    "\n",
    "    # Labels\n",
    "    ax.text(-1.3, 0, f'H({label_x}|{label_y})\\n{h_x_only:.3f}',\n",
    "             ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "             color=COLORS['blue'])\n",
    "    ax.text(0, 0, f'I({label_x};{label_y})\\n{mi_xy:.3f}',\n",
    "             ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "             color=COLORS['purple'])\n",
    "    ax.text(1.3, 0, f'H({label_y}|{label_x})\\n{h_y_only:.3f}',\n",
    "             ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "             color=COLORS['red'])\n",
    "\n",
    "    # Overall labels\n",
    "    ax.text(-0.5, 1.8, f'H({label_x}) = {h_x:.3f}',\n",
    "             ha='center', fontsize=11, color=COLORS['blue'])\n",
    "    ax.text(0.5, 1.8, f'H({label_y}) = {h_y:.3f}',\n",
    "             ha='center', fontsize=11, color=COLORS['red'])\n",
    "    ax.text(0, -2.0, f'H({label_x},{label_y}) = {h_xy:.3f}',\n",
    "             ha='center', fontsize=11, color=COLORS['gray'])\n",
    "\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Information Venn Diagram', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_information_venn(\n",
    "    h_x=h_w, h_y=h_u, h_xy=h_wu, mi_xy=mi_formula,\n",
    "    label_x='Weather', label_y='Umbrella',\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 2 — Putting It All Together: InformationTheoryToolkit",
    "",
    "We assemble our information-theoretic primitives into a reusable class",
    "that can analyze distributions and compute all key quantities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class InformationTheoryToolkit:\n",
    "    \"\"\"Reusable information theory toolkit for ML.\n",
    "\n",
    "    Provides methods for computing entropy, cross-entropy, KL divergence,\n",
    "    mutual information, and related quantities.\n",
    "\n",
    "    Attributes:\n",
    "        base: Logarithm base (2 for bits, e for nats).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base: float = 2.0) -> None:\n",
    "        \"\"\"Initialize toolkit.\n",
    "\n",
    "        Args:\n",
    "            base: Logarithm base (2 for bits, e for nats).\n",
    "        \"\"\"\n",
    "        self.base = base\n",
    "\n",
    "    def entropy(self, p: np.ndarray) -> float:\n",
    "        \"\"\"Compute Shannon entropy.\n",
    "\n",
    "        Args:\n",
    "            p: Probability distribution.\n",
    "\n",
    "        Returns:\n",
    "            Entropy value.\n",
    "        \"\"\"\n",
    "        return shannon_entropy(p, self.base)\n",
    "\n",
    "    def cross_entropy(self, p: np.ndarray, q: np.ndarray) -> float:\n",
    "        \"\"\"Compute cross-entropy H(p, q).\n",
    "\n",
    "        Args:\n",
    "            p: True distribution.\n",
    "            q: Model distribution.\n",
    "\n",
    "        Returns:\n",
    "            Cross-entropy value.\n",
    "        \"\"\"\n",
    "        return cross_entropy(p, q, self.base)\n",
    "\n",
    "    def kl_divergence(self, p: np.ndarray, q: np.ndarray) -> float:\n",
    "        \"\"\"Compute KL divergence D_KL(p || q).\n",
    "\n",
    "        Args:\n",
    "            p: Reference distribution.\n",
    "            q: Approximate distribution.\n",
    "\n",
    "        Returns:\n",
    "            KL divergence value.\n",
    "        \"\"\"\n",
    "        return kl_divergence(p, q, self.base)\n",
    "\n",
    "    def js_divergence(self, p: np.ndarray, q: np.ndarray) -> float:\n",
    "        \"\"\"Compute Jensen-Shannon divergence.\n",
    "\n",
    "        Args:\n",
    "            p: First distribution.\n",
    "            q: Second distribution.\n",
    "\n",
    "        Returns:\n",
    "            JS divergence value.\n",
    "        \"\"\"\n",
    "        return js_divergence(p, q, self.base)\n",
    "\n",
    "    def mutual_information(self, joint_prob: np.ndarray) -> float:\n",
    "        \"\"\"Compute mutual information from joint probability table.\n",
    "\n",
    "        Args:\n",
    "            joint_prob: Joint probability matrix.\n",
    "\n",
    "        Returns:\n",
    "            Mutual information value.\n",
    "        \"\"\"\n",
    "        return mutual_information_direct(joint_prob, self.base)\n",
    "\n",
    "    def summary(self, joint_prob: np.ndarray, label_x: str = 'X', label_y: str = 'Y') -> pd.DataFrame:\n",
    "        \"\"\"Compute all information-theoretic quantities for a joint distribution.\n",
    "\n",
    "        Args:\n",
    "            joint_prob: Joint probability matrix.\n",
    "            label_x: Name for the row variable.\n",
    "            label_y: Name for the column variable.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame summarizing all quantities.\n",
    "        \"\"\"\n",
    "        p_x = joint_prob.sum(axis=1)\n",
    "        p_y = joint_prob.sum(axis=0)\n",
    "\n",
    "        h_x = self.entropy(p_x)\n",
    "        h_y = self.entropy(p_y)\n",
    "        h_xy = joint_entropy(joint_prob, self.base)\n",
    "        h_y_given_x = conditional_entropy(joint_prob, self.base)\n",
    "        h_x_given_y = h_xy - h_y\n",
    "        mi = self.mutual_information(joint_prob)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'Quantity': [\n",
    "                f'H({label_x})', f'H({label_y})', f'H({label_x},{label_y})',\n",
    "                f'H({label_y}|{label_x})', f'H({label_x}|{label_y})',\n",
    "                f'I({label_x};{label_y})',\n",
    "            ],\n",
    "            'Value (bits)': [h_x, h_y, h_xy, h_y_given_x, h_x_given_y, mi],\n",
    "        })\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "toolkit = InformationTheoryToolkit(base=2.0)\n",
    "\n",
    "# Verify toolkit with the weather-umbrella example\n",
    "summary_df = toolkit.summary(joint_wu, 'Weather', 'Umbrella')\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Quick check: MI via toolkit matches our manual computation\n",
    "assert np.isclose(toolkit.mutual_information(joint_wu), mi_formula), 'Toolkit MI mismatch!'\n",
    "print('\\nToolkit verification PASSED')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 3 — Application: Information Theory in Practice",
    "",
    "We apply our information-theoretic tools to real ML scenarios:",
    "cross-entropy as a loss function, KL divergence for model comparison,",
    "and mutual information for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cross-Entropy as Classification Loss",
    "",
    "When we train a classifier, the cross-entropy loss measures how well",
    "the model's predicted distribution matches the true label distribution.",
    "Let's see how cross-entropy behaves during simulated training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def simulate_training_progress() -> None:\n",
    "    \"\"\"Simulate how cross-entropy changes as a model improves.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    n_samples = 500\n",
    "    n_cls = 5\n",
    "\n",
    "    # True labels (one-hot)\n",
    "    true_labels = rng.randint(0, n_cls, n_samples)\n",
    "\n",
    "    # Simulate model improvement stages\n",
    "    stages = [\n",
    "        ('Random (untrained)', 0.0),\n",
    "        ('Early training', 0.3),\n",
    "        ('Mid training', 0.7),\n",
    "        ('Late training', 0.9),\n",
    "        ('Well-trained', 0.95),\n",
    "        ('Overconfident', 1.0),\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.ravel()\n",
    "    results = []\n",
    "\n",
    "    for idx, (name, confidence) in enumerate(stages):\n",
    "        # Generate predicted probabilities\n",
    "        logits = rng.randn(n_samples, n_cls)\n",
    "        # Increase logit for correct class\n",
    "        for i in range(n_samples):\n",
    "            logits[i, true_labels[i]] += confidence * 5\n",
    "\n",
    "        # Softmax\n",
    "        exp_logits = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "        probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Cross-entropy\n",
    "        ce = categorical_cross_entropy_loss(true_labels, logits)\n",
    "\n",
    "        # Average predicted probability for correct class\n",
    "        correct_probs = probs[np.arange(n_samples), true_labels]\n",
    "        avg_correct = correct_probs.mean()\n",
    "\n",
    "        results.append({'Stage': name, 'CE Loss': ce, 'Avg P(correct)': avg_correct})\n",
    "\n",
    "        # Plot distribution of predicted probabilities for correct class\n",
    "        axes[idx].hist(correct_probs, bins=30, color=COLOR_LIST[idx],\n",
    "                        alpha=0.7, edgecolor='white', density=True)\n",
    "        axes[idx].set_xlabel('P(correct class)')\n",
    "        axes[idx].set_ylabel('Density')\n",
    "        axes[idx].set_title(f'{name}\\nCE={ce:.3f}, Avg P(c)={avg_correct:.3f}')\n",
    "        axes[idx].set_xlim(0, 1)\n",
    "\n",
    "    plt.suptitle('Cross-Entropy Loss During Training', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary table\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "\n",
    "simulate_training_progress()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 KL Divergence for Distribution Comparison",
    "",
    "KL divergence is commonly used to compare how well a model's output",
    "distribution matches a reference. Here we compare histograms of the",
    "Digits dataset features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compare_digit_distributions() -> None:\n",
    "    \"\"\"Compare pixel distributions across digit classes using KL/JS divergence.\"\"\"\n",
    "    # Sum all pixel values per image as a simple feature\n",
    "    pixel_sums = X_digits.sum(axis=1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Histogram per digit class\n",
    "    n_bins = 30\n",
    "    bin_edges = np.linspace(pixel_sums.min(), pixel_sums.max(), n_bins + 1)\n",
    "\n",
    "    class_hists = {}\n",
    "    for c in range(n_classes):\n",
    "        mask = y_digits == c\n",
    "        hist, _ = np.histogram(pixel_sums[mask], bins=bin_edges, density=False)\n",
    "        hist = hist.astype(np.float64) + 1e-10  # Smoothing\n",
    "        hist = hist / hist.sum()\n",
    "        class_hists[c] = hist\n",
    "        if c < 5:\n",
    "            axes[0].plot(bin_edges[:-1], hist, '-', linewidth=1.5,\n",
    "                          color=COLOR_LIST[c], label=f'Digit {c}', alpha=0.8)\n",
    "\n",
    "    axes[0].set_xlabel('Pixel Sum')\n",
    "    axes[0].set_ylabel('Probability')\n",
    "    axes[0].set_title('Pixel Sum Distribution per Class')\n",
    "    axes[0].legend(fontsize=9)\n",
    "    axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "    # KL divergence matrix\n",
    "    kl_matrix = np.zeros((n_classes, n_classes))\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            kl_matrix[i, j] = kl_divergence(class_hists[i], class_hists[j])\n",
    "\n",
    "    im = axes[1].imshow(kl_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    axes[1].set_xlabel('Digit (q)')\n",
    "    axes[1].set_ylabel('Digit (p)')\n",
    "    axes[1].set_title('KL(digit_p || digit_q) Matrix')\n",
    "    axes[1].set_xticks(range(n_classes))\n",
    "    axes[1].set_yticks(range(n_classes))\n",
    "    plt.colorbar(im, ax=axes[1], label='KL Divergence (bits)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Find most and least similar pairs (excluding self)\n",
    "    kl_no_diag = kl_matrix.copy()\n",
    "    np.fill_diagonal(kl_no_diag, np.inf)\n",
    "    min_idx = np.unravel_index(kl_no_diag.argmin(), kl_no_diag.shape)\n",
    "    np.fill_diagonal(kl_no_diag, 0)\n",
    "    max_idx = np.unravel_index(kl_no_diag.argmax(), kl_no_diag.shape)\n",
    "\n",
    "    print(f'Most similar pair:  digits {min_idx[0]} and {min_idx[1]} '\n",
    "          f'(KL = {kl_matrix[min_idx]:.4f})')\n",
    "    print(f'Most different pair: digits {max_idx[0]} and {max_idx[1]} '\n",
    "          f'(KL = {kl_matrix[max_idx]:.4f})')\n",
    "\n",
    "\n",
    "compare_digit_distributions()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Selection with Mutual Information",
    "",
    "Mutual information can measure how informative each feature is about",
    "the target variable. Features with high MI are good candidates for",
    "model input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def estimate_mi_continuous(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    n_bins: int = 20,\n",
    ") -> float:\n",
    "    \"\"\"Estimate mutual information between a continuous feature and discrete labels.\n",
    "\n",
    "    Uses histogram-based discretization.\n",
    "\n",
    "    Args:\n",
    "        x: Continuous feature values of shape (n,).\n",
    "        y: Discrete labels of shape (n,).\n",
    "        n_bins: Number of bins for discretizing x.\n",
    "\n",
    "    Returns:\n",
    "        Estimated mutual information in bits.\n",
    "    \"\"\"\n",
    "    # Discretize x\n",
    "    x_bins = np.digitize(x, np.linspace(x.min(), x.max(), n_bins + 1)[1:-1])\n",
    "\n",
    "    # Build joint distribution\n",
    "    classes = np.unique(y)\n",
    "    joint = np.zeros((n_bins, len(classes)))\n",
    "    for i in range(len(x)):\n",
    "        bin_idx = min(x_bins[i], n_bins - 1)\n",
    "        class_idx = np.searchsorted(classes, y[i])\n",
    "        joint[bin_idx, class_idx] += 1\n",
    "\n",
    "    # Add smoothing and normalize\n",
    "    joint += 1e-10\n",
    "    joint = joint / joint.sum()\n",
    "\n",
    "    return mutual_information_direct(joint)\n",
    "\n",
    "\n",
    "def mi_feature_selection() -> None:\n",
    "    \"\"\"Rank digit pixel positions by mutual information with digit label.\"\"\"\n",
    "    n_features = X_digits.shape[1]\n",
    "    mi_scores = np.zeros(n_features)\n",
    "\n",
    "    for f in range(n_features):\n",
    "        mi_scores[f] = estimate_mi_continuous(X_digits[:, f], y_digits)\n",
    "\n",
    "    # Compare with sklearn's mutual_info_classif\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    mi_sklearn = mutual_info_classif(\n",
    "        X_digits, y_digits, discrete_features=False, random_state=SEED,\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Reshape MI scores to 8x8 image\n",
    "    mi_image = mi_scores.reshape(8, 8)\n",
    "    im1 = axes[0].imshow(mi_image, cmap='hot', interpolation='nearest')\n",
    "    axes[0].set_title('Our MI Scores (Histogram-based)')\n",
    "    axes[0].set_xlabel('Pixel Column')\n",
    "    axes[0].set_ylabel('Pixel Row')\n",
    "    plt.colorbar(im1, ax=axes[0], label='MI (bits)')\n",
    "\n",
    "    # Sklearn MI scores\n",
    "    mi_sk_image = mi_sklearn.reshape(8, 8)\n",
    "    im2 = axes[1].imshow(mi_sk_image, cmap='hot', interpolation='nearest')\n",
    "    axes[1].set_title('sklearn MI Scores (k-NN based)')\n",
    "    axes[1].set_xlabel('Pixel Column')\n",
    "    axes[1].set_ylabel('Pixel Row')\n",
    "    plt.colorbar(im2, ax=axes[1], label='MI (nats)')\n",
    "\n",
    "    # Correlation between our and sklearn MI\n",
    "    axes[2].scatter(mi_scores, mi_sklearn, alpha=0.6, color=COLORS['blue'])\n",
    "    axes[2].set_xlabel('Our MI (bits)')\n",
    "    axes[2].set_ylabel('sklearn MI (nats)')\n",
    "    axes[2].set_title(f'Our MI vs sklearn MI\\nCorrelation: {np.corrcoef(mi_scores, mi_sklearn)[0,1]:.4f}')\n",
    "    axes[2].grid(True, alpha=0.2)\n",
    "\n",
    "    plt.suptitle('Feature Importance via Mutual Information', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Top and bottom features\n",
    "    top_indices = np.argsort(mi_scores)[::-1][:5]\n",
    "    bot_indices = np.argsort(mi_scores)[:5]\n",
    "    print('Top 5 most informative pixels:')\n",
    "    for i in top_indices:\n",
    "        row, col = divmod(i, 8)\n",
    "        print(f'  Pixel ({row},{col}): MI = {mi_scores[i]:.4f} bits')\n",
    "    print('\\nBottom 5 least informative pixels:')\n",
    "    for i in bot_indices:\n",
    "        row, col = divmod(i, 8)\n",
    "        print(f'  Pixel ({row},{col}): MI = {mi_scores[i]:.4f} bits')\n",
    "\n",
    "\n",
    "mi_feature_selection()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Prediction Entropy as Uncertainty Measure",
    "",
    "The entropy of a model's predicted probability vector indicates how",
    "uncertain the model is. Low entropy = confident prediction, high",
    "entropy = uncertain prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_prediction_entropy() -> None:\n",
    "    \"\"\"Show how prediction entropy correlates with model confidence.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    n_test = 200\n",
    "    n_cls = 10\n",
    "\n",
    "    # Simulate model predictions with varying confidence\n",
    "    true_labels = rng.randint(0, n_cls, n_test)\n",
    "    logits = rng.randn(n_test, n_cls) * 0.5\n",
    "\n",
    "    # Add signal for correct class (varying strength)\n",
    "    signal_strengths = rng.uniform(0, 6, n_test)\n",
    "    for i in range(n_test):\n",
    "        logits[i, true_labels[i]] += signal_strengths[i]\n",
    "\n",
    "    # Softmax\n",
    "    exp_logits = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Compute entropy for each prediction\n",
    "    pred_entropies = np.array([shannon_entropy(probs[i]) for i in range(n_test)])\n",
    "\n",
    "    # Check correctness\n",
    "    predicted = probs.argmax(axis=1)\n",
    "    correct = predicted == true_labels\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Entropy distribution for correct vs incorrect\n",
    "    axes[0].hist(pred_entropies[correct], bins=25, alpha=0.7, color=COLORS['green'],\n",
    "                  label=f'Correct ({correct.sum()})', density=True)\n",
    "    axes[0].hist(pred_entropies[~correct], bins=25, alpha=0.7, color=COLORS['red'],\n",
    "                  label=f'Incorrect ({(~correct).sum()})', density=True)\n",
    "    axes[0].set_xlabel('Prediction Entropy (bits)')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].set_title('Entropy: Correct vs Incorrect Predictions')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Max probability vs entropy\n",
    "    max_probs = probs.max(axis=1)\n",
    "    axes[1].scatter(max_probs, pred_entropies, c=correct.astype(int),\n",
    "                     cmap='RdYlGn', alpha=0.6, edgecolors='gray', linewidth=0.3)\n",
    "    axes[1].set_xlabel('Max Predicted Probability')\n",
    "    axes[1].set_ylabel('Prediction Entropy (bits)')\n",
    "    axes[1].set_title('Confidence vs Entropy')\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    # Accuracy at different entropy thresholds\n",
    "    thresholds = np.linspace(0, np.log2(n_cls), 50)\n",
    "    accs = []\n",
    "    coverages = []\n",
    "    for t in thresholds:\n",
    "        mask = pred_entropies <= t\n",
    "        if mask.sum() > 0:\n",
    "            accs.append(correct[mask].mean())\n",
    "            coverages.append(mask.mean())\n",
    "        else:\n",
    "            accs.append(np.nan)\n",
    "            coverages.append(0)\n",
    "\n",
    "    ax_twin = axes[2].twinx()\n",
    "    axes[2].plot(thresholds, accs, color=COLORS['blue'], linewidth=2, label='Accuracy')\n",
    "    ax_twin.plot(thresholds, coverages, color=COLORS['orange'], linewidth=2, label='Coverage')\n",
    "    axes[2].set_xlabel('Entropy Threshold (bits)')\n",
    "    axes[2].set_ylabel('Accuracy', color=COLORS['blue'])\n",
    "    ax_twin.set_ylabel('Coverage', color=COLORS['orange'])\n",
    "    axes[2].set_title('Accuracy-Coverage Trade-off')\n",
    "    axes[2].grid(True, alpha=0.2)\n",
    "\n",
    "    # Combined legend\n",
    "    lines1, labels1 = axes[2].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax_twin.get_legend_handles_labels()\n",
    "    axes[2].legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "    plt.suptitle('Prediction Entropy as Uncertainty', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Key insight\n",
    "    low_entropy_mask = pred_entropies < 1.0\n",
    "    high_entropy_mask = pred_entropies > 2.5\n",
    "    print(f'Low entropy (<1.0 bits): accuracy={correct[low_entropy_mask].mean():.2%}, '\n",
    "          f'coverage={low_entropy_mask.mean():.2%}')\n",
    "    if high_entropy_mask.sum() > 0:\n",
    "        print(f'High entropy (>2.5 bits): accuracy={correct[high_entropy_mask].mean():.2%}, '\n",
    "              f'coverage={high_entropy_mask.mean():.2%}')\n",
    "\n",
    "\n",
    "demonstrate_prediction_entropy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 4 — Evaluation & Analysis",
    "",
    "We analyze edge cases, verify theoretical properties numerically,",
    "and benchmark our implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Verifying Theoretical Properties",
    "",
    "Let's systematically verify the key properties of entropy, KL divergence,",
    "and mutual information."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def verify_properties() -> None:\n",
    "    \"\"\"Numerically verify key information-theoretic properties.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    results = []\n",
    "\n",
    "    # 1. Entropy non-negativity\n",
    "    for _ in range(100):\n",
    "        p = rng.dirichlet(np.ones(5))\n",
    "        h = shannon_entropy(p)\n",
    "        assert h >= -1e-10, f'Entropy negative: {h}'\n",
    "    results.append(('H(X) >= 0', 'PASSED', 'Tested 100 random distributions'))\n",
    "\n",
    "    # 2. Maximum entropy is log2(K) for uniform\n",
    "    for k in [2, 5, 10, 50, 100]:\n",
    "        uniform_p = np.ones(k) / k\n",
    "        h = shannon_entropy(uniform_p)\n",
    "        assert np.isclose(h, np.log2(k)), f'Max entropy mismatch for K={k}'\n",
    "    results.append(('H(uniform) = log2(K)', 'PASSED', 'Verified for K=2,5,10,50,100'))\n",
    "\n",
    "    # 3. KL non-negativity (Gibbs' inequality)\n",
    "    for _ in range(100):\n",
    "        p = rng.dirichlet(np.ones(5))\n",
    "        q = rng.dirichlet(np.ones(5))\n",
    "        kl = kl_divergence(p, q)\n",
    "        assert kl >= -1e-10, f'KL negative: {kl}'\n",
    "    results.append(('D_KL(p||q) >= 0', 'PASSED', 'Tested 100 random distribution pairs'))\n",
    "\n",
    "    # 4. KL = 0 iff p = q\n",
    "    p = rng.dirichlet(np.ones(5))\n",
    "    kl_same = kl_divergence(p, p)\n",
    "    assert np.isclose(kl_same, 0), f'KL(p||p) != 0: {kl_same}'\n",
    "    results.append(('D_KL(p||p) = 0', 'PASSED', f'KL(p||p) = {kl_same:.2e}'))\n",
    "\n",
    "    # 5. Cross-entropy >= entropy\n",
    "    for _ in range(100):\n",
    "        p = rng.dirichlet(np.ones(5))\n",
    "        q = rng.dirichlet(np.ones(5))\n",
    "        ce = cross_entropy(p, q)\n",
    "        h = shannon_entropy(p)\n",
    "        assert ce >= h - 1e-10, f'CE < H: {ce} < {h}'\n",
    "    results.append(('H(p,q) >= H(p)', 'PASSED', 'Tested 100 pairs'))\n",
    "\n",
    "    # 6. Mutual information non-negativity\n",
    "    for _ in range(50):\n",
    "        joint = rng.dirichlet(np.ones(12)).reshape(3, 4)\n",
    "        mi = mutual_information_direct(joint)\n",
    "        assert mi >= -1e-10, f'MI negative: {mi}'\n",
    "    results.append(('I(X;Y) >= 0', 'PASSED', 'Tested 50 joint distributions'))\n",
    "\n",
    "    # 7. MI = 0 for independent variables\n",
    "    p_x = np.array([0.3, 0.7])\n",
    "    p_y = np.array([0.4, 0.6])\n",
    "    joint_indep = np.outer(p_x, p_y)\n",
    "    mi_indep = mutual_information_direct(joint_indep)\n",
    "    assert np.isclose(mi_indep, 0, atol=1e-10), f'MI for independent not 0: {mi_indep}'\n",
    "    results.append(('I(X;Y) = 0 for independent', 'PASSED', f'MI = {mi_indep:.2e}'))\n",
    "\n",
    "    # 8. JS symmetry\n",
    "    for _ in range(50):\n",
    "        p = rng.dirichlet(np.ones(5))\n",
    "        q = rng.dirichlet(np.ones(5))\n",
    "        assert np.isclose(js_divergence(p, q), js_divergence(q, p)), 'JS not symmetric'\n",
    "    results.append(('JS(p,q) = JS(q,p)', 'PASSED', 'Tested 50 pairs'))\n",
    "\n",
    "    # 9. JS bounded [0, 1]\n",
    "    for _ in range(100):\n",
    "        p = rng.dirichlet(np.ones(5))\n",
    "        q = rng.dirichlet(np.ones(5))\n",
    "        js = js_divergence(p, q)\n",
    "        assert -1e-10 <= js <= 1 + 1e-10, f'JS out of bounds: {js}'\n",
    "    results.append(('0 <= JS <= 1', 'PASSED', 'Tested 100 pairs'))\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['Property', 'Status', 'Details'])\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "\n",
    "verify_properties()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Entropy Estimation from Finite Samples",
    "",
    "Estimating entropy from data (using empirical frequencies) introduces",
    "a systematic **negative bias** — the plug-in estimator underestimates",
    "the true entropy, especially with small samples. This is important",
    "when using MI for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def entropy_estimation_bias() -> None:\n",
    "    \"\"\"Demonstrate bias of plug-in entropy estimator.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    true_k = 10\n",
    "    true_p = np.ones(true_k) / true_k  # Uniform\n",
    "    true_entropy = np.log2(true_k)\n",
    "\n",
    "    sample_sizes = [5, 10, 20, 50, 100, 200, 500, 1000, 5000]\n",
    "    n_trials = 200\n",
    "\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    for n in sample_sizes:\n",
    "        estimates = []\n",
    "        for _ in range(n_trials):\n",
    "            samples = rng.choice(true_k, size=n, p=true_p)\n",
    "            counts = np.bincount(samples, minlength=true_k)\n",
    "            freq = counts / counts.sum()\n",
    "            h_est = shannon_entropy(freq)\n",
    "            estimates.append(h_est)\n",
    "        means.append(np.mean(estimates))\n",
    "        stds.append(np.std(estimates))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].errorbar(sample_sizes, means, yerr=stds, fmt='o-',\n",
    "                      color=COLORS['blue'], capsize=4, linewidth=2,\n",
    "                      label='Estimated H (mean ± std)')\n",
    "    axes[0].axhline(true_entropy, color=COLORS['red'], linestyle='--',\n",
    "                     linewidth=2, label=f'True H = log₂({true_k}) = {true_entropy:.4f}')\n",
    "    axes[0].set_xlabel('Sample Size')\n",
    "    axes[0].set_ylabel('Entropy (bits)')\n",
    "    axes[0].set_title('Plug-in Entropy Estimator Bias')\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "    # Bias as function of sample size\n",
    "    biases = [true_entropy - m for m in means]\n",
    "    axes[1].plot(sample_sizes, biases, 'o-', color=COLORS['green'], linewidth=2)\n",
    "    axes[1].axhline(0, color=COLORS['gray'], linestyle='--')\n",
    "    axes[1].set_xlabel('Sample Size')\n",
    "    axes[1].set_ylabel('Bias (bits)')\n",
    "    axes[1].set_title('Negative Bias of Plug-in Estimator')\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    # Miller-Madow correction: H_corrected = H_plugin + (K-1)/(2n)\n",
    "    axes[1].plot(sample_sizes, [(true_k - 1) / (2 * n) for n in sample_sizes],\n",
    "                  's--', color=COLORS['orange'], label='Miller-Madow correction')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'True entropy: {true_entropy:.4f} bits')\n",
    "    print(f'Bias at n=10:   {biases[1]:.4f} bits')\n",
    "    print(f'Bias at n=1000: {biases[7]:.4f} bits')\n",
    "    print(f'Miller-Madow correction at n=10: +{(true_k-1)/(2*10):.4f}')\n",
    "\n",
    "\n",
    "entropy_estimation_bias()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data Processing Inequality",
    "",
    "The **data processing inequality** states that processing data cannot",
    "increase information:",
    "",
    "$$X \\to Y \\to Z \\implies I(X; Z) \\leq I(X; Y)$$",
    "",
    "This is fundamental to understanding why deep networks lose information",
    "in later layers and motivates the information bottleneck theory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_data_processing_inequality() -> None:\n",
    "    \"\"\"Empirically verify the data processing inequality.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    # X: original signal (4 classes)\n",
    "    n_samples = 10000\n",
    "    x = rng.choice(4, size=n_samples, p=[0.4, 0.3, 0.2, 0.1])\n",
    "\n",
    "    # Y = noisy observation of X\n",
    "    noise_levels = [0.0, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "    mi_xy_list = []\n",
    "    mi_xz_list = []\n",
    "\n",
    "    for noise in noise_levels:\n",
    "        # Y: noisy version of X (flip with probability 'noise')\n",
    "        flip_mask = rng.random(n_samples) < noise\n",
    "        y = x.copy()\n",
    "        y[flip_mask] = rng.choice(4, size=flip_mask.sum())\n",
    "\n",
    "        # Z: further processed Y (quantize to 2 classes)\n",
    "        z = (y >= 2).astype(int)\n",
    "\n",
    "        # Estimate MI(X; Y)\n",
    "        joint_xy = np.zeros((4, 4))\n",
    "        for i in range(n_samples):\n",
    "            joint_xy[x[i], y[i]] += 1\n",
    "        joint_xy = (joint_xy + 1e-10) / joint_xy.sum()\n",
    "        mi_xy = mutual_information_direct(joint_xy)\n",
    "\n",
    "        # Estimate MI(X; Z)\n",
    "        joint_xz = np.zeros((4, 2))\n",
    "        for i in range(n_samples):\n",
    "            joint_xz[x[i], z[i]] += 1\n",
    "        joint_xz = (joint_xz + 1e-10) / joint_xz.sum()\n",
    "        mi_xz = mutual_information_direct(joint_xz)\n",
    "\n",
    "        mi_xy_list.append(mi_xy)\n",
    "        mi_xz_list.append(mi_xz)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(noise_levels, mi_xy_list, 'o-', color=COLORS['blue'],\n",
    "             linewidth=2, markersize=8, label='I(X; Y) — noisy copy')\n",
    "    ax.plot(noise_levels, mi_xz_list, 's-', color=COLORS['red'],\n",
    "             linewidth=2, markersize=8, label='I(X; Z) — quantized Y')\n",
    "    ax.fill_between(noise_levels, mi_xz_list, mi_xy_list,\n",
    "                     alpha=0.15, color=COLORS['gray'])\n",
    "    ax.set_xlabel('Noise Level', fontsize=12)\n",
    "    ax.set_ylabel('Mutual Information (bits)', fontsize=12)\n",
    "    ax.set_title('Data Processing Inequality: I(X;Z) ≤ I(X;Y)')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Verify inequality holds for all noise levels\n",
    "    for noise, mi_xy_val, mi_xz_val in zip(noise_levels, mi_xy_list, mi_xz_list):\n",
    "        violation = mi_xz_val > mi_xy_val + 1e-10\n",
    "        status = 'VIOLATED!' if violation else 'OK'\n",
    "        print(f'Noise={noise:.2f}: I(X;Y)={mi_xy_val:.4f} >= I(X;Z)={mi_xz_val:.4f} [{status}]')\n",
    "\n",
    "\n",
    "demonstrate_data_processing_inequality()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Benchmarks: Our Implementations vs Library",
    "",
    "We compare speed and accuracy of our implementations against",
    "scipy's implementations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def benchmark_implementations() -> None:\n",
    "    \"\"\"Benchmark our information theory functions against scipy.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    results = []\n",
    "\n",
    "    # Generate test distributions\n",
    "    sizes = [10, 100, 1000, 10000]\n",
    "    n_repeats = 1000\n",
    "\n",
    "    for k in sizes:\n",
    "        p = rng.dirichlet(np.ones(k))\n",
    "        q = rng.dirichlet(np.ones(k))\n",
    "\n",
    "        # Entropy benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_repeats):\n",
    "            h_ours = shannon_entropy(p, base=np.e)\n",
    "        our_time = (time.perf_counter() - start) / n_repeats * 1e6\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_repeats):\n",
    "            h_scipy = sp_stats.entropy(p)\n",
    "        scipy_time = (time.perf_counter() - start) / n_repeats * 1e6\n",
    "\n",
    "        diff = abs(h_ours - h_scipy)\n",
    "        results.append({\n",
    "            'Function': 'Entropy', 'K': k,\n",
    "            'Ours (μs)': f'{our_time:.1f}',\n",
    "            'SciPy (μs)': f'{scipy_time:.1f}',\n",
    "            'Max Diff': f'{diff:.2e}',\n",
    "        })\n",
    "\n",
    "        # KL divergence benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_repeats):\n",
    "            kl_ours = kl_divergence(p, q, base=np.e)\n",
    "        our_time = (time.perf_counter() - start) / n_repeats * 1e6\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_repeats):\n",
    "            kl_scipy = sp_stats.entropy(p, q)  # scipy uses natural log\n",
    "        scipy_time = (time.perf_counter() - start) / n_repeats * 1e6\n",
    "\n",
    "        diff = abs(kl_ours - kl_scipy)\n",
    "        results.append({\n",
    "            'Function': 'KL Divergence', 'K': k,\n",
    "            'Ours (μs)': f'{our_time:.1f}',\n",
    "            'SciPy (μs)': f'{scipy_time:.1f}',\n",
    "            'Max Diff': f'{diff:.2e}',\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "\n",
    "benchmark_implementations()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Common Information Theory Mistakes in ML",
    "",
    "Let's illustrate pitfalls that practitioners frequently encounter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def common_mistakes() -> None:\n",
    "    \"\"\"Demonstrate common information theory mistakes.\"\"\"\n",
    "    print('=== Mistake 1: Treating KL divergence as a symmetric distance ===')\n",
    "    p = np.array([0.9, 0.05, 0.05])\n",
    "    q = np.array([0.33, 0.34, 0.33])\n",
    "    print(f'  p = {p}')\n",
    "    print(f'  q = {q}')\n",
    "    print(f'  KL(p||q) = {kl_divergence(p, q):.4f} bits')\n",
    "    print(f'  KL(q||p) = {kl_divergence(q, p):.4f} bits')\n",
    "    print(f'  Ratio: {kl_divergence(p, q) / kl_divergence(q, p):.1f}x')\n",
    "    print(f'  Use JS divergence if you need symmetry: JS = {js_divergence(p, q):.4f}')\n",
    "    print()\n",
    "\n",
    "    print('=== Mistake 2: Ignoring numerical stability in log probabilities ===')\n",
    "    pred_probs = np.array([1.0, 0.0, 0.0])  # Overconfident prediction\n",
    "    true_label = 1  # But the true label is class 1!\n",
    "    # Without clipping: log(0) = -inf\n",
    "    # With clipping: small but finite loss\n",
    "    eps = 1e-15\n",
    "    clipped = np.clip(pred_probs, eps, 1.0)\n",
    "    loss = -np.log(clipped[true_label])\n",
    "    print(f'  Overconfident wrong prediction: P(correct) = {pred_probs[true_label]}')\n",
    "    print(f'  Without clipping: loss = -log(0) = inf')\n",
    "    print(f'  With clipping (eps={eps}): loss = {loss:.2f}')\n",
    "    print(f'  Lesson: Always use log-softmax or clip probabilities!\\n')\n",
    "\n",
    "    print('=== Mistake 3: Confusing bits and nats ===')\n",
    "    p = np.array([0.5, 0.3, 0.2])\n",
    "    h_bits = shannon_entropy(p, base=2.0)\n",
    "    h_nats = shannon_entropy(p, base=np.e)\n",
    "    print(f'  H(p) in bits: {h_bits:.4f}')\n",
    "    print(f'  H(p) in nats: {h_nats:.4f}')\n",
    "    print(f'  Conversion: bits × ln(2) = nats: {h_bits * np.log(2):.4f} = {h_nats:.4f}')\n",
    "    print(f'  PyTorch uses nats (natural log); most textbooks use bits (log₂)\\n')\n",
    "\n",
    "    print('=== Mistake 4: MI from small samples is biased upward ===')\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    # Independent variables\n",
    "    x = rng.choice(5, size=20)\n",
    "    y = rng.choice(5, size=20)\n",
    "    joint = np.zeros((5, 5))\n",
    "    for i in range(20):\n",
    "        joint[x[i], y[i]] += 1\n",
    "    joint_norm = (joint + 1e-10) / joint.sum()\n",
    "    mi_small = mutual_information_direct(joint_norm)\n",
    "\n",
    "    # Same with large sample\n",
    "    x_large = rng.choice(5, size=100000)\n",
    "    y_large = rng.choice(5, size=100000)\n",
    "    joint_large = np.zeros((5, 5))\n",
    "    for i in range(100000):\n",
    "        joint_large[x_large[i], y_large[i]] += 1\n",
    "    joint_large_norm = (joint_large + 1e-10) / joint_large.sum()\n",
    "    mi_large = mutual_information_direct(joint_large_norm)\n",
    "\n",
    "    print(f'  Independent variables with n=20:     MI = {mi_small:.4f} (should be ~0)')\n",
    "    print(f'  Independent variables with n=100000: MI = {mi_large:.6f} (closer to 0)')\n",
    "    print(f'  Small samples overestimate MI!')\n",
    "\n",
    "\n",
    "common_mistakes()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Error Analysis: When Information Measures Mislead",
    "",
    "Information-theoretic measures have failure modes that can mislead",
    "practitioners. Let's examine specific cases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def error_analysis() -> None:\n",
    "    \"\"\"Analyze failure cases of information-theoretic measures.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Case 1: KL divergence explodes when supports don't overlap\n",
    "    p1 = np.array([0.5, 0.5, 0.0, 0.0])\n",
    "    q1_cases = [\n",
    "        ('Full overlap', np.array([0.4, 0.4, 0.1, 0.1])),\n",
    "        ('Partial overlap', np.array([0.1, 0.1, 0.4, 0.4])),\n",
    "        ('No overlap (clipped)', np.array([0.0, 0.0, 0.5, 0.5])),\n",
    "    ]\n",
    "\n",
    "    kl_vals = []\n",
    "    js_vals_case1 = []\n",
    "    labels_case1 = []\n",
    "    for name, q_case in q1_cases:\n",
    "        kl_val = kl_divergence(p1, q_case)\n",
    "        js_val = js_divergence(p1, q_case)\n",
    "        kl_vals.append(kl_val)\n",
    "        js_vals_case1.append(js_val)\n",
    "        labels_case1.append(name)\n",
    "\n",
    "    x_pos = np.arange(len(labels_case1))\n",
    "    axes[0].bar(x_pos - 0.15, kl_vals, width=0.3, color=COLORS['blue'],\n",
    "                 label='KL(p||q)', alpha=0.7)\n",
    "    axes[0].bar(x_pos + 0.15, js_vals_case1, width=0.3, color=COLORS['green'],\n",
    "                 label='JS(p,q)', alpha=0.7)\n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(labels_case1, fontsize=9)\n",
    "    axes[0].set_ylabel('Divergence (bits)')\n",
    "    axes[0].set_title('Support Mismatch Problem')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Case 2: Cross-entropy sensitive to label noise\n",
    "    n_test = 1000\n",
    "    true_labels = rng.randint(0, 5, n_test)\n",
    "    logits = rng.randn(n_test, 5) * 0.5\n",
    "    for i in range(n_test):\n",
    "        logits[i, true_labels[i]] += 3.0\n",
    "\n",
    "    noise_rates = np.linspace(0, 0.5, 20)\n",
    "    ce_values = []\n",
    "    for noise_rate in noise_rates:\n",
    "        noisy_labels = true_labels.copy()\n",
    "        n_flip = int(noise_rate * n_test)\n",
    "        flip_idx = rng.choice(n_test, n_flip, replace=False)\n",
    "        noisy_labels[flip_idx] = rng.randint(0, 5, n_flip)\n",
    "        ce = categorical_cross_entropy_loss(noisy_labels, logits)\n",
    "        ce_values.append(ce)\n",
    "\n",
    "    axes[1].plot(noise_rates, ce_values, 'o-', color=COLORS['red'], linewidth=2)\n",
    "    axes[1].set_xlabel('Label Noise Rate')\n",
    "    axes[1].set_ylabel('Cross-Entropy Loss')\n",
    "    axes[1].set_title('CE Sensitivity to Label Noise')\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    # Case 3: MI estimation variance with different bin counts\n",
    "    x_cont = rng.randn(500)\n",
    "    y_cont = x_cont + rng.randn(500) * 0.5  # Correlated\n",
    "    bin_counts = [3, 5, 10, 20, 30, 50, 100]\n",
    "    mi_by_bins = []\n",
    "    for n_b in bin_counts:\n",
    "        x_d = np.digitize(x_cont, np.linspace(x_cont.min(), x_cont.max(), n_b + 1)[1:-1])\n",
    "        y_d = np.digitize(y_cont, np.linspace(y_cont.min(), y_cont.max(), n_b + 1)[1:-1])\n",
    "        joint = np.zeros((n_b, n_b))\n",
    "        for i in range(len(x_cont)):\n",
    "            joint[min(x_d[i], n_b - 1), min(y_d[i], n_b - 1)] += 1\n",
    "        joint = (joint + 1e-10) / joint.sum()\n",
    "        mi_by_bins.append(mutual_information_direct(joint))\n",
    "\n",
    "    axes[2].plot(bin_counts, mi_by_bins, 'o-', color=COLORS['purple'], linewidth=2)\n",
    "    axes[2].set_xlabel('Number of Bins')\n",
    "    axes[2].set_ylabel('Estimated MI (bits)')\n",
    "    axes[2].set_title('MI Estimate vs Binning Choice\\n(true MI is constant)')\n",
    "    axes[2].grid(True, alpha=0.2)\n",
    "\n",
    "    plt.suptitle('Error Analysis: Information Theory Failure Modes',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('Key failure modes:')\n",
    "    print('  1. KL divergence → ∞ when supports do not overlap (JS stays bounded)')\n",
    "    print('  2. Cross-entropy is highly sensitive to label noise')\n",
    "    print('  3. Histogram-based MI estimates depend heavily on bin count')\n",
    "\n",
    "\n",
    "error_analysis()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 5 — Summary & Lessons Learned",
    "",
    "### Key Takeaways",
    "",
    "- **Shannon entropy** $H(X) = -\\sum p(x) \\log p(x)$ measures the average",
    "  surprise (uncertainty) in a random variable. It is maximized by the",
    "  uniform distribution and equals 0 for deterministic variables.",
    "",
    "- **Cross-entropy** $H(p, q) = -\\sum p(x) \\log q(x)$ is the standard",
    "  classification loss. Minimizing it is equivalent to minimizing KL",
    "  divergence between true labels and model predictions.",
    "",
    "- **KL divergence** $D_{\\text{KL}}(p \\| q)$ measures distributional",
    "  mismatch. It is non-negative, asymmetric, and appears in VAE losses,",
    "  knowledge distillation, and policy optimization. Use JS divergence",
    "  when you need a symmetric alternative.",
    "",
    "- **Mutual information** $I(X; Y)$ quantifies how much one variable tells",
    "  you about another. It is the gold standard for feature relevance but",
    "  suffers from positive bias in finite samples.",
    "",
    "- The **data processing inequality** ($I(X; Z) \\leq I(X; Y)$ for",
    "  $X \\to Y \\to Z$) explains why deep networks must prioritize preserving",
    "  relevant information through their layers.",
    "",
    "### What's Next",
    "",
    "→ **01-09 (Calculus & Optimization Foundations)** builds on the loss",
    "function perspective developed here, showing how gradients optimize",
    "cross-entropy and other objectives."
   ]
  }
 ]
}