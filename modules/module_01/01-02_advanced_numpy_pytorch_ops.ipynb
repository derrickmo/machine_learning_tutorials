{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations\n",
    "## 1-02: Advanced NumPy & PyTorch Operations\n",
    "\n",
    "**Objective:** Master tensor manipulation operations — stacking, reshaping, transposing, matrix multiplication, einsum notation, and advanced indexing — that form the building blocks of every ML algorithm.\n",
    "\n",
    "**Prerequisites:** 1-01 (Python, NumPy & Tensor Speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 — Setup & Prerequisites\n",
    "\n",
    "This notebook covers the advanced tensor manipulation operations that ML practitioners use daily. We will explore stacking, reshaping, transposing, matrix multiplication, einsum notation, advanced indexing, and in-place aliasing — all in both NumPy and PyTorch. These skills are prerequisites for implementing attention mechanisms (Module 8), convolutional operations (Module 6), and any from-scratch algorithm that requires careful tensor manipulation.\n",
    "\n",
    "**Prerequisites:** 1-01 (Python, NumPy & Tensor Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Reproducibility ─────────────────────────────────────────────────────────\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "NUM_TIMING_RUNS = 50          # Number of timing iterations for benchmarks\n",
    "SMALL_DIM = 4                 # Small dimension for demonstration\n",
    "MEDIUM_DIM = 64               # Medium dimension for practical examples\n",
    "LARGE_DIM = 256               # Larger dimension for performance tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data for Demonstrations\n",
    "\n",
    "We generate a variety of synthetic arrays and tensors that we will use throughout the notebook. Using synthetic data keeps us focused on the operations themselves rather than data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generate synthetic data ──────────────────────────────────────────────────\n",
    "# Small arrays for step-by-step demonstrations\n",
    "array_1d = np.random.randn(6)\n",
    "array_2d = np.random.randn(3, 4)\n",
    "array_3d = np.random.randn(2, 3, 4)\n",
    "\n",
    "# Equivalent PyTorch tensors\n",
    "tensor_1d = torch.from_numpy(array_1d.copy())\n",
    "tensor_2d = torch.from_numpy(array_2d.copy())\n",
    "tensor_3d = torch.from_numpy(array_3d.copy())\n",
    "\n",
    "# Feature vectors for stacking demonstration\n",
    "NUM_SAMPLES = 5\n",
    "FEATURE_DIM = 8\n",
    "feature_vectors = [np.random.randn(FEATURE_DIM) for _ in range(NUM_SAMPLES)]\n",
    "\n",
    "# Larger matrices for performance demonstrations\n",
    "matrix_a = np.random.randn(LARGE_DIM, LARGE_DIM)\n",
    "matrix_b = np.random.randn(LARGE_DIM, LARGE_DIM)\n",
    "\n",
    "print(f\"array_1d shape: {array_1d.shape}\")\n",
    "print(f\"array_2d shape: {array_2d.shape}\")\n",
    "print(f\"array_3d shape: {array_3d.shape}\")\n",
    "print(f\"Feature vectors: {NUM_SAMPLES} vectors of dim {FEATURE_DIM}\")\n",
    "print(f\"Large matrices: {matrix_a.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 — Tensor Operations from Scratch\n",
    "\n",
    "We systematically cover the seven families of tensor operations that every ML practitioner needs: stacking/concatenation, reshaping, transposing, matrix multiplication, einsum, advanced indexing, and in-place aliasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Stacking and Concatenation\n",
    "\n",
    "Stacking and concatenation combine multiple arrays into one. The key distinction is:\n",
    "\n",
    "- **Concatenation** joins arrays along an **existing** axis.\n",
    "- **Stacking** joins arrays along a **new** axis.\n",
    "\n",
    "In NumPy: `np.concatenate`, `np.vstack`, `np.hstack`  \n",
    "In PyTorch: `torch.cat` (concatenate) vs `torch.stack` (stack along new dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── NumPy: vstack, hstack, concatenate ──────────────────────────────────────\n",
    "row_a = np.array([1, 2, 3])\n",
    "row_b = np.array([4, 5, 6])\n",
    "\n",
    "# vstack: stack vertically (along axis=0) — rows become rows of a 2D array\n",
    "vstacked = np.vstack([row_a, row_b])\n",
    "print(f\"vstack result shape: {vstacked.shape}\")\n",
    "print(f\"vstack result:\\n{vstacked}\\n\")\n",
    "\n",
    "# hstack: stack horizontally (along axis=1 for 2D, concatenate for 1D)\n",
    "hstacked = np.hstack([row_a, row_b])\n",
    "print(f\"hstack result shape: {hstacked.shape}\")\n",
    "print(f\"hstack result: {hstacked}\\n\")\n",
    "\n",
    "# concatenate with explicit axis parameter\n",
    "mat_a = np.random.randn(2, 3)\n",
    "mat_b = np.random.randn(2, 3)\n",
    "concat_axis0 = np.concatenate([mat_a, mat_b], axis=0)\n",
    "concat_axis1 = np.concatenate([mat_a, mat_b], axis=1)\n",
    "print(f\"Concatenate axis=0: {mat_a.shape} + {mat_b.shape} -> {concat_axis0.shape}\")\n",
    "print(f\"Concatenate axis=1: {mat_a.shape} + {mat_b.shape} -> {concat_axis1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PyTorch: torch.cat vs torch.stack ────────────────────────────────────────\n",
    "vec_a = torch.tensor([1.0, 2.0, 3.0])\n",
    "vec_b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# torch.cat: concatenate along an EXISTING dimension\n",
    "catted = torch.cat([vec_a, vec_b], dim=0)\n",
    "print(f\"torch.cat (1D): {vec_a.shape} + {vec_b.shape} -> {catted.shape}\")\n",
    "print(f\"  Result: {catted}\\n\")\n",
    "\n",
    "# torch.stack: create a NEW dimension and stack along it\n",
    "stacked = torch.stack([vec_a, vec_b], dim=0)\n",
    "print(f\"torch.stack dim=0: {vec_a.shape} + {vec_b.shape} -> {stacked.shape}\")\n",
    "print(f\"  Result:\\n{stacked}\\n\")\n",
    "\n",
    "stacked_dim1 = torch.stack([vec_a, vec_b], dim=1)\n",
    "print(f\"torch.stack dim=1: {vec_a.shape} + {vec_b.shape} -> {stacked_dim1.shape}\")\n",
    "print(f\"  Result:\\n{stacked_dim1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Practical example: Stacking feature vectors into a batch matrix ──────────\n",
    "# In ML, we frequently collect individual feature vectors and need to form a batch\n",
    "torch_features = [torch.randn(FEATURE_DIM) for _ in range(NUM_SAMPLES)]\n",
    "\n",
    "# Using torch.stack to create a (num_samples, feature_dim) batch matrix\n",
    "batch_matrix = torch.stack(torch_features, dim=0)\n",
    "assert batch_matrix.shape == (NUM_SAMPLES, FEATURE_DIM), (\n",
    "    f\"Expected ({NUM_SAMPLES}, {FEATURE_DIM}), got {batch_matrix.shape}\"\n",
    ")\n",
    "print(f\"Individual feature shape: {torch_features[0].shape}\")\n",
    "print(f\"Batch matrix shape:       {batch_matrix.shape}\")\n",
    "print(f\"Batch matrix (first 2 rows):\\n{batch_matrix[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reshape vs View\n",
    "\n",
    "Both `reshape` and `view` change the shape of a tensor without changing the data. The critical difference lies in **memory layout**:\n",
    "\n",
    "| Operation | Library | Behavior |\n",
    "|-----------|---------|----------|\n",
    "| `np.reshape` | NumPy | Returns a view if possible, a copy otherwise |\n",
    "| `tensor.view()` | PyTorch | **Requires** contiguous memory — fails otherwise |\n",
    "| `tensor.reshape()` | PyTorch | Works like `view` when possible, copies when needed |\n",
    "\n",
    "A tensor is **contiguous** when its elements are stored in a single, unbroken block of memory in row-major (C) order. Operations like `transpose` change strides without moving data, making the tensor non-contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── NumPy reshape ────────────────────────────────────────────────────────────\n",
    "original_np = np.arange(12)\n",
    "print(f\"Original: {original_np}, shape={original_np.shape}\\n\")\n",
    "\n",
    "# Reshape to 3x4 — returns a view (shared memory)\n",
    "reshaped_np = original_np.reshape(3, 4)\n",
    "print(f\"Reshaped to (3,4):\\n{reshaped_np}\")\n",
    "print(f\"Shares memory: {np.shares_memory(original_np, reshaped_np)}\")\n",
    "\n",
    "# Modify the view — original changes too!\n",
    "reshaped_np[0, 0] = 999\n",
    "print(f\"\\nAfter modifying reshaped[0,0] = 999:\")\n",
    "print(f\"  Original[0] = {original_np[0]} (also changed!)\")\n",
    "original_np[0] = 0  # Reset\n",
    "\n",
    "# Shape inference with -1: NumPy infers the unknown dimension\n",
    "auto_reshaped = original_np.reshape(2, -1)  # -1 means \"infer this dimension\"\n",
    "print(f\"\\nreshape(2, -1): {original_np.shape} -> {auto_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PyTorch view vs reshape ──────────────────────────────────────────────────\n",
    "original_pt = torch.arange(12, dtype=torch.float32)\n",
    "print(f\"Original tensor: {original_pt}\")\n",
    "print(f\"Is contiguous: {original_pt.is_contiguous()}\\n\")\n",
    "\n",
    "# view: works on contiguous tensors — returns a view (shared memory)\n",
    "viewed = original_pt.view(3, 4)\n",
    "print(f\"view(3, 4):\\n{viewed}\")\n",
    "print(f\"Same storage: {viewed.data_ptr() == original_pt.data_ptr()}\\n\")\n",
    "\n",
    "# Transpose makes a tensor non-contiguous\n",
    "transposed = viewed.t()  # (3,4) -> (4,3)\n",
    "print(f\"After transpose: shape={transposed.shape}\")\n",
    "print(f\"Is contiguous: {transposed.is_contiguous()}\")\n",
    "\n",
    "# view FAILS on non-contiguous tensor\n",
    "try:\n",
    "    transposed.view(12)\n",
    "except RuntimeError as error:\n",
    "    print(f\"\\nview() fails on non-contiguous tensor:\")\n",
    "    print(f\"  Error: {error}\")\n",
    "\n",
    "# Fix 1: use .contiguous() first, then view\n",
    "fixed_view = transposed.contiguous().view(12)\n",
    "print(f\"\\n.contiguous().view(12): {fixed_view.shape}\")\n",
    "\n",
    "# Fix 2: use .reshape() — handles both contiguous and non-contiguous\n",
    "reshaped_pt = transposed.reshape(12)\n",
    "print(f\".reshape(12):           {reshaped_pt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_contiguity(tensor: torch.Tensor, name: str) -> None:\n",
    "    \"\"\"Print contiguity info, strides, and storage details for a tensor.\n",
    "\n",
    "    Args:\n",
    "        tensor: The tensor to inspect.\n",
    "        name: A descriptive name for display.\n",
    "    \"\"\"\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Shape:      {tuple(tensor.shape)}\")\n",
    "    print(f\"  Strides:    {tensor.stride()}\")\n",
    "    print(f\"  Contiguous: {tensor.is_contiguous()}\")\n",
    "    print(f\"  Data ptr:   {tensor.data_ptr()}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "base = torch.arange(12, dtype=torch.float32).view(3, 4)\n",
    "demonstrate_contiguity(base, \"Original (3x4)\")\n",
    "demonstrate_contiguity(base.t(), \"Transposed (4x3)\")\n",
    "demonstrate_contiguity(base.t().contiguous(), \"Transposed + contiguous()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Transpose Mechanics\n",
    "\n",
    "Transposing swaps dimensions of a tensor. Under the hood, it only changes the **strides** — the actual data in memory stays put. This is why transposed tensors become non-contiguous.\n",
    "\n",
    "| NumPy | PyTorch | Description |\n",
    "|-------|---------|-------------|\n",
    "| `.T` | `.T` | Reverse all dimensions |\n",
    "| `np.transpose(a, axes)` | `.permute(*dims)` | Arbitrary axis reordering |\n",
    "| `np.swapaxes(a, ax1, ax2)` | `.transpose(dim0, dim1)` | Swap exactly two axes |\n",
    "| — | `.t()` | 2D-only transpose |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── NumPy transpose operations ───────────────────────────────────────────────\n",
    "mat = np.arange(12).reshape(3, 4)\n",
    "print(f\"Original (3x4):\\n{mat}\\n\")\n",
    "\n",
    "# .T — simple transpose\n",
    "print(f\".T (4x3):\\n{mat.T}\\n\")\n",
    "\n",
    "# For 3D arrays, .T reverses ALL dimensions\n",
    "arr_3d = np.arange(24).reshape(2, 3, 4)\n",
    "print(f\"3D shape: {arr_3d.shape}\")\n",
    "print(f\"3D .T shape: {arr_3d.T.shape}  (reversed: 4,3,2)\")\n",
    "\n",
    "# np.transpose with explicit axis order\n",
    "permuted_np = np.transpose(arr_3d, (1, 0, 2))  # swap first two axes\n",
    "print(f\"np.transpose(arr, (1,0,2)): {arr_3d.shape} -> {permuted_np.shape}\")\n",
    "\n",
    "# np.swapaxes — swap exactly two axes\n",
    "swapped_np = np.swapaxes(arr_3d, 0, 2)\n",
    "print(f\"np.swapaxes(arr, 0, 2): {arr_3d.shape} -> {swapped_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PyTorch transpose operations ─────────────────────────────────────────────\n",
    "tensor_mat = torch.arange(12, dtype=torch.float32).view(3, 4)\n",
    "print(f\"Original (3x4):\\n{tensor_mat}\\n\")\n",
    "\n",
    "# .t() — 2D only\n",
    "print(f\".t() (4x3):\\n{tensor_mat.t()}\\n\")\n",
    "\n",
    "# .transpose(dim0, dim1) — swap exactly two dimensions\n",
    "tensor_3d = torch.arange(24, dtype=torch.float32).view(2, 3, 4)\n",
    "transposed_pt = tensor_3d.transpose(0, 2)  # swap dim 0 and dim 2\n",
    "print(f\".transpose(0, 2): {tuple(tensor_3d.shape)} -> {tuple(transposed_pt.shape)}\")\n",
    "\n",
    "# .permute(*dims) — arbitrary reordering (most flexible)\n",
    "permuted_pt = tensor_3d.permute(1, 0, 2)  # (2,3,4) -> (3,2,4)\n",
    "print(f\".permute(1, 0, 2): {tuple(tensor_3d.shape)} -> {tuple(permuted_pt.shape)}\")\n",
    "\n",
    "# Verify non-contiguity after transpose\n",
    "print(f\"\\nOriginal contiguous:   {tensor_3d.is_contiguous()}\")\n",
    "print(f\"Transposed contiguous: {transposed_pt.is_contiguous()}\")\n",
    "print(f\"Permuted contiguous:   {permuted_pt.is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Practical: Transposing for matrix multiplication ─────────────────────────\n",
    "# Computing X^T X (common in linear regression normal equation)\n",
    "data_matrix = torch.randn(100, 10)  # 100 samples, 10 features\n",
    "xtx = data_matrix.t() @ data_matrix  # (10, 100) @ (100, 10) -> (10, 10)\n",
    "assert xtx.shape == (10, 10), f\"Expected (10, 10), got {xtx.shape}\"\n",
    "print(f\"X shape: {tuple(data_matrix.shape)}\")\n",
    "print(f\"X^T X shape: {tuple(xtx.shape)}\")\n",
    "print(f\"X^T X is symmetric: {torch.allclose(xtx, xtx.t())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is the most fundamental operation in ML. Different functions handle different cases:\n",
    "\n",
    "**NumPy:**\n",
    "- `np.dot(a, b)` — dot product for 1D, matrix multiply for 2D, sum-product over last/second-to-last axes for higher dims\n",
    "- `np.matmul(a, b)` / `a @ b` — matrix multiply with **broadcasting** for 3D+ tensors\n",
    "\n",
    "**PyTorch:**\n",
    "- `torch.mm(a, b)` — strict 2D matrix multiply only\n",
    "- `torch.matmul(a, b)` / `a @ b` — matrix multiply with broadcasting\n",
    "- `torch.bmm(a, b)` — batched matrix multiply (both inputs must be 3D)\n",
    "\n",
    "For two matrices $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times p}$:\n",
    "\n",
    "$$\\mathbf{C}_{ij} = \\sum_{k=1}^{n} \\mathbf{A}_{ik} \\mathbf{B}_{kj}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── NumPy: dot vs matmul vs @ ───────────────────────────────────────────────\n",
    "vec_x = np.array([1.0, 2.0, 3.0])\n",
    "vec_y = np.array([4.0, 5.0, 6.0])\n",
    "\n",
    "# 1D: dot product\n",
    "print(f\"np.dot (1D): {np.dot(vec_x, vec_y)}\")\n",
    "print(f\"np.matmul (1D): {np.matmul(vec_x, vec_y)}\")\n",
    "print(f\"@ operator (1D): {vec_x @ vec_y}\\n\")\n",
    "\n",
    "# 2D: matrix multiplication — all three are equivalent\n",
    "mat_x = np.random.randn(3, 4)\n",
    "mat_y = np.random.randn(4, 2)\n",
    "print(f\"np.dot (2D):    shape = {np.dot(mat_x, mat_y).shape}\")\n",
    "print(f\"np.matmul (2D): shape = {np.matmul(mat_x, mat_y).shape}\")\n",
    "print(f\"@ operator (2D): shape = {(mat_x @ mat_y).shape}\")\n",
    "print(f\"All equal: {np.allclose(np.dot(mat_x, mat_y), mat_x @ mat_y)}\\n\")\n",
    "\n",
    "# 3D: dot and matmul DIFFER for higher-dimensional arrays\n",
    "batch_x = np.random.randn(2, 3, 4)\n",
    "batch_y = np.random.randn(2, 4, 5)\n",
    "\n",
    "matmul_result = np.matmul(batch_x, batch_y)  # Broadcasts: (2,3,4) @ (2,4,5) -> (2,3,5)\n",
    "dot_result = np.dot(batch_x, batch_y)  # Different behavior for 3D+!\n",
    "\n",
    "print(f\"np.matmul (3D): {batch_x.shape} @ {batch_y.shape} -> {matmul_result.shape}\")\n",
    "print(f\"np.dot (3D):    {batch_x.shape} dot {batch_y.shape} -> {dot_result.shape}\")\n",
    "print(f\"Note: np.dot gives a DIFFERENT shape for 3D tensors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PyTorch: mm vs matmul vs bmm ────────────────────────────────────────────\n",
    "# torch.mm: strictly 2D\n",
    "pt_a = torch.randn(3, 4)\n",
    "pt_b = torch.randn(4, 5)\n",
    "mm_result = torch.mm(pt_a, pt_b)\n",
    "print(f\"torch.mm: ({tuple(pt_a.shape)}) x ({tuple(pt_b.shape)}) -> {tuple(mm_result.shape)}\")\n",
    "\n",
    "# torch.matmul: handles broadcasting for batched operations\n",
    "batch_a = torch.randn(2, 3, 4)\n",
    "batch_b = torch.randn(2, 4, 5)\n",
    "matmul_result_pt = torch.matmul(batch_a, batch_b)\n",
    "print(f\"torch.matmul: ({tuple(batch_a.shape)}) x ({tuple(batch_b.shape)}) -> {tuple(matmul_result_pt.shape)}\")\n",
    "\n",
    "# torch.bmm: strictly batched 3D — both inputs must have same batch size\n",
    "bmm_result = torch.bmm(batch_a, batch_b)\n",
    "print(f\"torch.bmm: ({tuple(batch_a.shape)}) x ({tuple(batch_b.shape)}) -> {tuple(bmm_result.shape)}\")\n",
    "\n",
    "# Verify matmul and bmm give same result for 3D inputs\n",
    "print(f\"\\nmatmul == bmm: {torch.allclose(matmul_result_pt, bmm_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul_manual(\n",
    "    tensor_a: torch.Tensor, tensor_b: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Batch matrix multiply using explicit loops (for understanding).\n",
    "\n",
    "    Multiplies each corresponding pair of matrices in the batch dimension.\n",
    "\n",
    "    Args:\n",
    "        tensor_a: Tensor of shape (batch_size, m, n).\n",
    "        tensor_b: Tensor of shape (batch_size, n, p).\n",
    "\n",
    "    Returns:\n",
    "        Result tensor of shape (batch_size, m, p).\n",
    "    \"\"\"\n",
    "    assert tensor_a.dim() == 3 and tensor_b.dim() == 3, \"Both inputs must be 3D\"\n",
    "    assert tensor_a.shape[0] == tensor_b.shape[0], \"Batch sizes must match\"\n",
    "    assert tensor_a.shape[2] == tensor_b.shape[1], \"Inner dimensions must match\"\n",
    "\n",
    "    batch_size = tensor_a.shape[0]\n",
    "    rows_m = tensor_a.shape[1]\n",
    "    cols_p = tensor_b.shape[2]\n",
    "    result = torch.zeros(batch_size, rows_m, cols_p, dtype=tensor_a.dtype)\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        result[batch_idx] = torch.mm(tensor_a[batch_idx], tensor_b[batch_idx])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Compare manual vs torch.bmm\n",
    "test_a = torch.randn(4, 3, 5)\n",
    "test_b = torch.randn(4, 5, 2)\n",
    "\n",
    "manual_result = batch_matmul_manual(test_a, test_b)\n",
    "bmm_result = torch.bmm(test_a, test_b)\n",
    "\n",
    "print(f\"Manual batch matmul shape: {tuple(manual_result.shape)}\")\n",
    "print(f\"torch.bmm shape:           {tuple(bmm_result.shape)}\")\n",
    "print(f\"Results match: {torch.allclose(manual_result, bmm_result, atol=1e-6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Einsum — The Swiss Army Knife\n",
    "\n",
    "Einstein summation (`einsum`) provides a compact, readable notation for expressing tensor operations. The key idea:\n",
    "\n",
    "- **Repeated indices** are summed over (contraction)\n",
    "- **Output indices** specify which dimensions remain\n",
    "- The `->` arrow separates input subscripts from the output subscript\n",
    "\n",
    "For example, matrix multiplication $\\mathbf{C}_{ik} = \\sum_j \\mathbf{A}_{ij} \\mathbf{B}_{jk}$ is written as `'ij,jk->ik'`.\n",
    "\n",
    "Einsum becomes critical in Module 8 (multi-head attention) and Module 12 (tensor decomposition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Einsum basics ────────────────────────────────────────────────────────────\n",
    "vec_p = torch.tensor([1.0, 2.0, 3.0])\n",
    "vec_q = torch.tensor([4.0, 5.0, 6.0])\n",
    "mat_m = torch.randn(3, 4)\n",
    "mat_n = torch.randn(4, 5)\n",
    "\n",
    "# 1. Vector dot product: sum_i (p_i * q_i)\n",
    "dot_einsum = torch.einsum('i,i->', vec_p, vec_q)\n",
    "dot_explicit = torch.dot(vec_p, vec_q)\n",
    "print(f\"Dot product:\")\n",
    "print(f\"  einsum 'i,i->':  {dot_einsum.item():.4f}\")\n",
    "print(f\"  torch.dot:       {dot_explicit.item():.4f}\\n\")\n",
    "\n",
    "# 2. Outer product: C_ij = p_i * q_j\n",
    "outer_einsum = torch.einsum('i,j->ij', vec_p, vec_q)\n",
    "outer_explicit = torch.outer(vec_p, vec_q)\n",
    "print(f\"Outer product:\")\n",
    "print(f\"  einsum 'i,j->ij': shape={tuple(outer_einsum.shape)}\")\n",
    "print(f\"  Match: {torch.allclose(outer_einsum, outer_explicit)}\\n\")\n",
    "\n",
    "# 3. Matrix multiply: C_ik = sum_j A_ij * B_jk\n",
    "mm_einsum = torch.einsum('ij,jk->ik', mat_m, mat_n)\n",
    "mm_explicit = torch.mm(mat_m, mat_n)\n",
    "print(f\"Matrix multiply:\")\n",
    "print(f\"  einsum 'ij,jk->ik': shape={tuple(mm_einsum.shape)}\")\n",
    "print(f\"  Match: {torch.allclose(mm_einsum, mm_explicit)}\\n\")\n",
    "\n",
    "# 4. Transpose: B_ji = A_ij\n",
    "transpose_einsum = torch.einsum('ij->ji', mat_m)\n",
    "print(f\"Transpose:\")\n",
    "print(f\"  einsum 'ij->ji': shape={tuple(transpose_einsum.shape)}\")\n",
    "print(f\"  Match: {torch.allclose(transpose_einsum, mat_m.t())}\\n\")\n",
    "\n",
    "# 5. Trace: sum_i A_ii\n",
    "square_mat = torch.randn(4, 4)\n",
    "trace_einsum = torch.einsum('ii->', square_mat)\n",
    "trace_explicit = torch.trace(square_mat)\n",
    "print(f\"Trace:\")\n",
    "print(f\"  einsum 'ii->':  {trace_einsum.item():.4f}\")\n",
    "print(f\"  torch.trace:    {trace_explicit.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Batch matrix multiply via einsum ─────────────────────────────────────────\n",
    "batch_p = torch.randn(8, 3, 4)  # 8 matrices of shape (3, 4)\n",
    "batch_q = torch.randn(8, 4, 5)  # 8 matrices of shape (4, 5)\n",
    "\n",
    "bmm_einsum = torch.einsum('bij,bjk->bik', batch_p, batch_q)\n",
    "bmm_explicit = torch.bmm(batch_p, batch_q)\n",
    "\n",
    "print(f\"Batch matmul via einsum 'bij,bjk->bik':\")\n",
    "print(f\"  Shape: {tuple(bmm_einsum.shape)}\")\n",
    "print(f\"  Match: {torch.allclose(bmm_einsum, bmm_explicit, atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Attention-like operation preview (Module 8) ──────────────────────────────\n",
    "# In multi-head attention, we compute: scores = Q @ K^T for each batch and head\n",
    "# Shape: Q, K are (batch, heads, seq_len, d_k)\n",
    "BATCH_SIZE_DEMO = 2\n",
    "NUM_HEADS = 4\n",
    "SEQ_LEN = 6\n",
    "D_K = 8\n",
    "\n",
    "query = torch.randn(BATCH_SIZE_DEMO, NUM_HEADS, SEQ_LEN, D_K)\n",
    "key = torch.randn(BATCH_SIZE_DEMO, NUM_HEADS, SEQ_LEN, D_K)\n",
    "\n",
    "# Using einsum: 'bhqd,bhkd->bhqk' — contract over d dimension\n",
    "attention_scores_einsum = torch.einsum('bhqd,bhkd->bhqk', query, key)\n",
    "\n",
    "# Explicit equivalent: Q @ K^T for each batch and head\n",
    "attention_scores_explicit = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "print(f\"Query shape: {tuple(query.shape)}\")\n",
    "print(f\"Key shape:   {tuple(key.shape)}\")\n",
    "print(f\"Attention scores shape (einsum): {tuple(attention_scores_einsum.shape)}\")\n",
    "print(f\"Match explicit: {torch.allclose(attention_scores_einsum, attention_scores_explicit, atol=1e-5)}\")\n",
    "print(f\"\\nNote: einsum 'bhqd,bhkd->bhqk' is a compact way to express\")\n",
    "print(f\"Q @ K^T across batch and head dimensions simultaneously.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_einsum_vs_explicit(\n",
    "    num_runs: int = NUM_TIMING_RUNS,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare execution time of einsum vs explicit operations.\n",
    "\n",
    "    Benchmarks five common operations to show when einsum is competitive\n",
    "    versus dedicated functions.\n",
    "\n",
    "    Args:\n",
    "        num_runs: Number of timing iterations for each operation.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing comparison results.\n",
    "    \"\"\"\n",
    "    size = 128\n",
    "    mat_x = torch.randn(size, size)\n",
    "    mat_y = torch.randn(size, size)\n",
    "    batch_x = torch.randn(16, size, size)\n",
    "    batch_y = torch.randn(16, size, size)\n",
    "    vec_x = torch.randn(size)\n",
    "    vec_y = torch.randn(size)\n",
    "\n",
    "    operations = [\n",
    "        (\"Dot product\", \"i,i->\", [vec_x, vec_y],\n",
    "         lambda: torch.dot(vec_x, vec_y)),\n",
    "        (\"Matrix multiply\", \"ij,jk->ik\", [mat_x, mat_y],\n",
    "         lambda: torch.mm(mat_x, mat_y)),\n",
    "        (\"Batch matmul\", \"bij,bjk->bik\", [batch_x, batch_y],\n",
    "         lambda: torch.bmm(batch_x, batch_y)),\n",
    "        (\"Trace\", \"ii->\", [mat_x],\n",
    "         lambda: torch.trace(mat_x)),\n",
    "        (\"Transpose\", \"ij->ji\", [mat_x],\n",
    "         lambda: mat_x.t()),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for op_name, subscripts, einsum_inputs, explicit_fn in operations:\n",
    "        # Time einsum\n",
    "        start_time = time.perf_counter()\n",
    "        for _ in range(num_runs):\n",
    "            torch.einsum(subscripts, *einsum_inputs)\n",
    "        einsum_time = (time.perf_counter() - start_time) / num_runs * 1000\n",
    "\n",
    "        # Time explicit\n",
    "        start_time = time.perf_counter()\n",
    "        for _ in range(num_runs):\n",
    "            explicit_fn()\n",
    "        explicit_time = (time.perf_counter() - start_time) / num_runs * 1000\n",
    "\n",
    "        results.append({\n",
    "            \"Operation\": op_name,\n",
    "            \"Einsum (ms)\": round(einsum_time, 4),\n",
    "            \"Explicit (ms)\": round(explicit_time, 4),\n",
    "            \"Ratio (einsum/explicit)\": round(einsum_time / max(explicit_time, 1e-9), 2),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "timing_df = compare_einsum_vs_explicit()\n",
    "print(\"Einsum vs Explicit Operations Timing Comparison:\")\n",
    "print(timing_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Advanced Indexing\n",
    "\n",
    "Beyond simple slicing (`a[0:3]`), NumPy and PyTorch support two powerful indexing modes:\n",
    "\n",
    "1. **Boolean indexing** — use a boolean mask to select elements where the condition is `True`\n",
    "2. **Fancy indexing** — use an integer array to select specific elements by their indices\n",
    "\n",
    "These patterns are used constantly in ML: filtering samples by label, selecting top-k predictions, masking padded tokens, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Boolean indexing ─────────────────────────────────────────────────────────\n",
    "data = np.array([3, -1, 4, -1, 5, 9, -2, 6, 5, 3])\n",
    "\n",
    "# Boolean mask: True where condition holds\n",
    "positive_mask = data > 0\n",
    "print(f\"Data:          {data}\")\n",
    "print(f\"Positive mask: {positive_mask}\")\n",
    "print(f\"Positive vals: {data[positive_mask]}\\n\")\n",
    "\n",
    "# Compound conditions\n",
    "range_mask = (data > 0) & (data < 6)\n",
    "print(f\"Between 0 and 6: {data[range_mask]}\\n\")\n",
    "\n",
    "# PyTorch equivalent\n",
    "tensor_data = torch.tensor(data, dtype=torch.float32)\n",
    "torch_mask = tensor_data > 0\n",
    "print(f\"PyTorch masked_select: {torch.masked_select(tensor_data, torch_mask)}\")\n",
    "print(f\"PyTorch boolean index: {tensor_data[torch_mask]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Fancy indexing (integer array indexing) ──────────────────────────────────\n",
    "data_2d = np.arange(20).reshape(4, 5)\n",
    "print(f\"Data (4x5):\\n{data_2d}\\n\")\n",
    "\n",
    "# Select specific rows\n",
    "row_indices = np.array([0, 2, 3])\n",
    "print(f\"Rows [0, 2, 3]:\\n{data_2d[row_indices]}\\n\")\n",
    "\n",
    "# Select specific elements: (row_i, col_i) pairs\n",
    "row_idx = np.array([0, 1, 2, 3])\n",
    "col_idx = np.array([1, 3, 0, 4])\n",
    "print(f\"Elements at (row, col) pairs: {data_2d[row_idx, col_idx]}\")\n",
    "\n",
    "# PyTorch equivalents\n",
    "tensor_2d = torch.arange(20, dtype=torch.float32).view(4, 5)\n",
    "selected_rows = torch.index_select(tensor_2d, dim=0, index=torch.tensor([0, 2, 3]))\n",
    "print(f\"\\ntorch.index_select (rows 0,2,3):\\n{selected_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── torch.where — element-wise conditional selection ─────────────────────────\n",
    "values = torch.randn(8)\n",
    "print(f\"Values: {values}\")\n",
    "\n",
    "# torch.where(condition, x, y) — select from x where True, y where False\n",
    "clamped = torch.where(values > 0, values, torch.zeros_like(values))\n",
    "print(f\"ReLU via where: {clamped}\")\n",
    "\n",
    "# torch.where(condition) — returns indices where True (like np.where)\n",
    "positive_indices = torch.where(values > 0)\n",
    "print(f\"Positive indices: {positive_indices[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Practical: Selecting samples by class label ──────────────────────────────\n",
    "NUM_SAMPLES_DEMO = 100\n",
    "NUM_FEATURES_DEMO = 5\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# Simulate a dataset with features and labels\n",
    "features = torch.randn(NUM_SAMPLES_DEMO, NUM_FEATURES_DEMO)\n",
    "labels = torch.randint(0, NUM_CLASSES, (NUM_SAMPLES_DEMO,))\n",
    "\n",
    "print(f\"Features shape: {tuple(features.shape)}\")\n",
    "print(f\"Labels shape:   {tuple(labels.shape)}\")\n",
    "print(f\"Class distribution: {[(labels == cls).sum().item() for cls in range(NUM_CLASSES)]}\\n\")\n",
    "\n",
    "# Select all samples belonging to class 1\n",
    "class_1_mask = labels == 1\n",
    "class_1_features = features[class_1_mask]\n",
    "print(f\"Class 1 samples: {class_1_features.shape[0]}\")\n",
    "print(f\"Class 1 mean features: {class_1_features.mean(dim=0)}\\n\")\n",
    "\n",
    "# Per-class statistics using boolean indexing\n",
    "for class_idx in range(NUM_CLASSES):\n",
    "    class_mask = labels == class_idx\n",
    "    class_features = features[class_mask]\n",
    "    print(f\"Class {class_idx}: n={class_features.shape[0]}, \"\n",
    "          f\"mean={class_features.mean():.4f}, std={class_features.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 In-Place Operations and Aliasing\n",
    "\n",
    "**Aliasing** occurs when two variables point to the same underlying memory. Modifying one silently changes the other. This is a frequent source of subtle bugs.\n",
    "\n",
    "**NumPy:** Views (from reshape, slicing, transpose) share memory with the original.  \n",
    "**PyTorch:** In-place operations (those ending with `_`, like `.add_()`, `.mul_()`) modify the tensor directly. This can **break autograd** computation graphs in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── NumPy: Views share memory — aliasing bugs ────────────────────────────────\n",
    "original = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "view_reshaped = original.reshape(2, 3)  # This is a VIEW, not a copy\n",
    "\n",
    "print(f\"Original:  {original}\")\n",
    "print(f\"View (2x3):\\n{view_reshaped}\\n\")\n",
    "\n",
    "# Modifying the view changes the original!\n",
    "view_reshaped[0, 0] = 999\n",
    "print(f\"After setting view[0,0] = 999:\")\n",
    "print(f\"  Original: {original}  <- also changed!\")\n",
    "print(f\"  View:     {view_reshaped[0]}\\n\")\n",
    "\n",
    "# Use .copy() to break the alias\n",
    "original = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "safe_copy = original.reshape(2, 3).copy()  # Independent copy\n",
    "safe_copy[0, 0] = 999\n",
    "print(f\"With .copy():\")\n",
    "print(f\"  Original: {original}  <- unchanged\")\n",
    "print(f\"  Copy:     {safe_copy[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── PyTorch: In-place operations ─────────────────────────────────────────────\n",
    "tensor_orig = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"Original: {tensor_orig}\")\n",
    "print(f\"Data pointer: {tensor_orig.data_ptr()}\\n\")\n",
    "\n",
    "# In-place add: modifies tensor_orig directly\n",
    "tensor_orig.add_(10)  # equivalent to tensor_orig += 10\n",
    "print(f\"After .add_(10): {tensor_orig}\")\n",
    "print(f\"Data pointer (same): {tensor_orig.data_ptr()}\\n\")\n",
    "\n",
    "# Out-of-place add: creates a new tensor\n",
    "tensor_new = tensor_orig.add(100)  # creates new tensor\n",
    "print(f\"After .add(100): {tensor_new}\")\n",
    "print(f\"Original unchanged: {tensor_orig}\")\n",
    "print(f\"Different pointers: {tensor_orig.data_ptr()} vs {tensor_new.data_ptr()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Danger: In-place operations break autograd ───────────────────────────────\n",
    "# This is critical when building neural networks (Modules 5+)\n",
    "\n",
    "# Safe: out-of-place operation preserves the computation graph\n",
    "param_safe = torch.tensor([2.0], requires_grad=True)\n",
    "result_safe = param_safe * 3  # out-of-place\n",
    "result_safe = result_safe + 1  # out-of-place\n",
    "result_safe.backward()\n",
    "print(f\"Safe gradient: {param_safe.grad}\")\n",
    "\n",
    "# Dangerous: in-place operation on a tensor that requires grad\n",
    "param_danger = torch.tensor([2.0], requires_grad=True)\n",
    "result_danger = param_danger * 3\n",
    "try:\n",
    "    result_danger.add_(1)  # in-place on tensor in computation graph\n",
    "    result_danger.backward()\n",
    "    print(f\"Danger gradient: {param_danger.grad}\")\n",
    "except RuntimeError as error:\n",
    "    print(f\"In-place operation error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_aliasing(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> bool:\n",
    "    \"\"\"Check if two tensors share the same underlying storage.\n",
    "\n",
    "    Args:\n",
    "        tensor_a: First tensor.\n",
    "        tensor_b: Second tensor.\n",
    "\n",
    "    Returns:\n",
    "        True if tensors share storage (are aliases), False otherwise.\n",
    "    \"\"\"\n",
    "    return tensor_a.storage().data_ptr() == tensor_b.storage().data_ptr()\n",
    "\n",
    "\n",
    "# Demonstrate alias detection\n",
    "base_tensor = torch.arange(12, dtype=torch.float32)\n",
    "view_tensor = base_tensor.view(3, 4)\n",
    "clone_tensor = base_tensor.clone()\n",
    "reshape_contig = base_tensor.reshape(3, 4)  # contiguous -> view\n",
    "\n",
    "print(f\"base vs view:    alias = {detect_aliasing(base_tensor, view_tensor)}\")\n",
    "print(f\"base vs clone:   alias = {detect_aliasing(base_tensor, clone_tensor)}\")\n",
    "print(f\"base vs reshape: alias = {detect_aliasing(base_tensor, reshape_contig)}\")\n",
    "\n",
    "# Non-contiguous reshape may create a copy\n",
    "transposed_tensor = view_tensor.t()  # non-contiguous\n",
    "reshaped_noncontig = transposed_tensor.reshape(12)  # must copy\n",
    "print(f\"transposed vs reshaped(12): alias = {detect_aliasing(transposed_tensor, reshaped_noncontig)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — Putting It All Together\n",
    "\n",
    "We combine the operations from Part 1 into a reusable `TensorToolkit` class with static methods for common ML tensor manipulation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorToolkit:\n",
    "    \"\"\"Collection of utility methods for safe, common tensor operations.\n",
    "\n",
    "    All methods are static and operate on PyTorch tensors. This class\n",
    "    provides checked versions of operations that are commonly needed in\n",
    "    ML pipelines: batch reshaping, einsum-based attention preview,\n",
    "    bounds-checked indexing, and contiguity verification.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_reshape(\n",
    "        tensor: torch.Tensor, new_shape: tuple[int, ...]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Reshape a batched tensor, preserving the batch dimension.\n",
    "\n",
    "        The first dimension (batch) is kept intact; only the remaining\n",
    "        dimensions are reshaped according to new_shape.\n",
    "\n",
    "        Args:\n",
    "            tensor: Input tensor of shape (batch_size, ...).\n",
    "            new_shape: Target shape for non-batch dimensions.\n",
    "                Use -1 to infer one dimension.\n",
    "\n",
    "        Returns:\n",
    "            Reshaped tensor of shape (batch_size, *new_shape).\n",
    "        \"\"\"\n",
    "        batch_size = tensor.shape[0]\n",
    "        full_shape = (batch_size,) + new_shape\n",
    "        return tensor.reshape(full_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def einsum_attention(\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute scaled dot-product attention using einsum notation.\n",
    "\n",
    "        This is a preview of the attention mechanism covered in Module 8.\n",
    "        Computes: softmax(Q @ K^T / sqrt(d_k)) @ V.\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor of shape (batch, heads, seq_len, d_k).\n",
    "            key: Key tensor of shape (batch, heads, seq_len, d_k).\n",
    "            value: Value tensor of shape (batch, heads, seq_len, d_v).\n",
    "\n",
    "        Returns:\n",
    "            Attention output of shape (batch, heads, seq_len, d_v).\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1]\n",
    "        scale = d_k ** 0.5\n",
    "\n",
    "        # Q @ K^T scaled\n",
    "        scores = torch.einsum('bhqd,bhkd->bhqk', query, key) / scale\n",
    "\n",
    "        # Softmax over key dimension\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = torch.einsum('bhqk,bhkd->bhqd', attention_weights, value)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def safe_index(\n",
    "        tensor: torch.Tensor,\n",
    "        indices: torch.Tensor,\n",
    "        dim: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Index a tensor along a dimension with bounds checking.\n",
    "\n",
    "        Args:\n",
    "            tensor: Input tensor to index.\n",
    "            indices: Integer tensor of indices to select.\n",
    "            dim: Dimension along which to index.\n",
    "\n",
    "        Returns:\n",
    "            Selected elements from the tensor.\n",
    "\n",
    "        Raises:\n",
    "            IndexError: If any index is out of bounds.\n",
    "        \"\"\"\n",
    "        max_idx = tensor.shape[dim]\n",
    "        if indices.max().item() >= max_idx or indices.min().item() < -max_idx:\n",
    "            raise IndexError(\n",
    "                f\"Index out of bounds: indices range [{indices.min().item()}, \"\n",
    "                f\"{indices.max().item()}] for dimension {dim} with size {max_idx}\"\n",
    "            )\n",
    "        return torch.index_select(tensor, dim=dim, index=indices)\n",
    "\n",
    "    @staticmethod\n",
    "    def check_contiguous(tensor: torch.Tensor, fix: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Check if a tensor is contiguous and optionally fix it.\n",
    "\n",
    "        Args:\n",
    "            tensor: Input tensor to check.\n",
    "            fix: If True, return a contiguous copy when non-contiguous.\n",
    "                If False, raise an error for non-contiguous tensors.\n",
    "\n",
    "        Returns:\n",
    "            The original tensor if contiguous, or a contiguous copy.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If tensor is non-contiguous and fix=False.\n",
    "        \"\"\"\n",
    "        if tensor.is_contiguous():\n",
    "            return tensor\n",
    "        if fix:\n",
    "            return tensor.contiguous()\n",
    "        raise RuntimeError(\n",
    "            f\"Tensor with shape {tuple(tensor.shape)} and strides \"\n",
    "            f\"{tensor.stride()} is not contiguous.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sanity check: TensorToolkit ──────────────────────────────────────────────\n",
    "toolkit = TensorToolkit()\n",
    "\n",
    "# Test batch_reshape\n",
    "batch_tensor = torch.randn(8, 3, 32, 32)  # (B, C, H, W) image batch\n",
    "flat = toolkit.batch_reshape(batch_tensor, (-1,))\n",
    "assert flat.shape == (8, 3072), f\"Expected (8, 3072), got {flat.shape}\"\n",
    "print(f\"batch_reshape: {tuple(batch_tensor.shape)} -> {tuple(flat.shape)} [PASS]\")\n",
    "\n",
    "# Test einsum_attention\n",
    "test_query = torch.randn(2, 4, 6, 8)\n",
    "test_key = torch.randn(2, 4, 6, 8)\n",
    "test_value = torch.randn(2, 4, 6, 8)\n",
    "attn_out = toolkit.einsum_attention(test_query, test_key, test_value)\n",
    "assert attn_out.shape == (2, 4, 6, 8), f\"Expected (2,4,6,8), got {attn_out.shape}\"\n",
    "print(f\"einsum_attention: Q{tuple(test_query.shape)} -> {tuple(attn_out.shape)} [PASS]\")\n",
    "\n",
    "# Test safe_index\n",
    "test_data = torch.randn(10, 5)\n",
    "selected = toolkit.safe_index(test_data, torch.tensor([0, 3, 7]), dim=0)\n",
    "assert selected.shape == (3, 5), f\"Expected (3, 5), got {selected.shape}\"\n",
    "print(f\"safe_index: rows [0,3,7] from (10,5) -> {tuple(selected.shape)} [PASS]\")\n",
    "\n",
    "# Test safe_index with out-of-bounds\n",
    "try:\n",
    "    toolkit.safe_index(test_data, torch.tensor([0, 15]), dim=0)\n",
    "    print(\"safe_index: out-of-bounds NOT caught [FAIL]\")\n",
    "except IndexError:\n",
    "    print(\"safe_index: out-of-bounds correctly caught [PASS]\")\n",
    "\n",
    "# Test check_contiguous\n",
    "non_contig = torch.randn(3, 4).t()  # non-contiguous\n",
    "fixed = toolkit.check_contiguous(non_contig, fix=True)\n",
    "assert fixed.is_contiguous(), \"Should be contiguous after fix\"\n",
    "print(f\"check_contiguous: non-contiguous -> contiguous [PASS]\")\n",
    "\n",
    "try:\n",
    "    toolkit.check_contiguous(non_contig, fix=False)\n",
    "    print(\"check_contiguous: should have raised [FAIL]\")\n",
    "except RuntimeError:\n",
    "    print(\"check_contiguous: correctly raised for fix=False [PASS]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — Application to ML Data Patterns\n",
    "\n",
    "Now we apply these operations to realistic ML scenarios. Each example demonstrates a pattern you will encounter repeatedly when building models in later modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Image Batch Manipulation\n",
    "\n",
    "In convolutional neural networks (Module 6), images are stored as tensors of shape $(B, C, H, W)$ — batch, channels, height, width. Many operations require flattening the spatial dimensions, e.g., before passing to a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Image batch: (B, C, H, W) -> (B, C*H*W) flattening ─────────────────────\n",
    "BATCH = 16\n",
    "CHANNELS = 3\n",
    "HEIGHT = 32\n",
    "WIDTH = 32\n",
    "\n",
    "image_batch = torch.randn(BATCH, CHANNELS, HEIGHT, WIDTH)\n",
    "print(f\"Image batch shape (B,C,H,W): {tuple(image_batch.shape)}\")\n",
    "\n",
    "# Method 1: reshape\n",
    "flat_reshape = image_batch.reshape(BATCH, -1)\n",
    "print(f\"Flattened (reshape):  {tuple(flat_reshape.shape)}\")\n",
    "\n",
    "# Method 2: view\n",
    "flat_view = image_batch.view(BATCH, -1)\n",
    "print(f\"Flattened (view):     {tuple(flat_view.shape)}\")\n",
    "\n",
    "# Method 3: torch.flatten with start_dim\n",
    "flat_flatten = torch.flatten(image_batch, start_dim=1)\n",
    "print(f\"Flattened (flatten):  {tuple(flat_flatten.shape)}\")\n",
    "\n",
    "# Verify all methods give the same result\n",
    "assert torch.allclose(flat_reshape, flat_view)\n",
    "assert torch.allclose(flat_reshape, flat_flatten)\n",
    "assert flat_reshape.shape == (BATCH, CHANNELS * HEIGHT * WIDTH)\n",
    "print(f\"\\nAll methods equivalent: True\")\n",
    "print(f\"Feature dimension: {CHANNELS} * {HEIGHT} * {WIDTH} = {CHANNELS * HEIGHT * WIDTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Batch Manipulation\n",
    "\n",
    "Recurrent neural networks (Module 7) often expect input in time-major format $(T, B, D)$ — time steps, batch, features — while data is typically stored as $(B, T, D)$. Transposing between these formats is a daily operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sequence batch: (B, T, D) <-> (T, B, D) ─────────────────────────────────\n",
    "BATCH_SEQ = 8\n",
    "TIME_STEPS = 20\n",
    "EMBED_DIM = 64\n",
    "\n",
    "# Batch-major format (common for data loading)\n",
    "batch_major = torch.randn(BATCH_SEQ, TIME_STEPS, EMBED_DIM)\n",
    "print(f\"Batch-major (B,T,D): {tuple(batch_major.shape)}\")\n",
    "\n",
    "# Convert to time-major format for RNN processing\n",
    "# Method 1: transpose\n",
    "time_major_transpose = batch_major.transpose(0, 1)\n",
    "print(f\"Time-major (transpose): {tuple(time_major_transpose.shape)}\")\n",
    "\n",
    "# Method 2: permute\n",
    "time_major_permute = batch_major.permute(1, 0, 2)\n",
    "print(f\"Time-major (permute):   {tuple(time_major_permute.shape)}\")\n",
    "\n",
    "# Verify equivalence\n",
    "assert torch.allclose(time_major_transpose, time_major_permute)\n",
    "assert time_major_transpose.shape == (TIME_STEPS, BATCH_SEQ, EMBED_DIM)\n",
    "print(f\"\\nBoth methods equivalent: True\")\n",
    "\n",
    "# Note: transposed tensor is NOT contiguous\n",
    "print(f\"Time-major contiguous: {time_major_transpose.is_contiguous()}\")\n",
    "# If a downstream operation needs contiguous data:\n",
    "time_major_contig = time_major_transpose.contiguous()\n",
    "print(f\"After .contiguous():   {time_major_contig.is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Matrix Operations with Boolean Indexing\n",
    "\n",
    "Class-conditional analysis is common in ML: computing per-class statistics, visualizing class distributions, or implementing class-balanced sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Class-conditional analysis with boolean indexing ──────────────────────────\n",
    "NUM_SAMPLES_FEAT = 200\n",
    "NUM_FEATURES_FEAT = 10\n",
    "NUM_CLASSES_FEAT = 4\n",
    "\n",
    "# Generate synthetic feature matrix and labels\n",
    "feature_matrix = torch.randn(NUM_SAMPLES_FEAT, NUM_FEATURES_FEAT)\n",
    "class_labels = torch.randint(0, NUM_CLASSES_FEAT, (NUM_SAMPLES_FEAT,))\n",
    "\n",
    "# Compute per-class means using boolean indexing\n",
    "class_means = torch.zeros(NUM_CLASSES_FEAT, NUM_FEATURES_FEAT)\n",
    "class_counts = torch.zeros(NUM_CLASSES_FEAT, dtype=torch.long)\n",
    "\n",
    "for class_idx in range(NUM_CLASSES_FEAT):\n",
    "    mask = class_labels == class_idx\n",
    "    class_counts[class_idx] = mask.sum()\n",
    "    class_means[class_idx] = feature_matrix[mask].mean(dim=0)\n",
    "\n",
    "print(f\"Feature matrix: {tuple(feature_matrix.shape)}\")\n",
    "print(f\"Class counts: {class_counts.tolist()}\")\n",
    "print(f\"\\nPer-class mean of first 3 features:\")\n",
    "for class_idx in range(NUM_CLASSES_FEAT):\n",
    "    means = class_means[class_idx, :3]\n",
    "    print(f\"  Class {class_idx} (n={class_counts[class_idx].item()}): \"\n",
    "          f\"[{means[0]:.4f}, {means[1]:.4f}, {means[2]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualize class-conditional feature distributions ─────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for feat_idx in range(3):\n",
    "    ax = axes[feat_idx]\n",
    "    for class_idx in range(NUM_CLASSES_FEAT):\n",
    "        mask = class_labels == class_idx\n",
    "        values = feature_matrix[mask, feat_idx].numpy()\n",
    "        ax.hist(values, bins=15, alpha=0.5, label=f\"Class {class_idx}\")\n",
    "    ax.set_xlabel(\"Feature Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Feature {feat_idx} Distribution\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Pairwise Distances via Einsum\n",
    "\n",
    "Computing pairwise distances between samples is a core operation in algorithms like k-NN, k-means, and kernel methods. We can express this efficiently with einsum.\n",
    "\n",
    "For two sets of vectors $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and $\\mathbf{Y} \\in \\mathbb{R}^{m \\times d}$, the squared Euclidean distance matrix is:\n",
    "\n",
    "$$D_{ij}^2 = \\|\\mathbf{x}_i - \\mathbf{y}_j\\|^2 = \\|\\mathbf{x}_i\\|^2 - 2\\mathbf{x}_i \\cdot \\mathbf{y}_j + \\|\\mathbf{y}_j\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_squared_distance_einsum(\n",
    "    tensor_x: torch.Tensor, tensor_y: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute pairwise squared Euclidean distances using einsum.\n",
    "\n",
    "    Uses the expansion ||x-y||^2 = ||x||^2 - 2*x.y + ||y||^2\n",
    "    where the cross term is computed efficiently via einsum.\n",
    "\n",
    "    Args:\n",
    "        tensor_x: First set of vectors, shape (n, d).\n",
    "        tensor_y: Second set of vectors, shape (m, d).\n",
    "\n",
    "    Returns:\n",
    "        Distance matrix of shape (n, m) where entry (i,j) is\n",
    "        the squared Euclidean distance between x_i and y_j.\n",
    "    \"\"\"\n",
    "    # ||x_i||^2 for each x_i\n",
    "    x_sq = torch.einsum('id,id->i', tensor_x, tensor_x)  # shape (n,)\n",
    "    # ||y_j||^2 for each y_j\n",
    "    y_sq = torch.einsum('jd,jd->j', tensor_y, tensor_y)  # shape (m,)\n",
    "    # Cross term: x_i . y_j\n",
    "    cross = torch.einsum('id,jd->ij', tensor_x, tensor_y)  # shape (n, m)\n",
    "\n",
    "    # ||x_i - y_j||^2 = ||x_i||^2 - 2*x_i.y_j + ||y_j||^2\n",
    "    distances = x_sq.unsqueeze(1) - 2 * cross + y_sq.unsqueeze(0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def pairwise_squared_distance_loop(\n",
    "    tensor_x: torch.Tensor, tensor_y: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute pairwise squared Euclidean distances using loops (baseline).\n",
    "\n",
    "    Args:\n",
    "        tensor_x: First set of vectors, shape (n, d).\n",
    "        tensor_y: Second set of vectors, shape (m, d).\n",
    "\n",
    "    Returns:\n",
    "        Distance matrix of shape (n, m).\n",
    "    \"\"\"\n",
    "    num_x = tensor_x.shape[0]\n",
    "    num_y = tensor_y.shape[0]\n",
    "    distances = torch.zeros(num_x, num_y)\n",
    "    for idx_i in range(num_x):\n",
    "        for idx_j in range(num_y):\n",
    "            diff = tensor_x[idx_i] - tensor_y[idx_j]\n",
    "            distances[idx_i, idx_j] = torch.dot(diff, diff)\n",
    "    return distances\n",
    "\n",
    "\n",
    "# Compare einsum vs loop vs library\n",
    "points_x = torch.randn(50, 10)\n",
    "points_y = torch.randn(30, 10)\n",
    "\n",
    "dist_einsum = pairwise_squared_distance_einsum(points_x, points_y)\n",
    "dist_loop = pairwise_squared_distance_loop(points_x, points_y)\n",
    "dist_cdist = torch.cdist(points_x, points_y, p=2) ** 2  # library version\n",
    "\n",
    "print(f\"Distance matrix shape: {tuple(dist_einsum.shape)}\")\n",
    "print(f\"Einsum vs loop match: {torch.allclose(dist_einsum, dist_loop, atol=1e-4)}\")\n",
    "print(f\"Einsum vs cdist match: {torch.allclose(dist_einsum, dist_cdist, atol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualize the pairwise distance matrix ───────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "im = ax.imshow(dist_einsum.numpy(), cmap='viridis', aspect='auto')\n",
    "ax.set_xlabel(\"Y Samples\")\n",
    "ax.set_ylabel(\"X Samples\")\n",
    "ax.set_title(\"Pairwise Squared Euclidean Distance Matrix\")\n",
    "plt.colorbar(im, ax=ax, label=\"Squared Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 — Evaluation & Analysis\n",
    "\n",
    "We now evaluate the performance characteristics of the operations covered, analyze memory behavior, build a comprehensive reference table, and demonstrate common bugs with their fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Performance Comparison: Einsum vs Explicit vs Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pairwise_distance(num_points: int, num_dims: int) -> dict[str, float]:\n",
    "    \"\"\"Benchmark three approaches to pairwise distance computation.\n",
    "\n",
    "    Args:\n",
    "        num_points: Number of points in each set.\n",
    "        num_dims: Dimensionality of each point.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping method name to execution time in milliseconds.\n",
    "    \"\"\"\n",
    "    pts_x = torch.randn(num_points, num_dims)\n",
    "    pts_y = torch.randn(num_points, num_dims)\n",
    "\n",
    "    timings = {}\n",
    "\n",
    "    # Einsum approach\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(NUM_TIMING_RUNS):\n",
    "        pairwise_squared_distance_einsum(pts_x, pts_y)\n",
    "    timings[\"Einsum\"] = (time.perf_counter() - start) / NUM_TIMING_RUNS * 1000\n",
    "\n",
    "    # torch.cdist (library)\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(NUM_TIMING_RUNS):\n",
    "        torch.cdist(pts_x, pts_y, p=2)\n",
    "    timings[\"torch.cdist\"] = (time.perf_counter() - start) / NUM_TIMING_RUNS * 1000\n",
    "\n",
    "    # Loop approach (only for small sizes)\n",
    "    if num_points <= 50:\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(min(NUM_TIMING_RUNS, 5)):\n",
    "            pairwise_squared_distance_loop(pts_x, pts_y)\n",
    "        timings[\"Loop\"] = (time.perf_counter() - start) / min(NUM_TIMING_RUNS, 5) * 1000\n",
    "    else:\n",
    "        timings[\"Loop\"] = float('nan')\n",
    "\n",
    "    return timings\n",
    "\n",
    "\n",
    "# Run benchmarks at different scales\n",
    "scales = [(20, 10), (50, 50), (100, 50), (200, 100)]\n",
    "benchmark_results = []\n",
    "\n",
    "for num_pts, num_d in scales:\n",
    "    timings = benchmark_pairwise_distance(num_pts, num_d)\n",
    "    benchmark_results.append({\n",
    "        \"Points\": num_pts,\n",
    "        \"Dims\": num_d,\n",
    "        \"Einsum (ms)\": round(timings[\"Einsum\"], 4),\n",
    "        \"torch.cdist (ms)\": round(timings[\"torch.cdist\"], 4),\n",
    "        \"Loop (ms)\": round(timings[\"Loop\"], 4) if not np.isnan(timings[\"Loop\"]) else \"N/A\",\n",
    "    })\n",
    "\n",
    "benchmark_df = pd.DataFrame(benchmark_results)\n",
    "print(\"Pairwise Distance Performance Comparison:\")\n",
    "print(benchmark_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Memory Analysis: Views vs Copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_sharing(\n",
    "    original: torch.Tensor, operations: dict[str, torch.Tensor]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Analyze memory sharing between an original tensor and derived tensors.\n",
    "\n",
    "    Args:\n",
    "        original: The base tensor.\n",
    "        operations: Dictionary mapping operation names to result tensors.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame summarizing memory relationship for each operation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    original_ptr = original.storage().data_ptr()\n",
    "    original_size = original.storage().nbytes()\n",
    "\n",
    "    for op_name, result_tensor in operations.items():\n",
    "        shares_memory = result_tensor.storage().data_ptr() == original_ptr\n",
    "        result_size = result_tensor.storage().nbytes()\n",
    "        results.append({\n",
    "            \"Operation\": op_name,\n",
    "            \"Shape\": str(tuple(result_tensor.shape)),\n",
    "            \"Contiguous\": result_tensor.is_contiguous(),\n",
    "            \"Shares Storage\": shares_memory,\n",
    "            \"Extra Memory\": \"None\" if shares_memory else f\"{result_size} bytes\",\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Create test tensor and apply various operations\n",
    "base = torch.randn(100, 200)\n",
    "\n",
    "derived_ops = {\n",
    "    \"view(200, 100)\": base.view(200, 100),\n",
    "    \"reshape(200, 100)\": base.reshape(200, 100),\n",
    "    \".t() (transpose)\": base.t(),\n",
    "    \".t().contiguous()\": base.t().contiguous(),\n",
    "    \".clone()\": base.clone(),\n",
    "    \"slice [:50]\": base[:50],\n",
    "    \"flatten()\": base.flatten(),\n",
    "}\n",
    "\n",
    "memory_df = analyze_memory_sharing(base, derived_ops)\n",
    "print(f\"Base tensor: shape={tuple(base.shape)}, storage={base.storage().nbytes()} bytes\\n\")\n",
    "print(\"Memory Sharing Analysis:\")\n",
    "print(memory_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Operations Reference Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Comprehensive operations reference table ─────────────────────────────────\n",
    "reference_data = [\n",
    "    {\"Operation\": \"Concatenate\",\n",
    "     \"NumPy\": \"np.concatenate(arrs, axis)\",\n",
    "     \"PyTorch\": \"torch.cat(tensors, dim)\",\n",
    "     \"Creates New Dim\": \"No\",\n",
    "     \"When to Use\": \"Joining arrays along existing dimension\"},\n",
    "    {\"Operation\": \"Stack\",\n",
    "     \"NumPy\": \"np.stack(arrs, axis)\",\n",
    "     \"PyTorch\": \"torch.stack(tensors, dim)\",\n",
    "     \"Creates New Dim\": \"Yes\",\n",
    "     \"When to Use\": \"Creating batch from individual samples\"},\n",
    "    {\"Operation\": \"Vertical stack\",\n",
    "     \"NumPy\": \"np.vstack(arrs)\",\n",
    "     \"PyTorch\": \"torch.vstack(tensors)\",\n",
    "     \"Creates New Dim\": \"If 1D\",\n",
    "     \"When to Use\": \"Stacking rows\"},\n",
    "    {\"Operation\": \"Horizontal stack\",\n",
    "     \"NumPy\": \"np.hstack(arrs)\",\n",
    "     \"PyTorch\": \"torch.hstack(tensors)\",\n",
    "     \"Creates New Dim\": \"No\",\n",
    "     \"When to Use\": \"Stacking columns\"},\n",
    "    {\"Operation\": \"Reshape\",\n",
    "     \"NumPy\": \"arr.reshape(shape)\",\n",
    "     \"PyTorch\": \"tensor.reshape(shape)\",\n",
    "     \"Creates New Dim\": \"—\",\n",
    "     \"When to Use\": \"Safe reshape (copies if needed)\"},\n",
    "    {\"Operation\": \"View\",\n",
    "     \"NumPy\": \"arr.reshape (view if possible)\",\n",
    "     \"PyTorch\": \"tensor.view(shape)\",\n",
    "     \"Creates New Dim\": \"—\",\n",
    "     \"When to Use\": \"Guaranteed view (fails if non-contiguous)\"},\n",
    "    {\"Operation\": \"Transpose (2D)\",\n",
    "     \"NumPy\": \"arr.T\",\n",
    "     \"PyTorch\": \"tensor.t() or .T\",\n",
    "     \"Creates New Dim\": \"No\",\n",
    "     \"When to Use\": \"Swap rows and columns\"},\n",
    "    {\"Operation\": \"Permute dims\",\n",
    "     \"NumPy\": \"np.transpose(arr, axes)\",\n",
    "     \"PyTorch\": \"tensor.permute(*dims)\",\n",
    "     \"Creates New Dim\": \"No\",\n",
    "     \"When to Use\": \"Arbitrary axis reordering\"},\n",
    "    {\"Operation\": \"Matrix multiply\",\n",
    "     \"NumPy\": \"np.matmul(a, b) or a @ b\",\n",
    "     \"PyTorch\": \"torch.matmul(a, b) or a @ b\",\n",
    "     \"Creates New Dim\": \"—\",\n",
    "     \"When to Use\": \"General matrix multiply with broadcasting\"},\n",
    "    {\"Operation\": \"Batch matmul\",\n",
    "     \"NumPy\": \"np.matmul(a, b)\",\n",
    "     \"PyTorch\": \"torch.bmm(a, b)\",\n",
    "     \"Creates New Dim\": \"—\",\n",
    "     \"When to Use\": \"Batched 3D matmul (strict)\"},\n",
    "    {\"Operation\": \"Einsum\",\n",
    "     \"NumPy\": \"np.einsum(subscripts, *ops)\",\n",
    "     \"PyTorch\": \"torch.einsum(subscripts, *ops)\",\n",
    "     \"Creates New Dim\": \"Varies\",\n",
    "     \"When to Use\": \"Complex tensor contractions, attention\"},\n",
    "    {\"Operation\": \"Boolean index\",\n",
    "     \"NumPy\": \"arr[mask]\",\n",
    "     \"PyTorch\": \"tensor[mask] or masked_select\",\n",
    "     \"Creates New Dim\": \"—\",\n",
    "     \"When to Use\": \"Filtering by condition\"},\n",
    "    {\"Operation\": \"Fancy index\",\n",
    "     \"NumPy\": \"arr[idx_array]\",\n",
    "     \"PyTorch\": \"tensor[idx] or index_select\",\n",
    "     \"Creates New Dim\": \"—\",\n",
    "     \"When to Use\": \"Selecting specific elements by index\"},\n",
    "]\n",
    "\n",
    "reference_df = pd.DataFrame(reference_data)\n",
    "print(\"Operations Reference Table:\")\n",
    "print(reference_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Common Bugs and Their Fixes\n",
    "\n",
    "Three common bugs arise from misunderstanding tensor operations. We demonstrate each one and show how to diagnose and fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bug 1: Aliasing — Modifying a view silently changes the original ─────────\n",
    "print(\"=\" * 60)\n",
    "print(\"BUG 1: Aliasing (Shared Memory)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: You reshape a weight matrix and accidentally modify the original\n",
    "weights = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "print(f\"Original weights: {weights}\")\n",
    "\n",
    "# Bug: view creates an alias\n",
    "reshaped_weights = weights.view(2, 3)\n",
    "reshaped_weights[0, 0] = 999.0\n",
    "print(f\"After modifying view[0,0]: weights = {weights}\")\n",
    "print(f\"  -> Original was corrupted!\\n\")\n",
    "\n",
    "# Fix: use .clone() to create an independent copy\n",
    "weights = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "safe_reshaped = weights.view(2, 3).clone()\n",
    "safe_reshaped[0, 0] = 999.0\n",
    "print(f\"Fix: clone() before modifying\")\n",
    "print(f\"  Original weights: {weights}  <- unchanged\")\n",
    "print(f\"  Cloned view:      {safe_reshaped[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bug 2: Non-contiguous view failure ───────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"BUG 2: Non-Contiguous View Failure\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: You transpose a tensor and try to view it\n",
    "tensor_bug2 = torch.arange(12, dtype=torch.float32).view(3, 4)\n",
    "transposed_bug2 = tensor_bug2.t()  # (4, 3) — non-contiguous!\n",
    "\n",
    "print(f\"Transposed shape: {tuple(transposed_bug2.shape)}\")\n",
    "print(f\"Contiguous: {transposed_bug2.is_contiguous()}\")\n",
    "\n",
    "try:\n",
    "    flat_bug2 = transposed_bug2.view(-1)\n",
    "    print(\"view() succeeded (unexpected)\")\n",
    "except RuntimeError as error:\n",
    "    print(f\"view() fails: {error}\\n\")\n",
    "\n",
    "# Fix 1: Use .contiguous() before .view()\n",
    "flat_fix1 = transposed_bug2.contiguous().view(-1)\n",
    "print(f\"Fix 1 — .contiguous().view(-1): {flat_fix1}\")\n",
    "\n",
    "# Fix 2: Use .reshape() which handles non-contiguous tensors\n",
    "flat_fix2 = transposed_bug2.reshape(-1)\n",
    "print(f\"Fix 2 — .reshape(-1):          {flat_fix2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bug 3: Shape mismatch in matrix multiplication ───────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"BUG 3: Shape Mismatch in Matmul\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: Forgetting to transpose when computing X^T X\n",
    "feature_data = torch.randn(100, 10)  # 100 samples, 10 features\n",
    "\n",
    "try:\n",
    "    wrong_result = torch.mm(feature_data, feature_data)  # (100,10) x (100,10) -> error!\n",
    "except RuntimeError as error:\n",
    "    print(f\"Shape mismatch: {error}\\n\")\n",
    "\n",
    "# Fix: Transpose the first operand\n",
    "correct_result = torch.mm(feature_data.t(), feature_data)  # (10,100) x (100,10) -> (10,10)\n",
    "print(f\"Fix: X.t() @ X gives shape {tuple(correct_result.shape)}\")\n",
    "\n",
    "# Diagnostic tip: always print shapes before matmul\n",
    "def safe_matmul(\n",
    "    tensor_a: torch.Tensor, tensor_b: torch.Tensor, verbose: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Matrix multiply with shape validation and helpful error messages.\n",
    "\n",
    "    Args:\n",
    "        tensor_a: Left operand.\n",
    "        tensor_b: Right operand.\n",
    "        verbose: If True, print shape information.\n",
    "\n",
    "    Returns:\n",
    "        Result of matrix multiplication.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If inner dimensions do not match.\n",
    "    \"\"\"\n",
    "    if tensor_a.shape[-1] != tensor_b.shape[-2]:\n",
    "        raise ValueError(\n",
    "            f\"Shape mismatch: {tuple(tensor_a.shape)} x {tuple(tensor_b.shape)}. \"\n",
    "            f\"Inner dims {tensor_a.shape[-1]} != {tensor_b.shape[-2]}. \"\n",
    "            f\"Did you forget to transpose?\"\n",
    "        )\n",
    "    if verbose:\n",
    "        result_shape = tuple(tensor_a.shape[:-1]) + (tensor_b.shape[-1],)\n",
    "        print(f\"Matmul: {tuple(tensor_a.shape)} x {tuple(tensor_b.shape)} -> {result_shape}\")\n",
    "    return torch.matmul(tensor_a, tensor_b)\n",
    "\n",
    "\n",
    "# Test the safe version\n",
    "result = safe_matmul(feature_data.t(), feature_data)\n",
    "\n",
    "try:\n",
    "    safe_matmul(feature_data, feature_data)\n",
    "except ValueError as error:\n",
    "    print(f\"Caught: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualize: einsum vs explicit ops across matrix sizes ─────────────────────\n",
    "sizes = [16, 32, 64, 128, 256]\n",
    "einsum_times = []\n",
    "explicit_times = []\n",
    "\n",
    "for size in sizes:\n",
    "    mat_test_a = torch.randn(size, size)\n",
    "    mat_test_b = torch.randn(size, size)\n",
    "\n",
    "    # Einsum matmul\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(NUM_TIMING_RUNS):\n",
    "        torch.einsum('ij,jk->ik', mat_test_a, mat_test_b)\n",
    "    einsum_times.append((time.perf_counter() - start) / NUM_TIMING_RUNS * 1000)\n",
    "\n",
    "    # Explicit matmul\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(NUM_TIMING_RUNS):\n",
    "        torch.mm(mat_test_a, mat_test_b)\n",
    "    explicit_times.append((time.perf_counter() - start) / NUM_TIMING_RUNS * 1000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(sizes, einsum_times, 'o-', label='einsum', linewidth=2)\n",
    "ax.plot(sizes, explicit_times, 's-', label='torch.mm', linewidth=2)\n",
    "ax.set_xlabel('Matrix Size (NxN)')\n",
    "ax.set_ylabel('Time (ms)')\n",
    "ax.set_title('Matrix Multiplication: einsum vs torch.mm')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Summary of all einsum patterns covered ───────────────────────────────────\n",
    "einsum_patterns = pd.DataFrame([\n",
    "    {\"Pattern\": \"i,i->\", \"Operation\": \"Vector dot product\",\n",
    "     \"Example Shapes\": \"(d,) x (d,) -> scalar\"},\n",
    "    {\"Pattern\": \"i,j->ij\", \"Operation\": \"Outer product\",\n",
    "     \"Example Shapes\": \"(m,) x (n,) -> (m,n)\"},\n",
    "    {\"Pattern\": \"ij,jk->ik\", \"Operation\": \"Matrix multiply\",\n",
    "     \"Example Shapes\": \"(m,n) x (n,p) -> (m,p)\"},\n",
    "    {\"Pattern\": \"ij->ji\", \"Operation\": \"Transpose\",\n",
    "     \"Example Shapes\": \"(m,n) -> (n,m)\"},\n",
    "    {\"Pattern\": \"ii->\", \"Operation\": \"Trace\",\n",
    "     \"Example Shapes\": \"(n,n) -> scalar\"},\n",
    "    {\"Pattern\": \"bij,bjk->bik\", \"Operation\": \"Batch matmul\",\n",
    "     \"Example Shapes\": \"(B,m,n) x (B,n,p) -> (B,m,p)\"},\n",
    "    {\"Pattern\": \"bhqd,bhkd->bhqk\", \"Operation\": \"Attention scores\",\n",
    "     \"Example Shapes\": \"(B,H,Q,d) x (B,H,K,d) -> (B,H,Q,K)\"},\n",
    "    {\"Pattern\": \"id,jd->ij\", \"Operation\": \"Pairwise dot products\",\n",
    "     \"Example Shapes\": \"(n,d) x (m,d) -> (n,m)\"},\n",
    "])\n",
    "\n",
    "print(\"Einsum Pattern Reference:\")\n",
    "print(einsum_patterns.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5 — Summary & Lessons Learned\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **reshape/view return views (shared memory) when possible** — understand that modifying a view changes the original. Use `.clone()` when you need an independent copy. PyTorch's `.view()` requires contiguous memory while `.reshape()` handles both cases.\n",
    "\n",
    "2. **einsum provides a unified notation for nearly all tensor operations** — from dot products (`'i,i->'`) to attention scores (`'bhqd,bhkd->bhqk'`). It is readable, general, and performs competitively with dedicated functions.\n",
    "\n",
    "3. **Advanced indexing enables efficient data selection without loops** — boolean masks filter by condition, fancy indexing selects by position, and `torch.where` provides element-wise conditional logic. These are essential for class-conditional analysis and data preprocessing.\n",
    "\n",
    "4. **In-place operations save memory but can break autograd graphs** — operations ending with `_` (like `.add_()`, `.mul_()`) modify tensors in place. Use them only when you are certain the tensor is not part of a computation graph that needs gradients.\n",
    "\n",
    "5. **Transposing changes strides, not data** — after transpose, the tensor is non-contiguous. This is efficient (no data copy) but means `.view()` will fail. Understanding contiguity is essential for debugging shape-related errors.\n",
    "\n",
    "### What's Next\n",
    "\n",
    "- **1-03 (Pandas for Tabular Data)** applies data manipulation skills to real-world tabular datasets with Pandas DataFrames.\n",
    "- **1-05 (Data Loading with PyTorch)** builds directly on tensor operations to create Dataset and DataLoader pipelines.\n",
    "- **1-06 (Linear Algebra for ML)** uses matrix multiplication and decompositions introduced here for eigendecomposition and SVD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}