{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01 — Mathematical & Programming Foundations",
    "## 01-02: Advanced NumPy & PyTorch Operations",
    "",
    "**Objective:** Master the tensor manipulation operations — reshape, einsum,",
    "advanced indexing, and in-place ops — that form the backbone of every ML/DL",
    "implementation in this course.",
    "",
    "**Prerequisites:** 01-01 (Python, NumPy & Tensor Speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 0 — Setup & Prerequisites",
    "",
    "In 01-01 we learned *why* vectorization matters and measured the speed gap between",
    "Python loops and NumPy/PyTorch. Now we learn *how* to manipulate tensor shapes and",
    "dimensions so that vectorized operations can replace loops in complex scenarios.",
    "",
    "We will cover:",
    "- **Reshaping & dimension manipulation** — reshape, view, squeeze, unsqueeze, permute, transpose",
    "- **Advanced indexing** — fancy indexing, boolean masks, `np.where`, scatter/gather",
    "- **Einsum notation** — a universal language for tensor contractions",
    "- **In-place operations** — when they help, when they break autograd",
    "- **Stacking & concatenation** — combining tensors along new or existing axes",
    "",
    "These operations appear in virtually every notebook from Module 2 onward.",
    "",
    "**Prerequisites:** 01-01 (Python, NumPy & Tensor Speed)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Imports ──────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA: {torch.version.cuda}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Reproducibility ──────────────────────────────────────────────────────────\n",
    "import random\n",
    "\n",
    "SEED = 1103\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Configuration ────────────────────────────────────────────────────────────\n",
    "# Visualization\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Timing helper — reused from 01-01\n",
    "def measure_time(\n",
    "    func: callable,\n",
    "    num_warmup: int = 2,\n",
    "    num_timed: int = 5,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Measure execution time of a zero-argument callable.\n",
    "\n",
    "    Args:\n",
    "        func: Zero-argument callable to benchmark.\n",
    "        num_warmup: Number of warmup runs before timing.\n",
    "        num_timed: Number of timed runs to average.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (mean_seconds, std_seconds).\n",
    "    \"\"\"\n",
    "    for _ in range(num_warmup):\n",
    "        func()\n",
    "    times: list[float] = []\n",
    "    for _ in range(num_timed):\n",
    "        start = time.perf_counter()\n",
    "        func()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "    return float(np.mean(times)), float(np.std(times))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 1 — Tensor Manipulation from Scratch",
    "",
    "Tensor manipulation is the art of rearranging data without changing the underlying",
    "values. In ML, we constantly need to:",
    "",
    "- Reshape a flat vector into a batch of images",
    "- Transpose matrices for matrix multiplication compatibility",
    "- Select specific elements based on conditions",
    "- Contract tensors along specific dimensions",
    "",
    "We'll build up from basic reshaping to the powerful einsum notation, implementing",
    "each operation from scratch to understand what it does under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reshape & View: Changing Tensor Geometry",
    "",
    "Reshaping changes how we *interpret* the same block of memory. No data is copied —",
    "only the shape metadata and strides change. This makes reshape essentially free.",
    "",
    "**Key distinction:**",
    "- `np.reshape()` / `tensor.reshape()` — always works, may copy if needed",
    "- `tensor.view()` — PyTorch only, requires contiguous memory (faster guarantee)",
    "- `tensor.contiguous().view()` — safe pattern when view might fail"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_reshape() -> None:\n",
    "    \"\"\"Show reshape operations with shape tracking at each step.\"\"\"\n",
    "    # Start with a 1D array of 24 elements\n",
    "    arr = np.arange(24)\n",
    "    print(f'Original: shape={arr.shape}, strides={arr.strides}')\n",
    "    print(f'  Data: {arr}')\n",
    "    print()\n",
    "\n",
    "    # Reshape to 2D (4 rows × 6 cols)\n",
    "    mat_4x6 = arr.reshape(4, 6)\n",
    "    print(f'reshape(4, 6): shape={mat_4x6.shape}, strides={mat_4x6.strides}')\n",
    "    print(f'  Shares memory: {np.shares_memory(arr, mat_4x6)}')\n",
    "    print(f'{mat_4x6}')\n",
    "    print()\n",
    "\n",
    "    # Reshape to 3D (2 × 3 × 4) — e.g., 2 images, 3 rows, 4 cols\n",
    "    tensor_3d = arr.reshape(2, 3, 4)\n",
    "    print(f'reshape(2, 3, 4): shape={tensor_3d.shape}, strides={tensor_3d.strides}')\n",
    "    print(f'  Element [1, 2, 3] = {tensor_3d[1, 2, 3]}')\n",
    "    print(f'  Flat index: 1×12 + 2×4 + 3 = {1*12 + 2*4 + 3}')\n",
    "    print()\n",
    "\n",
    "    # Using -1 to infer one dimension\n",
    "    auto_shape = arr.reshape(6, -1)  # -1 infers 4\n",
    "    print(f'reshape(6, -1): shape={auto_shape.shape} (-1 inferred as 4)')\n",
    "    print()\n",
    "\n",
    "    # Flatten back\n",
    "    flat = tensor_3d.reshape(-1)\n",
    "    print(f'reshape(-1): shape={flat.shape} (flattened back to 1D)')\n",
    "    assert np.array_equal(flat, arr), 'Flatten should recover original data'\n",
    "\n",
    "\n",
    "demonstrate_reshape()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how PyTorch's `view()` differs from `reshape()`. The key difference",
    "is that `view()` guarantees no data copy — it fails if the tensor isn't contiguous",
    "in memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_view_vs_reshape() -> None:\n",
    "    \"\"\"Compare PyTorch view() and reshape() behavior.\"\"\"\n",
    "    t = torch.arange(24)\n",
    "    print(f'Original: shape={t.shape}, is_contiguous={t.is_contiguous()}')\n",
    "    print()\n",
    "\n",
    "    # view() works on contiguous tensors\n",
    "    v1 = t.view(4, 6)\n",
    "    print(f'view(4, 6): shape={v1.shape}, is_contiguous={v1.is_contiguous()}')\n",
    "    print(f'  data_ptr matches: {t.data_ptr() == v1.data_ptr()}')\n",
    "    print()\n",
    "\n",
    "    # Transpose makes tensor non-contiguous\n",
    "    v2 = v1.t()  # Transpose: (4, 6) → (6, 4)\n",
    "    print(f'After .t(): shape={v2.shape}, is_contiguous={v2.is_contiguous()}')\n",
    "    print(f'  Strides: {v2.stride()}')\n",
    "    print()\n",
    "\n",
    "    # view() fails on non-contiguous tensor\n",
    "    try:\n",
    "        v2.view(-1)\n",
    "    except RuntimeError as e:\n",
    "        print(f'view() on non-contiguous: RuntimeError')\n",
    "        print(f'  {str(e)[:80]}...')\n",
    "    print()\n",
    "\n",
    "    # reshape() works (makes a copy internally)\n",
    "    r1 = v2.reshape(-1)\n",
    "    print(f'reshape() on non-contiguous: shape={r1.shape} (works, may copy)')\n",
    "    print()\n",
    "\n",
    "    # contiguous().view() — safe pattern\n",
    "    safe = v2.contiguous().view(-1)\n",
    "    print(f'contiguous().view(): shape={safe.shape} (always works)')\n",
    "    assert torch.equal(r1, safe)\n",
    "\n",
    "\n",
    "demonstrate_view_vs_reshape()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Squeeze & Unsqueeze: Adding and Removing Size-1 Dimensions",
    "",
    "These operations add or remove dimensions of size 1. They're essential for",
    "making tensors compatible for broadcasting and batch operations.",
    "",
    "Common use cases:",
    "- `unsqueeze(0)` — add a batch dimension to a single sample",
    "- `unsqueeze(-1)` — turn a vector into a column vector",
    "- `squeeze()` — remove all size-1 dimensions from a result"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_squeeze_unsqueeze() -> None:\n",
    "    \"\"\"Show squeeze and unsqueeze operations with practical ML examples.\"\"\"\n",
    "    # Start with a vector (common: a single feature vector)\n",
    "    feature_vec = torch.randn(512)\n",
    "    print(f'Feature vector: shape={feature_vec.shape}')\n",
    "    print()\n",
    "\n",
    "    # unsqueeze(0): Add batch dimension — needed to pass through nn.Module\n",
    "    batched = feature_vec.unsqueeze(0)\n",
    "    print(f'unsqueeze(0) — add batch dim: shape={batched.shape}')\n",
    "    print(f'  Use case: single sample → model expects (batch, features)')\n",
    "    print()\n",
    "\n",
    "    # unsqueeze(-1): Column vector for matrix operations\n",
    "    col_vec = feature_vec.unsqueeze(-1)\n",
    "    print(f'unsqueeze(-1) — column vector: shape={col_vec.shape}')\n",
    "    print()\n",
    "\n",
    "    # Multiple unsqueezes: prepare for broadcasting\n",
    "    # E.g., (C,) → (1, C, 1, 1) for channel-wise operations on images\n",
    "    channel_weights = torch.randn(3)  # RGB channel weights\n",
    "    broadcast_ready = channel_weights.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "    print(f'Channel weights (3,) → {broadcast_ready.shape} for (N, C, H, W) broadcasting')\n",
    "    print()\n",
    "\n",
    "    # Alternative: use [None, :, None, None] indexing (equivalent)\n",
    "    broadcast_alt = channel_weights[None, :, None, None]\n",
    "    print(f'Equivalent via indexing: {broadcast_alt.shape}')\n",
    "    assert torch.equal(broadcast_ready, broadcast_alt)\n",
    "    print()\n",
    "\n",
    "    # squeeze: Remove size-1 dimensions\n",
    "    bloated = torch.randn(1, 3, 1, 5, 1)\n",
    "    squeezed = bloated.squeeze()\n",
    "    print(f'squeeze(): {bloated.shape} → {squeezed.shape}')\n",
    "    print()\n",
    "\n",
    "    # squeeze(dim): Remove only a specific size-1 dimension\n",
    "    partial = bloated.squeeze(0)\n",
    "    print(f'squeeze(0): {bloated.shape} → {partial.shape}')\n",
    "    partial2 = bloated.squeeze(2)\n",
    "    print(f'squeeze(2): {bloated.shape} → {partial2.shape}')\n",
    "    print()\n",
    "\n",
    "    # Shape summary table\n",
    "    operations = pd.DataFrame({\n",
    "        'Operation': ['unsqueeze(0)', 'unsqueeze(-1)', 'unsqueeze(1)',\n",
    "                      'squeeze()', 'squeeze(0)', '[None, :]'],\n",
    "        'Input Shape': ['(D,)', '(D,)', '(N, D)',\n",
    "                        '(1, D, 1)', '(1, D)', '(D,)'],\n",
    "        'Output Shape': ['(1, D)', '(D, 1)', '(N, 1, D)',\n",
    "                         '(D,)', '(D,)', '(1, D)'],\n",
    "        'ML Use Case': [\n",
    "            'Add batch dim for inference',\n",
    "            'Column vector for matmul',\n",
    "            'Add sequence dim',\n",
    "            'Remove all size-1 dims',\n",
    "            'Remove batch dim after inference',\n",
    "            'Same as unsqueeze(0)',\n",
    "        ],\n",
    "    })\n",
    "    print('=== Squeeze/Unsqueeze Reference ===')\n",
    "    print(operations.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_squeeze_unsqueeze()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Transpose & Permute: Reordering Dimensions",
    "",
    "Transpose swaps two dimensions; permute reorders all dimensions at once.",
    "These are critical for converting between data format conventions:",
    "",
    "- Images: `(H, W, C)` ↔ `(C, H, W)` (channels-last ↔ channels-first)",
    "- Sequences: `(batch, seq, features)` ↔ `(seq, batch, features)`",
    "- Attention: rearranging `(batch, heads, seq, d_k)` dimensions",
    "",
    "**Important:** Transpose and permute return *views* — the data stays in place,",
    "only the stride metadata changes. The result is typically non-contiguous."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_transpose_permute() -> None:\n",
    "    \"\"\"Show transpose and permute with shape tracking.\"\"\"\n",
    "    # 2D transpose\n",
    "    mat = torch.arange(12).reshape(3, 4)\n",
    "    print(f'Original: shape={mat.shape}')\n",
    "    print(mat)\n",
    "    print()\n",
    "\n",
    "    # .T shorthand (2D only)\n",
    "    mat_t = mat.T\n",
    "    print(f'.T: shape={mat_t.shape}')\n",
    "    print(mat_t)\n",
    "    print()\n",
    "\n",
    "    # .t() method (2D only, equivalent)\n",
    "    mat_t2 = mat.t()\n",
    "    assert torch.equal(mat_t, mat_t2)\n",
    "    print(f'.t(): same as .T for 2D')\n",
    "    print()\n",
    "\n",
    "    # transpose(dim0, dim1) — works for any number of dimensions\n",
    "    tensor_3d = torch.randn(2, 3, 4)\n",
    "    swapped = tensor_3d.transpose(1, 2)  # Swap dims 1 and 2\n",
    "    print(f'3D transpose(1, 2): {tensor_3d.shape} → {swapped.shape}')\n",
    "    print(f'  Is contiguous: {swapped.is_contiguous()}')\n",
    "    print()\n",
    "\n",
    "    # permute — reorder all dimensions at once\n",
    "    # Common: convert image from (H, W, C) to (C, H, W)\n",
    "    image_hwc = torch.randn(224, 224, 3)  # Height, Width, Channels\n",
    "    image_chw = image_hwc.permute(2, 0, 1)  # Channels, Height, Width\n",
    "    print(f'Image HWC → CHW: {image_hwc.shape} → {image_chw.shape}')\n",
    "    print()\n",
    "\n",
    "    # Batch of images: (N, H, W, C) → (N, C, H, W)\n",
    "    batch_hwc = torch.randn(32, 224, 224, 3)\n",
    "    batch_chw = batch_hwc.permute(0, 3, 1, 2)\n",
    "    print(f'Batch HWC → CHW: {batch_hwc.shape} → {batch_chw.shape}')\n",
    "    print()\n",
    "\n",
    "    # Attention tensor rearrangement\n",
    "    # (batch, seq, num_heads, d_k) → (batch, num_heads, seq, d_k)\n",
    "    attn = torch.randn(8, 64, 12, 64)\n",
    "    attn_rearranged = attn.permute(0, 2, 1, 3)\n",
    "    print(f'Attention rearrange: {attn.shape} → {attn_rearranged.shape}')\n",
    "    print(f'  (batch, seq, heads, d_k) → (batch, heads, seq, d_k)')\n",
    "\n",
    "\n",
    "demonstrate_transpose_permute()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stacking & Concatenation: Combining Tensors",
    "",
    "Two ways to combine tensors:",
    "",
    "- **`cat` / `concatenate`** — join along an *existing* dimension (sizes must match on all other dims)",
    "- **`stack`** — join along a *new* dimension (all tensors must have the same shape)",
    "",
    "These appear constantly: concatenating features, stacking batch elements,",
    "building sequence tensors from individual timesteps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_cat_stack() -> None:\n",
    "    \"\"\"Show concatenation and stacking operations.\"\"\"\n",
    "    # Three (3, 4) matrices\n",
    "    a = torch.ones(3, 4)\n",
    "    b = torch.ones(3, 4) * 2\n",
    "    c = torch.ones(3, 4) * 3\n",
    "    print(f'Input shapes: a={a.shape}, b={b.shape}, c={c.shape}')\n",
    "    print()\n",
    "\n",
    "    # cat along dim=0 (stack rows)\n",
    "    cat0 = torch.cat([a, b, c], dim=0)\n",
    "    print(f'cat(dim=0): {cat0.shape}  (9 rows = 3+3+3)')\n",
    "    print()\n",
    "\n",
    "    # cat along dim=1 (stack columns)\n",
    "    cat1 = torch.cat([a, b, c], dim=1)\n",
    "    print(f'cat(dim=1): {cat1.shape}  (12 cols = 4+4+4)')\n",
    "    print()\n",
    "\n",
    "    # stack creates a NEW dimension\n",
    "    stacked0 = torch.stack([a, b, c], dim=0)\n",
    "    print(f'stack(dim=0): {stacked0.shape}  (new batch dim)')\n",
    "    print()\n",
    "\n",
    "    stacked1 = torch.stack([a, b, c], dim=1)\n",
    "    print(f'stack(dim=1): {stacked1.shape}  (new dim inserted at position 1)')\n",
    "    print()\n",
    "\n",
    "    # Practical: building a batch from individual samples\n",
    "    samples = [torch.randn(3, 32, 32) for _ in range(16)]  # 16 RGB images\n",
    "    batch = torch.stack(samples, dim=0)\n",
    "    print(f'Stack 16 images: list of {samples[0].shape} → {batch.shape}')\n",
    "    print()\n",
    "\n",
    "    # Practical: concatenating feature vectors\n",
    "    visual_features = torch.randn(8, 256)\n",
    "    text_features = torch.randn(8, 128)\n",
    "    combined = torch.cat([visual_features, text_features], dim=1)\n",
    "    print(f'Feature concat: {visual_features.shape} + {text_features.shape} → {combined.shape}')\n",
    "    print()\n",
    "\n",
    "    # NumPy equivalents\n",
    "    np_a = np.ones((3, 4))\n",
    "    np_b = np.ones((3, 4)) * 2\n",
    "    print(f'NumPy concatenate(axis=0): {np.concatenate([np_a, np_b], axis=0).shape}')\n",
    "    print(f'NumPy stack(axis=0): {np.stack([np_a, np_b], axis=0).shape}')\n",
    "    print(f'NumPy vstack: {np.vstack([np_a, np_b]).shape}')\n",
    "    print(f'NumPy hstack: {np.hstack([np_a, np_b]).shape}')\n",
    "\n",
    "\n",
    "demonstrate_cat_stack()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Advanced Indexing: Selecting and Modifying Elements",
    "",
    "Beyond basic slicing, NumPy and PyTorch support **fancy indexing** (integer arrays)",
    "and **boolean indexing** (masks). These are essential for:",
    "",
    "- Selecting specific samples from a batch",
    "- Gathering predictions for specific classes",
    "- Masking out padded positions in sequences",
    "- Implementing attention masks",
    "",
    "**Critical difference from basic slicing:** fancy and boolean indexing always",
    "return *copies*, not views."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_fancy_indexing() -> None:\n",
    "    \"\"\"Show integer (fancy) indexing patterns.\"\"\"\n",
    "    # Sample data: batch of 5 feature vectors, each with 4 features\n",
    "    data = torch.tensor([\n",
    "        [1.0, 2.0, 3.0, 4.0],\n",
    "        [5.0, 6.0, 7.0, 8.0],\n",
    "        [9.0, 10.0, 11.0, 12.0],\n",
    "        [13.0, 14.0, 15.0, 16.0],\n",
    "        [17.0, 18.0, 19.0, 20.0],\n",
    "    ])\n",
    "    print(f'Data shape: {data.shape}')\n",
    "    print()\n",
    "\n",
    "    # Select specific rows (samples)\n",
    "    indices = torch.tensor([0, 2, 4])\n",
    "    selected = data[indices]\n",
    "    print(f'Select rows [0, 2, 4]: shape={selected.shape}')\n",
    "    print(selected)\n",
    "    print()\n",
    "\n",
    "    # Select specific (row, col) pairs\n",
    "    row_idx = torch.tensor([0, 1, 2, 3, 4])\n",
    "    col_idx = torch.tensor([3, 2, 1, 0, 3])\n",
    "    diagonal_like = data[row_idx, col_idx]\n",
    "    print(f'Select (row, col) pairs: shape={diagonal_like.shape}')\n",
    "    print(f'  Values: {diagonal_like}')\n",
    "    print(f'  data[0,3]={data[0,3]:.0f}, data[1,2]={data[1,2]:.0f}, ...')\n",
    "    print()\n",
    "\n",
    "    # Practical: gather predicted class probabilities\n",
    "    # logits shape: (batch, num_classes)\n",
    "    logits = torch.randn(4, 10)  # 4 samples, 10 classes\n",
    "    targets = torch.tensor([3, 7, 1, 5])  # True class for each sample\n",
    "    target_logits = logits[torch.arange(4), targets]\n",
    "    print(f'Gather target logits: logits{list(logits.shape)} → {target_logits.shape}')\n",
    "    print(f'  target_logits[0] = logits[0, {targets[0]}] = {target_logits[0]:.4f}')\n",
    "    print()\n",
    "\n",
    "    # Negative indexing\n",
    "    last_two = data[[-2, -1]]\n",
    "    print(f'Negative indices [-2, -1]: shape={last_two.shape}')\n",
    "    print(last_two)\n",
    "\n",
    "\n",
    "demonstrate_fancy_indexing()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean indexing uses a mask of True/False values to select elements. This is",
    "the vectorized replacement for `if` statements inside loops."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_boolean_indexing() -> None:\n",
    "    \"\"\"Show boolean masking patterns common in ML.\"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "    data = torch.randn(5, 4)\n",
    "    print(f'Data:\\n{data}')\n",
    "    print()\n",
    "\n",
    "    # Basic condition: select positive values\n",
    "    mask = data > 0\n",
    "    print(f'Mask (data > 0):\\n{mask}')\n",
    "    print(f'Positive values: {data[mask]}')\n",
    "    print(f'Count: {mask.sum()} / {mask.numel()}')\n",
    "    print()\n",
    "\n",
    "    # Combining conditions\n",
    "    complex_mask = (data > -0.5) & (data < 0.5)\n",
    "    print(f'Values in (-0.5, 0.5): {data[complex_mask]}')\n",
    "    print()\n",
    "\n",
    "    # Practical: ReLU activation via masking\n",
    "    relu_manual = data.clone()\n",
    "    relu_manual[relu_manual < 0] = 0\n",
    "    relu_torch = torch.relu(data)\n",
    "    print(f'Manual ReLU matches torch.relu: {torch.allclose(relu_manual, relu_torch)}')\n",
    "    print()\n",
    "\n",
    "    # Practical: mask padded positions in a sequence\n",
    "    seq_lengths = torch.tensor([3, 5, 2, 4])  # 4 sequences, max_len=5\n",
    "    max_len = 5\n",
    "    # Create padding mask: True where position < seq_length\n",
    "    positions = torch.arange(max_len).unsqueeze(0)  # (1, max_len)\n",
    "    lengths = seq_lengths.unsqueeze(1)               # (batch, 1)\n",
    "    padding_mask = positions < lengths               # (batch, max_len)\n",
    "    print(f'Sequence lengths: {seq_lengths}')\n",
    "    print(f'Padding mask (True = valid):\\n{padding_mask}')\n",
    "    print()\n",
    "\n",
    "    # Apply mask to zero out padded positions\n",
    "    sequences = torch.randn(4, 5, 8)  # (batch, seq_len, features)\n",
    "    masked_sequences = sequences * padding_mask.unsqueeze(-1)\n",
    "    print(f'Masked sequences shape: {masked_sequences.shape}')\n",
    "    print(f'  Padded positions are zero: '\n",
    "          f'{(masked_sequences[2, 2:, :].abs().sum() == 0).item()}')\n",
    "\n",
    "\n",
    "demonstrate_boolean_indexing()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Conditional Selection: where, clamp, and masked_fill",
    "",
    "These operations apply conditions element-wise without explicit loops:",
    "",
    "- `torch.where(condition, x, y)` — select from `x` where True, `y` where False",
    "- `torch.clamp(x, min, max)` — clip values to a range",
    "- `tensor.masked_fill(mask, value)` — fill masked positions with a constant",
    "",
    "These are building blocks for activation functions, loss clipping, and",
    "attention masking."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_conditional_ops() -> None:\n",
    "    \"\"\"Show conditional tensor operations.\"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "    x = torch.randn(3, 4)\n",
    "    print(f'x:\\n{x}')\n",
    "    print()\n",
    "\n",
    "    # torch.where: element-wise conditional\n",
    "    result = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "    print(f'where(x > 0, x, 0) — manual ReLU:\\n{result}')\n",
    "    print()\n",
    "\n",
    "    # torch.where with different fill values\n",
    "    labels = torch.where(x > 0, torch.ones_like(x), -torch.ones_like(x))\n",
    "    print(f'where(x > 0, 1, -1) — sign function:\\n{labels}')\n",
    "    print()\n",
    "\n",
    "    # torch.clamp: clip to range\n",
    "    clamped = torch.clamp(x, min=-0.5, max=0.5)\n",
    "    print(f'clamp(-0.5, 0.5):\\n{clamped}')\n",
    "    print()\n",
    "\n",
    "    # Practical: gradient clipping simulation\n",
    "    gradients = torch.randn(1000) * 5\n",
    "    clipped = torch.clamp(gradients, min=-1.0, max=1.0)\n",
    "    print(f'Gradient clipping: max before={gradients.abs().max():.2f}, '\n",
    "          f'max after={clipped.abs().max():.2f}')\n",
    "    print()\n",
    "\n",
    "    # masked_fill: used in transformer attention\n",
    "    attention_scores = torch.randn(4, 4)\n",
    "    # Create causal mask (upper triangular = future tokens)\n",
    "    causal_mask = torch.triu(torch.ones(4, 4), diagonal=1).bool()\n",
    "    masked_scores = attention_scores.masked_fill(causal_mask, float('-inf'))\n",
    "    print(f'Causal mask:\\n{causal_mask.int()}')\n",
    "    print(f'Masked attention scores:\\n{masked_scores}')\n",
    "    print(f'  -inf positions will become 0 after softmax')\n",
    "\n",
    "\n",
    "demonstrate_conditional_ops()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Gather & Scatter: Advanced Element Selection",
    "",
    "`torch.gather` and `torch.scatter` are the most powerful (and most confusing)",
    "indexing operations. They select or place elements along a specific dimension",
    "using an index tensor.",
    "",
    "**`gather(input, dim, index)`** — for each position in `index`, look up the",
    "element in `input` at that index along `dim`.",
    "",
    "**`scatter(dim, index, src)`** — the inverse of gather: place elements from",
    "`src` into positions specified by `index` along `dim`.",
    "",
    "These are used in:",
    "- Cross-entropy loss (gathering log-probabilities for target classes)",
    "- One-hot encoding (scattering 1s into class positions)",
    "- Top-k selection in beam search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_gather_scatter() -> None:\n",
    "    \"\"\"Show gather and scatter operations with practical examples.\"\"\"\n",
    "    # ── Gather ──────────────────────────────────────────────────────────────\n",
    "    # Scenario: select the probability of the correct class for each sample\n",
    "    torch.manual_seed(SEED)\n",
    "    probs = torch.softmax(torch.randn(4, 5), dim=1)  # (batch=4, classes=5)\n",
    "    targets = torch.tensor([2, 0, 4, 1])              # Correct class per sample\n",
    "    print(f'Class probabilities (4 samples, 5 classes):')\n",
    "    print(f'{probs}')\n",
    "    print(f'Targets: {targets}')\n",
    "    print()\n",
    "\n",
    "    # Gather: select target class probability for each sample\n",
    "    target_probs = probs.gather(dim=1, index=targets.unsqueeze(1))\n",
    "    print(f'Gathered target probs: {target_probs.squeeze()}')\n",
    "    print(f'  Verify: probs[0, 2] = {probs[0, 2]:.4f}, '\n",
    "          f'gathered[0] = {target_probs[0, 0]:.4f}')\n",
    "    print()\n",
    "\n",
    "    # Equivalent using fancy indexing (simpler but less general)\n",
    "    target_probs_fancy = probs[torch.arange(4), targets]\n",
    "    assert torch.allclose(target_probs.squeeze(), target_probs_fancy)\n",
    "    print(f'Fancy indexing gives same result: True')\n",
    "    print()\n",
    "\n",
    "    # ── Scatter ─────────────────────────────────────────────────────────────\n",
    "    # Scenario: create one-hot encoding\n",
    "    num_classes = 5\n",
    "    labels = torch.tensor([0, 3, 1, 4, 2])\n",
    "    one_hot = torch.zeros(5, num_classes)\n",
    "    one_hot.scatter_(dim=1, index=labels.unsqueeze(1), value=1.0)\n",
    "    print(f'One-hot encoding (scatter):')\n",
    "    print(one_hot)\n",
    "    print()\n",
    "\n",
    "    # Verify against F.one_hot\n",
    "    import torch.nn.functional as F\n",
    "    one_hot_lib = F.one_hot(labels, num_classes=num_classes).float()\n",
    "    assert torch.equal(one_hot, one_hot_lib)\n",
    "    print(f'Matches F.one_hot: True')\n",
    "    print()\n",
    "\n",
    "    # ── Top-k gather ────────────────────────────────────────────────────────\n",
    "    scores = torch.randn(3, 8)  # 3 sequences, vocabulary of 8\n",
    "    topk_vals, topk_idx = scores.topk(3, dim=1)  # Top 3 per sequence\n",
    "    print(f'Top-3 values: {topk_vals.shape}')\n",
    "    print(f'Top-3 indices: {topk_idx}')\n",
    "    print(f'  Gathered back: {scores.gather(1, topk_idx)}')\n",
    "    assert torch.equal(topk_vals, scores.gather(1, topk_idx))\n",
    "\n",
    "\n",
    "demonstrate_gather_scatter()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Einsum: The Universal Tensor Operation",
    "",
    "Einstein summation (`einsum`) is a compact notation for expressing tensor",
    "contractions, transpositions, traces, and outer products in a single string.",
    "",
    "The notation uses subscript labels for each dimension:",
    "- Repeated indices are summed over (contraction)",
    "- Output indices appear on the right side of `→`",
    "- Indices that appear in input but not output are summed out",
    "",
    "Examples:",
    "- `'ij,jk->ik'` — matrix multiplication (sum over j)",
    "- `'ii->'` — trace (sum of diagonal)",
    "- `'ij->ji'` — transpose",
    "- `'i,j->ij'` — outer product",
    "- `'bij,bjk->bik'` — batched matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_einsum_basics() -> None:\n",
    "    \"\"\"Show einsum for common operations, verifying against explicit implementations.\"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # ── Dot product: 'i,i->' ────────────────────────────────────────────────\n",
    "    a = torch.randn(5)\n",
    "    b = torch.randn(5)\n",
    "    dot_einsum = torch.einsum('i,i->', a, b)\n",
    "    dot_manual = torch.dot(a, b)\n",
    "    print(f'Dot product:')\n",
    "    print(f'  einsum: {dot_einsum:.4f}')\n",
    "    print(f'  manual: {dot_manual:.4f}')\n",
    "    assert torch.allclose(dot_einsum, dot_manual)\n",
    "    print()\n",
    "\n",
    "    # ── Matrix multiplication: 'ij,jk->ik' ─────────────────────────────────\n",
    "    A = torch.randn(3, 4)\n",
    "    B = torch.randn(4, 5)\n",
    "    matmul_einsum = torch.einsum('ij,jk->ik', A, B)\n",
    "    matmul_manual = A @ B\n",
    "    print(f'Matrix multiply: {A.shape} × {B.shape} → {matmul_einsum.shape}')\n",
    "    assert torch.allclose(matmul_einsum, matmul_manual)\n",
    "    print(f'  Matches @ operator: True')\n",
    "    print()\n",
    "\n",
    "    # ── Outer product: 'i,j->ij' ────────────────────────────────────────────\n",
    "    x = torch.tensor([1.0, 2.0, 3.0])\n",
    "    y = torch.tensor([4.0, 5.0])\n",
    "    outer_einsum = torch.einsum('i,j->ij', x, y)\n",
    "    outer_manual = x.unsqueeze(1) * y.unsqueeze(0)\n",
    "    print(f'Outer product: ({x.shape[0]},) × ({y.shape[0]},) → {outer_einsum.shape}')\n",
    "    print(outer_einsum)\n",
    "    assert torch.allclose(outer_einsum, outer_manual)\n",
    "    print()\n",
    "\n",
    "    # ── Transpose: 'ij->ji' ─────────────────────────────────────────────────\n",
    "    M = torch.randn(3, 4)\n",
    "    transpose_einsum = torch.einsum('ij->ji', M)\n",
    "    assert torch.allclose(transpose_einsum, M.T)\n",
    "    print(f'Transpose: {M.shape} → {transpose_einsum.shape} (matches .T)')\n",
    "    print()\n",
    "\n",
    "    # ── Trace: 'ii->' ───────────────────────────────────────────────────────\n",
    "    sq = torch.randn(4, 4)\n",
    "    trace_einsum = torch.einsum('ii->', sq)\n",
    "    trace_manual = torch.trace(sq)\n",
    "    print(f'Trace: {trace_einsum:.4f} (matches torch.trace: {trace_manual:.4f})')\n",
    "    print()\n",
    "\n",
    "    # ── Diagonal: 'ii->i' ───────────────────────────────────────────────────\n",
    "    diag_einsum = torch.einsum('ii->i', sq)\n",
    "    diag_manual = torch.diagonal(sq)\n",
    "    print(f'Diagonal: {diag_einsum} (sum = trace = {diag_einsum.sum():.4f})')\n",
    "    assert torch.allclose(diag_einsum, diag_manual)\n",
    "    print()\n",
    "\n",
    "    # ── Element-wise multiply and sum: 'ij,ij->' ────────────────────────────\n",
    "    C = torch.randn(3, 4)\n",
    "    D = torch.randn(3, 4)\n",
    "    hadamard_sum_einsum = torch.einsum('ij,ij->', C, D)\n",
    "    hadamard_sum_manual = (C * D).sum()\n",
    "    print(f'Frobenius inner product: {hadamard_sum_einsum:.4f}')\n",
    "    assert torch.allclose(hadamard_sum_einsum, hadamard_sum_manual)\n",
    "\n",
    "\n",
    "demonstrate_einsum_basics()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the einsum patterns that appear most frequently in deep learning:",
    "batched operations, attention computation, and multi-head projections."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_einsum_ml_patterns() -> None:\n",
    "    \"\"\"Show einsum patterns commonly used in ML/DL implementations.\"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # ── Batched matrix multiplication: 'bij,bjk->bik' ──────────────────────\n",
    "    batch_A = torch.randn(8, 3, 4)  # 8 batches of (3×4) matrices\n",
    "    batch_B = torch.randn(8, 4, 5)  # 8 batches of (4×5) matrices\n",
    "    bmm_einsum = torch.einsum('bij,bjk->bik', batch_A, batch_B)\n",
    "    bmm_torch = torch.bmm(batch_A, batch_B)\n",
    "    print(f'Batched matmul: {batch_A.shape} × {batch_B.shape} → {bmm_einsum.shape}')\n",
    "    assert torch.allclose(bmm_einsum, bmm_torch)\n",
    "    print(f'  Matches torch.bmm: True')\n",
    "    print()\n",
    "\n",
    "    # ── Attention scores: 'bhid,bhjd->bhij' ─────────────────────────────────\n",
    "    batch, heads, seq_q, seq_k, d_k = 2, 4, 6, 8, 16\n",
    "    Q = torch.randn(batch, heads, seq_q, d_k)\n",
    "    K = torch.randn(batch, heads, seq_k, d_k)\n",
    "    attn_scores = torch.einsum('bhid,bhjd->bhij', Q, K)\n",
    "    attn_manual = Q @ K.transpose(-2, -1)\n",
    "    print(f'Attention scores: Q{list(Q.shape)} × K{list(K.shape)} → {list(attn_scores.shape)}')\n",
    "    assert torch.allclose(attn_scores, attn_manual, atol=1e-6)\n",
    "    print(f'  Matches Q @ K^T: True')\n",
    "    print()\n",
    "\n",
    "    # ── Bilinear form: 'bi,ij,bj->b' ────────────────────────────────────────\n",
    "    x = torch.randn(4, 3)\n",
    "    W = torch.randn(3, 3)\n",
    "    y = torch.randn(4, 3)\n",
    "    bilinear = torch.einsum('bi,ij,bj->b', x, W, y)\n",
    "    bilinear_manual = (x @ W * y).sum(dim=1)\n",
    "    print(f'Bilinear form x^T W y: batch result shape={bilinear.shape}')\n",
    "    assert torch.allclose(bilinear, bilinear_manual, atol=1e-5)\n",
    "    print()\n",
    "\n",
    "    # ── Row-wise and column-wise sum ─────────────────────────────────────────\n",
    "    mat = torch.randn(3, 4)\n",
    "    row_sum = torch.einsum('ij->i', mat)  # Sum each row\n",
    "    col_sum = torch.einsum('ij->j', mat)  # Sum each column\n",
    "    total = torch.einsum('ij->', mat)      # Sum all elements\n",
    "    print(f'Row sums: {row_sum} (matches .sum(1): {torch.allclose(row_sum, mat.sum(1))})')\n",
    "    print(f'Col sums: {col_sum} (matches .sum(0): {torch.allclose(col_sum, mat.sum(0))})')\n",
    "    print(f'Total: {total:.4f} (matches .sum(): {torch.allclose(total, mat.sum())})')\n",
    "    print()\n",
    "\n",
    "    # ── Summary table ───────────────────────────────────────────────────────\n",
    "    einsum_ref = pd.DataFrame({\n",
    "        'Pattern': [\n",
    "            'i,i->', 'ij,jk->ik', 'bij,bjk->bik', 'i,j->ij',\n",
    "            'ij->ji', 'ii->', 'ij,ij->', 'bhid,bhjd->bhij',\n",
    "        ],\n",
    "        'Operation': [\n",
    "            'Dot product', 'Matrix multiply', 'Batched matmul', 'Outer product',\n",
    "            'Transpose', 'Trace', 'Frobenius inner', 'Attention scores',\n",
    "        ],\n",
    "        'Equivalent': [\n",
    "            'torch.dot', 'A @ B', 'torch.bmm', 'outer()',\n",
    "            '.T', 'torch.trace', '(A*B).sum()', 'Q @ K.T',\n",
    "        ],\n",
    "    })\n",
    "    print('=== Einsum Quick Reference ===')\n",
    "    print(einsum_ref.to_string(index=False))\n",
    "\n",
    "\n",
    "demonstrate_einsum_ml_patterns()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 In-Place Operations: Speed vs Safety",
    "",
    "In-place operations modify a tensor without allocating new memory. In PyTorch,",
    "they're marked with a trailing underscore: `add_()`, `mul_()`, `relu_()`.",
    "",
    "**Benefits:**",
    "- Save memory (no temporary allocation)",
    "- Slightly faster for large tensors",
    "",
    "**Dangers:**",
    "- **Break autograd** if applied to tensors that require gradients",
    "- **Corrupt views** if another tensor shares the same memory",
    "- **Make debugging harder** — original values are lost",
    "",
    "**Rule of thumb:** Use in-place ops only in inference or data preprocessing.",
    "Never use them during training on tensors involved in gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_inplace_ops() -> None:\n",
    "    \"\"\"Show in-place operations and their gotchas.\"\"\"\n",
    "    # Basic in-place operations\n",
    "    a = torch.tensor([1.0, 2.0, 3.0])\n",
    "    print(f'Before: {a}')\n",
    "    print(f'  id(a.data): {id(a.storage())}')\n",
    "\n",
    "    a.add_(10)  # In-place add\n",
    "    print(f'After add_(10): {a}')\n",
    "    print(f'  id(a.data): {id(a.storage())} (same!)')\n",
    "    print()\n",
    "\n",
    "    # Compare: out-of-place creates new tensor\n",
    "    b = torch.tensor([1.0, 2.0, 3.0])\n",
    "    c = b + 10  # Out-of-place\n",
    "    print(f'Out-of-place: b unchanged = {b}, c = {c}')\n",
    "    print()\n",
    "\n",
    "    # ── Danger 1: Breaking autograd ─────────────────────────────────────────\n",
    "    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "    y = x * 2  # y depends on x through the computation graph\n",
    "    try:\n",
    "        y.add_(1)  # In-place on a tensor needed for backward\n",
    "        print('In-place on grad tensor: succeeded (may cause issues in backward)')\n",
    "    except RuntimeError as e:\n",
    "        print(f'In-place on grad tensor: RuntimeError')\n",
    "        print(f'  {str(e)[:80]}')\n",
    "    print()\n",
    "\n",
    "    # ── Danger 2: Corrupting views ──────────────────────────────────────────\n",
    "    original = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    view_slice = original[1:4]  # View of elements [2, 3, 4]\n",
    "    print(f'original: {original}')\n",
    "    print(f'view:     {view_slice}')\n",
    "\n",
    "    view_slice.mul_(10)  # In-place multiply\n",
    "    print(f'After view.mul_(10):')\n",
    "    print(f'  view:     {view_slice}')\n",
    "    print(f'  original: {original}  ← CORRUPTED!')\n",
    "    print()\n",
    "\n",
    "    # ── Common in-place operations ──────────────────────────────────────────\n",
    "    ops_table = pd.DataFrame({\n",
    "        'In-place': ['add_()', 'mul_()', 'zero_()', 'fill_(v)',\n",
    "                     'clamp_()', 'relu_()', 'uniform_()', 'normal_()'],\n",
    "        'Out-of-place': ['add() / +', 'mul() / *', 'torch.zeros_like()', 'torch.full_like()',\n",
    "                         'torch.clamp()', 'torch.relu()', 'torch.rand_like()', 'torch.randn_like()'],\n",
    "        'Safe in Training?': ['No*', 'No*', 'Yes (param init)', 'Yes (param init)',\n",
    "                              'No*', 'No*', 'Yes (param init)', 'Yes (param init)'],\n",
    "    })\n",
    "    print('=== In-place Operations Reference ===')\n",
    "    print(ops_table.to_string(index=False))\n",
    "    print('* Not safe if tensor requires grad or is used in computation graph')\n",
    "\n",
    "\n",
    "demonstrate_inplace_ops()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 2 — Putting It All Together: TensorOps Toolkit",
    "",
    "We've covered many individual operations. Now let's assemble them into a",
    "reusable `TensorOps` class that provides common tensor manipulation patterns",
    "used throughout this course, with shape validation at each step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TensorOps:\n",
    "    \"\"\"Collection of common tensor manipulation patterns with shape validation.\n",
    "\n",
    "    All methods are static and work with both NumPy arrays and PyTorch tensors.\n",
    "    Each method includes assertion checks to catch shape mismatches early.\n",
    "\n",
    "    Attributes:\n",
    "        None — all methods are static.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_flatten(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Flatten all dimensions except the batch dimension.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, ...).\n",
    "\n",
    "        Returns:\n",
    "            Flattened tensor of shape (batch, product_of_remaining_dims).\n",
    "        \"\"\"\n",
    "        assert x.dim() >= 2, f'Expected at least 2D tensor, got {x.dim()}D'\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_batch_dim(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add a batch dimension at position 0.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of any shape.\n",
    "\n",
    "        Returns:\n",
    "            Tensor with shape (1, *original_shape).\n",
    "        \"\"\"\n",
    "        return x.unsqueeze(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def channels_last_to_first(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert image tensor from (N, H, W, C) to (N, C, H, W).\n",
    "\n",
    "        Args:\n",
    "            x: Image tensor in channels-last format.\n",
    "\n",
    "        Returns:\n",
    "            Image tensor in channels-first format.\n",
    "        \"\"\"\n",
    "        assert x.dim() == 4, f'Expected 4D tensor (N,H,W,C), got {x.dim()}D'\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def channels_first_to_last(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert image tensor from (N, C, H, W) to (N, H, W, C).\n",
    "\n",
    "        Args:\n",
    "            x: Image tensor in channels-first format.\n",
    "\n",
    "        Returns:\n",
    "            Image tensor in channels-last format.\n",
    "        \"\"\"\n",
    "        assert x.dim() == 4, f'Expected 4D tensor (N,C,H,W), got {x.dim()}D'\n",
    "        return x.permute(0, 2, 3, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_padding_mask(\n",
    "        lengths: torch.Tensor,\n",
    "        max_len: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Create a boolean padding mask from sequence lengths.\n",
    "\n",
    "        Args:\n",
    "            lengths: Tensor of sequence lengths, shape (batch,).\n",
    "            max_len: Maximum sequence length.\n",
    "\n",
    "        Returns:\n",
    "            Boolean mask of shape (batch, max_len). True = valid, False = padding.\n",
    "        \"\"\"\n",
    "        positions = torch.arange(max_len, device=lengths.device).unsqueeze(0)\n",
    "        return positions < lengths.unsqueeze(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"Create a causal (lower-triangular) attention mask.\n",
    "\n",
    "        Args:\n",
    "            seq_len: Sequence length.\n",
    "\n",
    "        Returns:\n",
    "            Boolean mask of shape (seq_len, seq_len). True = masked (future).\n",
    "        \"\"\"\n",
    "        return torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(\n",
    "        labels: torch.Tensor,\n",
    "        num_classes: int,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Create one-hot encoding using scatter.\n",
    "\n",
    "        Args:\n",
    "            labels: Integer labels of shape (batch,).\n",
    "            num_classes: Total number of classes.\n",
    "\n",
    "        Returns:\n",
    "            One-hot tensor of shape (batch, num_classes).\n",
    "        \"\"\"\n",
    "        assert labels.dim() == 1, f'Expected 1D labels, got {labels.dim()}D'\n",
    "        one_hot = torch.zeros(labels.shape[0], num_classes, dtype=torch.float32)\n",
    "        one_hot.scatter_(1, labels.unsqueeze(1), 1.0)\n",
    "        return one_hot\n",
    "\n",
    "    @staticmethod\n",
    "    def attention_scores(\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute scaled dot-product attention scores using einsum.\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor of shape (batch, heads, seq_q, d_k).\n",
    "            key: Key tensor of shape (batch, heads, seq_k, d_k).\n",
    "\n",
    "        Returns:\n",
    "            Attention scores of shape (batch, heads, seq_q, seq_k).\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1]\n",
    "        scores = torch.einsum('bhid,bhjd->bhij', query, key)\n",
    "        return scores / (d_k ** 0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that each method in the toolkit works correctly on sample data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def test_tensor_ops() -> None:\n",
    "    \"\"\"Test all TensorOps methods with assertions.\"\"\"\n",
    "    ops = TensorOps()\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # batch_flatten\n",
    "    images = torch.randn(8, 3, 32, 32)\n",
    "    flat = ops.batch_flatten(images)\n",
    "    assert flat.shape == (8, 3 * 32 * 32), f'Expected (8, 3072), got {flat.shape}'\n",
    "    print(f'batch_flatten: {images.shape} → {flat.shape} ✓')\n",
    "\n",
    "    # add_batch_dim\n",
    "    single = torch.randn(3, 32, 32)\n",
    "    batched = ops.add_batch_dim(single)\n",
    "    assert batched.shape == (1, 3, 32, 32)\n",
    "    print(f'add_batch_dim: {single.shape} → {batched.shape} ✓')\n",
    "\n",
    "    # channels_last_to_first\n",
    "    nhwc = torch.randn(8, 32, 32, 3)\n",
    "    nchw = ops.channels_last_to_first(nhwc)\n",
    "    assert nchw.shape == (8, 3, 32, 32)\n",
    "    print(f'channels_last_to_first: {nhwc.shape} → {nchw.shape} ✓')\n",
    "\n",
    "    # channels_first_to_last\n",
    "    back = ops.channels_first_to_last(nchw)\n",
    "    assert back.shape == (8, 32, 32, 3)\n",
    "    assert torch.equal(back, nhwc)\n",
    "    print(f'channels_first_to_last: {nchw.shape} → {back.shape} ✓')\n",
    "\n",
    "    # create_padding_mask\n",
    "    lens = torch.tensor([3, 5, 2])\n",
    "    mask = ops.create_padding_mask(lens, max_len=6)\n",
    "    assert mask.shape == (3, 6)\n",
    "    assert mask[0].sum() == 3\n",
    "    assert mask[1].sum() == 5\n",
    "    print(f'create_padding_mask: lengths={lens.tolist()} → {mask.shape} ✓')\n",
    "\n",
    "    # create_causal_mask\n",
    "    cmask = ops.create_causal_mask(4)\n",
    "    assert cmask.shape == (4, 4)\n",
    "    assert cmask[0, 1] == True   # Future position masked\n",
    "    assert cmask[1, 0] == False  # Past position not masked\n",
    "    print(f'create_causal_mask: seq_len=4 → {cmask.shape} ✓')\n",
    "\n",
    "    # one_hot_encode\n",
    "    labels = torch.tensor([0, 3, 1, 4])\n",
    "    onehot = ops.one_hot_encode(labels, num_classes=5)\n",
    "    assert onehot.shape == (4, 5)\n",
    "    assert onehot[0, 0] == 1.0\n",
    "    assert onehot[1, 3] == 1.0\n",
    "    print(f'one_hot_encode: {labels.tolist()} → {onehot.shape} ✓')\n",
    "\n",
    "    # attention_scores\n",
    "    Q = torch.randn(2, 4, 6, 16)\n",
    "    K = torch.randn(2, 4, 8, 16)\n",
    "    scores = ops.attention_scores(Q, K)\n",
    "    assert scores.shape == (2, 4, 6, 8)\n",
    "    print(f'attention_scores: Q{list(Q.shape)} × K{list(K.shape)} → {list(scores.shape)} ✓')\n",
    "\n",
    "    print()\n",
    "    print('All TensorOps tests passed!')\n",
    "\n",
    "\n",
    "test_tensor_ops()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 3 — Application: Real-World Tensor Manipulation",
    "",
    "Now we apply our tensor manipulation skills to realistic scenarios. We'll work",
    "through three practical problems that combine multiple operations:",
    "",
    "1. **Image data pipeline** — loading, reshaping, and normalizing image batches",
    "2. **Einsum-powered linear layer** — building a neural network layer with einsum",
    "3. **Sequence padding and batching** — preparing variable-length text for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Image Data Pipeline",
    "",
    "When working with image data, we constantly need to convert between formats,",
    "normalize pixel values, and reshape tensors for model input. Let's build a",
    "complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def image_pipeline_demo() -> None:\n",
    "    \"\"\"Demonstrate a complete image tensor manipulation pipeline.\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # Simulate loading 16 images as uint8 arrays in (H, W, C) format\n",
    "    raw_images = np.random.randint(0, 256, size=(16, 64, 64, 3), dtype=np.uint8)\n",
    "    print(f'Raw images: shape={raw_images.shape}, dtype={raw_images.dtype}')\n",
    "    print(f'  Pixel range: [{raw_images.min()}, {raw_images.max()}]')\n",
    "    print()\n",
    "\n",
    "    # Step 1: Convert to float32 and normalize to [0, 1]\n",
    "    images_float = raw_images.astype(np.float32) / 255.0\n",
    "    print(f'Step 1 — Float32 normalize: dtype={images_float.dtype}, '\n",
    "          f'range=[{images_float.min():.2f}, {images_float.max():.2f}]')\n",
    "    print()\n",
    "\n",
    "    # Step 2: Convert to PyTorch tensor\n",
    "    images_tensor = torch.from_numpy(images_float)\n",
    "    print(f'Step 2 — To tensor: {images_tensor.shape}')\n",
    "    print()\n",
    "\n",
    "    # Step 3: Channels-last to channels-first (N, H, W, C) → (N, C, H, W)\n",
    "    images_chw = images_tensor.permute(0, 3, 1, 2)\n",
    "    print(f'Step 3 — Channels first: {images_chw.shape}')\n",
    "    assert images_chw.shape == (16, 3, 64, 64)\n",
    "    print()\n",
    "\n",
    "    # Step 4: Normalize with ImageNet mean/std (per channel)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
    "    images_normalized = (images_chw - mean) / std\n",
    "    print(f'Step 4 — ImageNet normalize: '\n",
    "          f'mean≈{images_normalized.mean(dim=(0,2,3)).tolist()}')\n",
    "    print()\n",
    "\n",
    "    # Step 5: Flatten for a fully-connected layer\n",
    "    images_flat = images_normalized.reshape(16, -1)\n",
    "    print(f'Step 5 — Flatten: {images_flat.shape} '\n",
    "          f'({3*64*64} = 3×64×64 features)')\n",
    "    print()\n",
    "\n",
    "    # Visualize the pipeline\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    for i in range(4):\n",
    "        # Original\n",
    "        axes[0, i].imshow(raw_images[i])\n",
    "        axes[0, i].set_title(f'Raw #{i}')\n",
    "        axes[0, i].axis('off')\n",
    "        # Normalized (undo normalization for display)\n",
    "        img_display = images_chw[i].permute(1, 2, 0).numpy()\n",
    "        axes[1, i].imshow(img_display)\n",
    "        axes[1, i].set_title(f'Float #{i}')\n",
    "        axes[1, i].axis('off')\n",
    "    axes[0, 0].set_ylabel('uint8 [0, 255]', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('float32 [0, 1]', fontsize=11)\n",
    "    plt.suptitle('Image Pipeline: Raw → Float → Channels-first', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Shape tracking summary\n",
    "    pipeline_steps = pd.DataFrame({\n",
    "        'Step': ['Load', 'Normalize', 'To Tensor', 'Permute', 'Normalize (IN)', 'Flatten'],\n",
    "        'Shape': ['(16,64,64,3)', '(16,64,64,3)', '(16,64,64,3)',\n",
    "                  '(16,3,64,64)', '(16,3,64,64)', '(16,12288)'],\n",
    "        'dtype': ['uint8', 'float32', 'float32', 'float32', 'float32', 'float32'],\n",
    "        'Range': ['[0,255]', '[0,1]', '[0,1]', '[0,1]', '~[-2,2]', '~[-2,2]'],\n",
    "    })\n",
    "    print('=== Pipeline Shape Tracking ===')\n",
    "    print(pipeline_steps.to_string(index=False))\n",
    "\n",
    "\n",
    "image_pipeline_demo()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Einsum-Powered Linear Layer",
    "",
    "Let's implement a linear layer (`y = xW + b`) using einsum and compare it",
    "against PyTorch's `nn.Linear`. This demonstrates how einsum replaces both",
    "matrix multiplication and broadcasting in a single expression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def einsum_linear_layer_demo() -> None:\n",
    "    \"\"\"Build and benchmark a linear layer using einsum.\"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # Layer parameters\n",
    "    in_features = 512\n",
    "    out_features = 256\n",
    "    batch_size = 64\n",
    "\n",
    "    # Create weight and bias\n",
    "    W = torch.randn(in_features, out_features) * 0.01\n",
    "    b = torch.zeros(out_features)\n",
    "    x = torch.randn(batch_size, in_features)\n",
    "\n",
    "    # Method 1: Explicit matmul + broadcast\n",
    "    y_matmul = x @ W + b\n",
    "    print(f'Matmul: x{list(x.shape)} @ W{list(W.shape)} + b{list(b.shape)} → {list(y_matmul.shape)}')\n",
    "\n",
    "    # Method 2: Einsum\n",
    "    y_einsum = torch.einsum('bi,io->bo', x, W) + b\n",
    "    print(f'Einsum: \"bi,io->bo\" → {list(y_einsum.shape)}')\n",
    "\n",
    "    # Verify equivalence\n",
    "    assert torch.allclose(y_matmul, y_einsum, atol=1e-5)\n",
    "    print(f'Outputs match: True')\n",
    "    print()\n",
    "\n",
    "    # Method 3: nn.Linear (library reference)\n",
    "    import torch.nn as nn\n",
    "    linear = nn.Linear(in_features, out_features, bias=True)\n",
    "    with torch.no_grad():\n",
    "        linear.weight.copy_(W.T)  # nn.Linear stores weight transposed\n",
    "        linear.bias.copy_(b)\n",
    "    y_nn = linear(x)\n",
    "    assert torch.allclose(y_matmul, y_nn, atol=1e-5)\n",
    "    print(f'Matches nn.Linear: True')\n",
    "    print()\n",
    "\n",
    "    # Benchmark all three\n",
    "    t_matmul, _ = measure_time(lambda: x @ W + b)\n",
    "    t_einsum, _ = measure_time(lambda: torch.einsum('bi,io->bo', x, W) + b)\n",
    "    t_nn, _ = measure_time(lambda: linear(x))\n",
    "\n",
    "    bench_df = pd.DataFrame({\n",
    "        'Method': ['x @ W + b', 'einsum(\"bi,io->bo\")', 'nn.Linear'],\n",
    "        'Time (ms)': [t_matmul * 1000, t_einsum * 1000, t_nn * 1000],\n",
    "        'Relative': [1.0, t_einsum / t_matmul, t_nn / t_matmul],\n",
    "    })\n",
    "    print('=== Linear Layer Benchmark ===')\n",
    "    print(bench_df.to_string(index=False))\n",
    "    print()\n",
    "    print('All three produce identical results. Einsum is typically similar speed')\n",
    "    print('to explicit matmul. nn.Linear may be slightly faster due to fused ops.')\n",
    "\n",
    "\n",
    "einsum_linear_layer_demo()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Sequence Padding and Batching",
    "",
    "In NLP, sequences have variable lengths but models need fixed-size tensors.",
    "We need to pad shorter sequences and create masks that tell the model which",
    "positions are real vs padded. This combines multiple operations we've learned."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def sequence_batching_demo() -> None:\n",
    "    \"\"\"Demonstrate variable-length sequence padding and masking.\"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # Simulate tokenized sequences of different lengths\n",
    "    sequences = [\n",
    "        torch.tensor([4, 12, 7, 23, 1]),        # length 5\n",
    "        torch.tensor([8, 3]),                     # length 2\n",
    "        torch.tensor([15, 6, 9, 2, 11, 4, 7]),  # length 7\n",
    "        torch.tensor([1, 22, 5, 18]),            # length 4\n",
    "    ]\n",
    "    lengths = torch.tensor([len(s) for s in sequences])\n",
    "    print(f'Sequences: {[s.tolist() for s in sequences]}')\n",
    "    print(f'Lengths: {lengths.tolist()}')\n",
    "    print()\n",
    "\n",
    "    # Step 1: Find max length and create padded tensor\n",
    "    max_len = lengths.max().item()\n",
    "    batch_size = len(sequences)\n",
    "    PAD_TOKEN = 0\n",
    "    padded = torch.full((batch_size, max_len), PAD_TOKEN, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    print(f'Padded tensor (pad={PAD_TOKEN}):')\n",
    "    print(padded)\n",
    "    print(f'Shape: {padded.shape}')\n",
    "    print()\n",
    "\n",
    "    # Step 2: Create padding mask\n",
    "    padding_mask = TensorOps.create_padding_mask(lengths, max_len)\n",
    "    print(f'Padding mask (True = valid):')\n",
    "    print(padding_mask.int())\n",
    "    print()\n",
    "\n",
    "    # Step 3: Simulate embedding lookup\n",
    "    vocab_size = 30\n",
    "    embed_dim = 8\n",
    "    embedding_table = torch.randn(vocab_size, embed_dim)\n",
    "    embedded = embedding_table[padded]  # Fancy indexing for lookup!\n",
    "    print(f'Embedded: {padded.shape} → {embedded.shape} (each token → {embed_dim}D vector)')\n",
    "    print()\n",
    "\n",
    "    # Step 4: Apply mask to zero out padded embeddings\n",
    "    mask_expanded = padding_mask.unsqueeze(-1)  # (batch, seq, 1)\n",
    "    embedded_masked = embedded * mask_expanded\n",
    "    print(f'Masked embedding shape: {embedded_masked.shape}')\n",
    "    # Verify padded positions are zeroed\n",
    "    assert embedded_masked[1, 2:, :].abs().sum() == 0, 'Padded positions should be zero'\n",
    "    print(f'Padded positions zeroed: True')\n",
    "    print()\n",
    "\n",
    "    # Step 5: Compute sequence representations (mean of valid tokens)\n",
    "    # Sum valid embeddings, divide by actual lengths\n",
    "    seq_sums = embedded_masked.sum(dim=1)  # (batch, embed_dim)\n",
    "    seq_means = seq_sums / lengths.unsqueeze(1).float()\n",
    "    print(f'Sequence representations: {seq_means.shape}')\n",
    "    print(f'  (Mean pooling over valid tokens only)')\n",
    "    print()\n",
    "\n",
    "    # Shape tracking\n",
    "    steps = pd.DataFrame({\n",
    "        'Step': ['Raw sequences', 'Padded', 'Embedded', 'Mask expanded',\n",
    "                 'Masked embedded', 'Mean pooled'],\n",
    "        'Shape': ['variable', f'({batch_size}, {max_len})',\n",
    "                  f'({batch_size}, {max_len}, {embed_dim})',\n",
    "                  f'({batch_size}, {max_len}, 1)',\n",
    "                  f'({batch_size}, {max_len}, {embed_dim})',\n",
    "                  f'({batch_size}, {embed_dim})'],\n",
    "        'Operation': ['—', 'torch.full + assign', 'embedding[padded]',\n",
    "                      'mask.unsqueeze(-1)', 'element-wise multiply',\n",
    "                      'sum(dim=1) / lengths'],\n",
    "    })\n",
    "    print('=== Sequence Pipeline Shape Tracking ===')\n",
    "    print(steps.to_string(index=False))\n",
    "\n",
    "\n",
    "sequence_batching_demo()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Library Comparison: Our Implementations vs PyTorch Built-ins",
    "",
    "Let's systematically verify that our manual implementations match PyTorch's",
    "built-in operations in both correctness and speed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def library_comparison() -> None:\n",
    "    \"\"\"Compare manual tensor ops against PyTorch built-in equivalents.\"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    comparisons: list[dict] = []\n",
    "\n",
    "    # 1. One-hot encoding\n",
    "    labels = torch.randint(0, 10, (1000,))\n",
    "    our_onehot = TensorOps.one_hot_encode(labels, 10)\n",
    "    lib_onehot = F.one_hot(labels, 10).float()\n",
    "    match = torch.equal(our_onehot, lib_onehot)\n",
    "    t_ours, _ = measure_time(lambda: TensorOps.one_hot_encode(labels, 10))\n",
    "    t_lib, _ = measure_time(lambda: F.one_hot(labels, 10).float())\n",
    "    comparisons.append({\n",
    "        'Operation': 'One-hot encode',\n",
    "        'Match': match,\n",
    "        'Ours (ms)': t_ours * 1000,\n",
    "        'PyTorch (ms)': t_lib * 1000,\n",
    "    })\n",
    "\n",
    "    # 2. Batch flatten\n",
    "    images = torch.randn(32, 3, 32, 32)\n",
    "    our_flat = TensorOps.batch_flatten(images)\n",
    "    lib_flat = torch.flatten(images, start_dim=1)\n",
    "    match = torch.equal(our_flat, lib_flat)\n",
    "    t_ours, _ = measure_time(lambda: TensorOps.batch_flatten(images))\n",
    "    t_lib, _ = measure_time(lambda: torch.flatten(images, start_dim=1))\n",
    "    comparisons.append({\n",
    "        'Operation': 'Batch flatten',\n",
    "        'Match': match,\n",
    "        'Ours (ms)': t_ours * 1000,\n",
    "        'PyTorch (ms)': t_lib * 1000,\n",
    "    })\n",
    "\n",
    "    # 3. Causal mask\n",
    "    our_mask = TensorOps.create_causal_mask(64)\n",
    "    lib_mask = torch.triu(torch.ones(64, 64), diagonal=1).bool()\n",
    "    match = torch.equal(our_mask, lib_mask)\n",
    "    t_ours, _ = measure_time(lambda: TensorOps.create_causal_mask(64))\n",
    "    t_lib, _ = measure_time(lambda: torch.triu(torch.ones(64, 64), diagonal=1).bool())\n",
    "    comparisons.append({\n",
    "        'Operation': 'Causal mask (64×64)',\n",
    "        'Match': match,\n",
    "        'Ours (ms)': t_ours * 1000,\n",
    "        'PyTorch (ms)': t_lib * 1000,\n",
    "    })\n",
    "\n",
    "    # 4. Attention scores\n",
    "    Q = torch.randn(4, 8, 32, 64)\n",
    "    K = torch.randn(4, 8, 32, 64)\n",
    "    our_attn = TensorOps.attention_scores(Q, K)\n",
    "    lib_attn = (Q @ K.transpose(-2, -1)) / (64 ** 0.5)\n",
    "    match = torch.allclose(our_attn, lib_attn, atol=1e-5)\n",
    "    t_ours, _ = measure_time(lambda: TensorOps.attention_scores(Q, K))\n",
    "    t_lib, _ = measure_time(lambda: (Q @ K.transpose(-2, -1)) / (64 ** 0.5))\n",
    "    comparisons.append({\n",
    "        'Operation': 'Attention scores',\n",
    "        'Match': match,\n",
    "        'Ours (ms)': t_ours * 1000,\n",
    "        'PyTorch (ms)': t_lib * 1000,\n",
    "    })\n",
    "\n",
    "    comp_df = pd.DataFrame(comparisons)\n",
    "    print('=== Library Comparison ===')\n",
    "    print(comp_df.to_string(index=False))\n",
    "    print()\n",
    "    all_match = all(c['Match'] for c in comparisons)\n",
    "    print(f'All implementations match PyTorch built-ins: {all_match}')\n",
    "\n",
    "\n",
    "library_comparison()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 4 — Evaluation & Analysis",
    "",
    "Let's analyze the performance characteristics of the operations we've learned,",
    "identify common pitfalls, and build a comprehensive reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Reshape Operations: Performance Impact",
    "",
    "Reshaping is supposed to be free (just metadata change), but some patterns",
    "force copies. Let's measure when reshaping has actual cost."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def benchmark_reshape_patterns() -> pd.DataFrame:\n",
    "    \"\"\"Benchmark various reshape patterns to identify hidden copies.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing and copy status.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "    n = 10_000_000\n",
    "    data = torch.randn(n)\n",
    "    mat = torch.randn(1000, 1000)\n",
    "\n",
    "    patterns: list[dict] = []\n",
    "\n",
    "    # reshape (contiguous → view, no copy)\n",
    "    t, _ = measure_time(lambda: data.reshape(1000, 10000))\n",
    "    patterns.append({'Pattern': 'reshape (contiguous)', 'Time (µs)': t * 1e6,\n",
    "                     'Copies Data': 'No', 'Note': 'Just changes metadata'})\n",
    "\n",
    "    # view (contiguous, no copy)\n",
    "    t, _ = measure_time(lambda: data.view(1000, 10000))\n",
    "    patterns.append({'Pattern': 'view (contiguous)', 'Time (µs)': t * 1e6,\n",
    "                     'Copies Data': 'No', 'Note': 'Identical to reshape'})\n",
    "\n",
    "    # reshape after transpose (non-contiguous → copy)\n",
    "    mat_t = mat.t()  # Non-contiguous\n",
    "    t, _ = measure_time(lambda: mat_t.reshape(-1))\n",
    "    patterns.append({'Pattern': 'reshape (non-contiguous)', 'Time (µs)': t * 1e6,\n",
    "                     'Copies Data': 'Yes', 'Note': 'Forces contiguous copy'})\n",
    "\n",
    "    # contiguous() cost\n",
    "    t, _ = measure_time(lambda: mat_t.contiguous())\n",
    "    patterns.append({'Pattern': '.contiguous()', 'Time (µs)': t * 1e6,\n",
    "                     'Copies Data': 'Yes', 'Note': 'Explicit copy to C-order'})\n",
    "\n",
    "    # flatten (contiguous → view)\n",
    "    t, _ = measure_time(lambda: data.reshape(100, 100, 1000).flatten())\n",
    "    patterns.append({'Pattern': 'flatten (contiguous)', 'Time (µs)': t * 1e6,\n",
    "                     'Copies Data': 'No', 'Note': 'View when possible'})\n",
    "\n",
    "    # unsqueeze/squeeze (always free)\n",
    "    t, _ = measure_time(lambda: data.unsqueeze(0).unsqueeze(-1).squeeze(0))\n",
    "    patterns.append({'Pattern': 'unsqueeze/squeeze', 'Time (µs)': t * 1e6,\n",
    "                     'Copies Data': 'No', 'Note': 'Always metadata-only'})\n",
    "\n",
    "    # permute (returns view but non-contiguous)\n",
    "    img = torch.randn(8, 3, 64, 64)\n",
    "    t, _ = measure_time(lambda: img.permute(0, 2, 3, 1))\n",
    "    patterns.append({'Pattern': 'permute', 'Time (µs)': t * 1e6,\n",
    "                     'Copies Data': 'No', 'Note': 'View, but non-contiguous'})\n",
    "\n",
    "    # permute + contiguous (forced copy)\n",
    "    t, _ = measure_time(lambda: img.permute(0, 2, 3, 1).contiguous())\n",
    "    patterns.append({'Pattern': 'permute + contiguous', 'Time (µs)': t * 1e6,\n",
    "                     'Copies Data': 'Yes', 'Note': 'Copy to make C-order'})\n",
    "\n",
    "    return pd.DataFrame(patterns)\n",
    "\n",
    "\n",
    "reshape_bench = benchmark_reshape_patterns()\n",
    "print('=== Reshape Performance ===')\n",
    "print(reshape_bench.to_string(index=False))\n",
    "print()\n",
    "print('Key insight: view/reshape/unsqueeze/squeeze are essentially free (~1µs).')\n",
    "print('The only cost comes when a copy is forced (non-contiguous → contiguous).')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Einsum vs Explicit Operations: Speed Comparison",
    "",
    "Einsum is convenient, but is it slower than explicit operations? Let's find out",
    "with a systematic benchmark."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def benchmark_einsum_vs_explicit() -> pd.DataFrame:\n",
    "    \"\"\"Compare einsum against explicit implementations for common operations.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing comparisons.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    records: list[dict] = []\n",
    "\n",
    "    # Matrix multiply\n",
    "    A = torch.randn(256, 512)\n",
    "    B = torch.randn(512, 256)\n",
    "    t_exp, _ = measure_time(lambda: A @ B)\n",
    "    t_ein, _ = measure_time(lambda: torch.einsum('ij,jk->ik', A, B))\n",
    "    records.append({'Operation': 'MatMul (256×512)×(512×256)',\n",
    "                    'Explicit (ms)': t_exp * 1000,\n",
    "                    'Einsum (ms)': t_ein * 1000,\n",
    "                    'Ratio': t_ein / t_exp})\n",
    "\n",
    "    # Batched matmul\n",
    "    bA = torch.randn(16, 64, 128)\n",
    "    bB = torch.randn(16, 128, 64)\n",
    "    t_exp, _ = measure_time(lambda: torch.bmm(bA, bB))\n",
    "    t_ein, _ = measure_time(lambda: torch.einsum('bij,bjk->bik', bA, bB))\n",
    "    records.append({'Operation': 'Batched MatMul (16×64×128)',\n",
    "                    'Explicit (ms)': t_exp * 1000,\n",
    "                    'Einsum (ms)': t_ein * 1000,\n",
    "                    'Ratio': t_ein / t_exp})\n",
    "\n",
    "    # Dot product\n",
    "    v1 = torch.randn(100_000)\n",
    "    v2 = torch.randn(100_000)\n",
    "    t_exp, _ = measure_time(lambda: torch.dot(v1, v2))\n",
    "    t_ein, _ = measure_time(lambda: torch.einsum('i,i->', v1, v2))\n",
    "    records.append({'Operation': 'Dot Product (100K)',\n",
    "                    'Explicit (ms)': t_exp * 1000,\n",
    "                    'Einsum (ms)': t_ein * 1000,\n",
    "                    'Ratio': t_ein / t_exp})\n",
    "\n",
    "    # Attention scores\n",
    "    Q = torch.randn(4, 8, 32, 64)\n",
    "    K = torch.randn(4, 8, 32, 64)\n",
    "    t_exp, _ = measure_time(lambda: Q @ K.transpose(-2, -1))\n",
    "    t_ein, _ = measure_time(lambda: torch.einsum('bhid,bhjd->bhij', Q, K))\n",
    "    records.append({'Operation': 'Attention (4×8×32×64)',\n",
    "                    'Explicit (ms)': t_exp * 1000,\n",
    "                    'Einsum (ms)': t_ein * 1000,\n",
    "                    'Ratio': t_ein / t_exp})\n",
    "\n",
    "    # Outer product\n",
    "    u = torch.randn(1000)\n",
    "    w = torch.randn(1000)\n",
    "    t_exp, _ = measure_time(lambda: u.unsqueeze(1) * w.unsqueeze(0))\n",
    "    t_ein, _ = measure_time(lambda: torch.einsum('i,j->ij', u, w))\n",
    "    records.append({'Operation': 'Outer Product (1000×1000)',\n",
    "                    'Explicit (ms)': t_exp * 1000,\n",
    "                    'Einsum (ms)': t_ein * 1000,\n",
    "                    'Ratio': t_ein / t_exp})\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "einsum_bench = benchmark_einsum_vs_explicit()\n",
    "print('=== Einsum vs Explicit Operations ===')\n",
    "print(einsum_bench.to_string(index=False))\n",
    "print()\n",
    "avg_ratio = einsum_bench['Ratio'].mean()\n",
    "print(f'Average ratio (einsum/explicit): {avg_ratio:.2f}×')\n",
    "print('Einsum has slight overhead for parsing the format string, but for')\n",
    "print('compute-heavy operations the difference is negligible.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualization: Operation Performance Overview",
    "",
    "Let's create a visual summary of all the performance data we've collected."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Reshape costs\n",
    "reshape_data = reshape_bench.copy()\n",
    "colors = ['#43A047' if c == 'No' else '#E53935' for c in reshape_data['Copies Data']]\n",
    "bars = axes[0].barh(reshape_data['Pattern'], reshape_data['Time (µs)'], color=colors)\n",
    "axes[0].set_xlabel('Time (µs)')\n",
    "axes[0].set_title('Reshape Operation Costs')\n",
    "axes[0].set_xscale('log')\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#43A047', label='No Copy (Free)'),\n",
    "                   Patch(facecolor='#E53935', label='Copies Data')]\n",
    "axes[0].legend(handles=legend_elements, loc='lower right')\n",
    "axes[0].grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Right: Einsum overhead\n",
    "x_pos = range(len(einsum_bench))\n",
    "width = 0.35\n",
    "axes[1].bar([p - width/2 for p in x_pos], einsum_bench['Explicit (ms)'],\n",
    "            width, label='Explicit', color='#1E88E5')\n",
    "axes[1].bar([p + width/2 for p in x_pos], einsum_bench['Einsum (ms)'],\n",
    "            width, label='Einsum', color='#FF9800')\n",
    "axes[1].set_xticks(list(x_pos))\n",
    "short_labels = [op.split('(')[0].strip() for op in einsum_bench['Operation']]\n",
    "axes[1].set_xticklabels(short_labels, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Time (ms)')\n",
    "axes[1].set_title('Einsum vs Explicit Operations')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Common Pitfalls and Error Analysis",
    "",
    "Let's document the most common mistakes when working with tensor operations",
    "and show how to diagnose and fix them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demonstrate_common_pitfalls() -> None:\n",
    "    \"\"\"Show common tensor manipulation mistakes and how to fix them.\"\"\"\n",
    "    print('=== Pitfall 1: Shape Mismatch in Broadcasting ===')\n",
    "    a = torch.randn(3, 4)\n",
    "    b = torch.randn(3)  # Intending to add to each row\n",
    "    try:\n",
    "        result = a + b  # Fails! (3, 4) + (3,) — 4 ≠ 3\n",
    "    except RuntimeError as e:\n",
    "        print(f'  Error: {str(e)[:60]}')\n",
    "        print(f'  Fix: b.unsqueeze(1) makes (3,) → (3, 1), which broadcasts')\n",
    "        result = a + b.unsqueeze(1)  # (3, 4) + (3, 1) ✓\n",
    "        print(f'  Result shape: {result.shape}')\n",
    "    print()\n",
    "\n",
    "    print('=== Pitfall 2: Forgetting contiguous() Before view() ===')\n",
    "    mat = torch.randn(4, 4)\n",
    "    transposed = mat.t()\n",
    "    print(f'  Transposed is contiguous: {transposed.is_contiguous()}')\n",
    "    try:\n",
    "        flat = transposed.view(-1)\n",
    "    except RuntimeError:\n",
    "        print(f'  view(-1) fails on non-contiguous tensor')\n",
    "        flat = transposed.contiguous().view(-1)  # Fix\n",
    "        print(f'  Fix: .contiguous().view(-1) → shape={flat.shape}')\n",
    "    print()\n",
    "\n",
    "    print('=== Pitfall 3: Unintended Dimension Collapse ===')\n",
    "    batch = torch.randn(8, 3, 32, 32)\n",
    "    # Wrong: sum without keepdim collapses the dimension\n",
    "    wrong_mean = batch.mean(dim=1)        # shape (8, 32, 32) — lost channel dim\n",
    "    right_mean = batch.mean(dim=1, keepdim=True)  # shape (8, 1, 32, 32)\n",
    "    print(f'  mean(dim=1):             shape={wrong_mean.shape} (channel dim gone!)')\n",
    "    print(f'  mean(dim=1, keepdim):    shape={right_mean.shape} (can still broadcast)')\n",
    "    print()\n",
    "\n",
    "    print('=== Pitfall 4: Integer vs Float Division ===')\n",
    "    int_tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "    # Integer division truncates\n",
    "    wrong_norm = int_tensor / int_tensor.sum()  # sum=15\n",
    "    right_norm = int_tensor.float() / int_tensor.sum().float()\n",
    "    print(f'  Integer division: {wrong_norm}')\n",
    "    print(f'  Float division:   {right_norm}')\n",
    "    print(f'  Sum check: int={wrong_norm.sum():.4f}, float={right_norm.sum():.4f}')\n",
    "    print()\n",
    "\n",
    "    print('=== Pitfall 5: Mixing NumPy and PyTorch Random Seeds ===')\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    np_vals = np.random.randn(3)\n",
    "    torch_vals = torch.randn(3).numpy()\n",
    "    print(f'  NumPy random:   {np_vals}')\n",
    "    print(f'  PyTorch random: {torch_vals}')\n",
    "    print(f'  Same values? {np.allclose(np_vals, torch_vals)}')\n",
    "    print(f'  Different RNGs! Set both seeds independently.')\n",
    "    # Restore seeds\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "demonstrate_common_pitfalls()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Comprehensive Operation Reference",
    "",
    "Let's build a master reference table summarizing every operation we've covered,",
    "with the NumPy and PyTorch equivalents side by side."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reference = pd.DataFrame({\n",
    "    'Operation': [\n",
    "        'Reshape', 'View (no copy)', 'Flatten', 'Squeeze',\n",
    "        'Unsqueeze', 'Transpose (2D)', 'Permute (nD)',\n",
    "        'Concatenate', 'Stack', 'Fancy index', 'Boolean mask',\n",
    "        'Where', 'Gather', 'Scatter', 'Einsum',\n",
    "    ],\n",
    "    'NumPy': [\n",
    "        'arr.reshape()', 'N/A', 'arr.flatten() / ravel()', 'np.squeeze()',\n",
    "        'np.expand_dims()', 'arr.T', 'arr.transpose()',\n",
    "        'np.concatenate()', 'np.stack()', 'arr[[0,2]]', 'arr[mask]',\n",
    "        'np.where()', 'N/A (use fancy idx)', 'N/A', 'np.einsum()',\n",
    "    ],\n",
    "    'PyTorch': [\n",
    "        't.reshape()', 't.view()', 't.flatten()', 't.squeeze()',\n",
    "        't.unsqueeze()', 't.t() / t.T', 't.permute()',\n",
    "        'torch.cat()', 'torch.stack()', 't[[0,2]]', 't[mask]',\n",
    "        'torch.where()', 'torch.gather()', 'torch.scatter()', 'torch.einsum()',\n",
    "    ],\n",
    "    'Returns View?': [\n",
    "        'If contiguous', 'Always', 'If contiguous', 'Yes',\n",
    "        'Yes', 'Yes', 'Yes',\n",
    "        'No (new tensor)', 'No (new tensor)', 'No (copy)', 'No (copy)',\n",
    "        'No (new tensor)', 'No (new tensor)', 'In-place variant', 'No (new tensor)',\n",
    "    ],\n",
    "})\n",
    "print('=== Tensor Operations Reference ===')\n",
    "print(reference.to_string(index=False))\n",
    "print()\n",
    "print(f'Total operations covered: {len(reference)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Indexing Performance: Fancy vs Boolean vs Gather",
    "",
    "Different selection methods have different performance profiles. Let's measure",
    "them for a common scenario: selecting specific elements from a large tensor."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def benchmark_indexing_methods() -> pd.DataFrame:\n",
    "    \"\"\"Compare indexing methods for selecting elements.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with timing comparison.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(SEED)\n",
    "    n_rows = 10_000\n",
    "    n_cols = 1_000\n",
    "    data = torch.randn(n_rows, n_cols)\n",
    "\n",
    "    # Select one element per row (like gathering class probabilities)\n",
    "    indices = torch.randint(0, n_cols, (n_rows,))\n",
    "\n",
    "    # Method 1: Fancy indexing\n",
    "    t_fancy, _ = measure_time(\n",
    "        lambda: data[torch.arange(n_rows), indices])\n",
    "\n",
    "    # Method 2: Gather\n",
    "    t_gather, _ = measure_time(\n",
    "        lambda: data.gather(1, indices.unsqueeze(1)).squeeze(1))\n",
    "\n",
    "    # Method 3: Loop (anti-pattern)\n",
    "    def loop_select() -> torch.Tensor:\n",
    "        \"\"\"Select elements with a loop (slow).\"\"\"\n",
    "        result = torch.empty(n_rows)\n",
    "        for i in range(n_rows):\n",
    "            result[i] = data[i, indices[i]]\n",
    "        return result\n",
    "    t_loop, _ = measure_time(loop_select, num_warmup=1, num_timed=2)\n",
    "\n",
    "    # Verify all methods give same result\n",
    "    r_fancy = data[torch.arange(n_rows), indices]\n",
    "    r_gather = data.gather(1, indices.unsqueeze(1)).squeeze(1)\n",
    "    r_loop = loop_select()\n",
    "    assert torch.allclose(r_fancy, r_gather)\n",
    "    assert torch.allclose(r_fancy, r_loop)\n",
    "\n",
    "    records = [\n",
    "        {'Method': 'Loop (anti-pattern)', 'Time (ms)': t_loop * 1000,\n",
    "         'Speedup': 1.0},\n",
    "        {'Method': 'Fancy indexing', 'Time (ms)': t_fancy * 1000,\n",
    "         'Speedup': t_loop / t_fancy},\n",
    "        {'Method': 'torch.gather', 'Time (ms)': t_gather * 1000,\n",
    "         'Speedup': t_loop / t_gather},\n",
    "    ]\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "idx_bench = benchmark_indexing_methods()\n",
    "print(f'Indexing Benchmark ({10_000} rows × {1_000} cols):')\n",
    "print(idx_bench.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## Part 5 — Summary & Lessons Learned",
    "",
    "### Key Takeaways",
    "",
    "1. **Reshape, view, squeeze, unsqueeze are free.** They only change metadata (shape and",
    "   strides), not the underlying data. Use them liberally to make tensors compatible",
    "   for broadcasting and batch operations.",
    "",
    "2. **Permute creates non-contiguous views.** After permuting, call `.contiguous()` if",
    "   you need to `view()` the result. Or just use `.reshape()` which handles this automatically.",
    "",
    "3. **Einsum is a universal tool.** It can express any tensor contraction — from dot",
    "   products to batched attention — in a single readable string. The performance overhead",
    "   vs explicit operations is negligible.",
    "",
    "4. **Advanced indexing (fancy + boolean) always copies.** Unlike slicing, which returns",
    "   views, fancy indexing and boolean masks create new tensors. Use `gather`/`scatter`",
    "   for differentiable selection in training.",
    "",
    "5. **In-place operations save memory but break autograd.** Only use `add_()`, `mul_()`,",
    "   etc. on tensors that don't require gradients. During training, always prefer",
    "   out-of-place operations.",
    "",
    "### What's Next",
    "",
    "→ **01-03 (Pandas for Tabular Data)** applies these manipulation skills to real-world",
    "  tabular datasets with Pandas — the primary tool for data exploration and preprocessing.",
    "",
    "### Going Further",
    "",
    "- [PyTorch Tensor Views](https://pytorch.org/docs/stable/tensor_view.html) — Official",
    "  documentation on which operations return views vs copies",
    "- [Einsum Is All You Need](https://rockt.github.io/2018/04/30/einsum) — Visual guide",
    "  to einsum notation with diagrams",
    "- [NumPy Broadcasting Rules](https://numpy.org/doc/stable/user/basics.broadcasting.html)",
    "  — The definitive reference for broadcasting semantics"
   ]
  }
 ]
}